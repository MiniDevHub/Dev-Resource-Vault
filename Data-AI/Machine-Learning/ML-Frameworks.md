<div align="center">

# ğŸ¤– Machine Learning Frameworks - Complete Guide

![ML](https://img.shields.io/badge/Machine_Learning-Frameworks-red?style=for-the-badge&logo=tensorflow)
![Python](https://img.shields.io/badge/Python-ML-blue?style=for-the-badge&logo=python)
![Level](https://img.shields.io/badge/Level-Intermediate-orange?style=for-the-badge)

### _From classical ML to deep learning - master them all_ ğŸš€

**Build intelligent systems with the right tools** âœ¨

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Framework Overview](#-framework-overview)
- [ğŸ“Š Scikit-learn](#-scikit-learn)
- [ğŸ§  TensorFlow & Keras](#-tensorflow--keras)
- [ğŸ”¥ PyTorch](#-pytorch)
- [âš¡ Gradient Boosting](#-gradient-boosting)
- [ğŸŒŸ Other Frameworks](#-other-frameworks)
- [ğŸ“Š Framework Comparison](#-framework-comparison)
- [ğŸš€ Model Deployment](#-model-deployment)
- [ğŸ’¡ Best Practices](#-best-practices)
- [ğŸ”§ Real-World Projects](#-real-world-projects)

---

<div align="center">

## ğŸ¯ Framework Overview

</div>

### Understanding ML Frameworks ğŸ“–

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MACHINE LEARNING FRAMEWORK LANDSCAPE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   WHAT ARE ML FRAMEWORKS?                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Machine Learning Frameworks: Tools and libraries for building ML models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Pre-built algorithms and tools
â€¢ Handle complex mathematics
â€¢ Optimize performance
â€¢ Simplify model development
â€¢ Provide deployment capabilities

Evolution:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2007: Scikit-learn    â†’ Classical ML
2011: Torch           â†’ Deep Learning (Lua)
2015: TensorFlow      â†’ Google's DL framework
2016: Keras           â†’ High-level DL API
2016: PyTorch         â†’ Facebook's DL framework
2016: XGBoost         â†’ Gradient boosting
2017: LightGBM        â†’ Fast gradient boosting
2017: CatBoost        â†’ Categorical features
2018: FastAI          â†’ Simplified deep learning
2018: JAX             â†’ High-performance ML

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   FRAMEWORK CATEGORIES                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Classical ML:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Scikit-learn        â­ Industry standard
â€¢ Statsmodels         Statistical models
â€¢ MLJAR               AutoML

Deep Learning:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ TensorFlow/Keras    â­ Production-ready
â€¢ PyTorch             â­ Research-friendly
â€¢ FastAI              Beginner-friendly
â€¢ JAX                 High-performance

Gradient Boosting:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ XGBoost             â­ Most popular
â€¢ LightGBM            â­ Fastest
â€¢ CatBoost            â­ Handles categories

Specialized:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Hugging Face        NLP/Transformers
â€¢ OpenCV              Computer Vision
â€¢ Surprise            Recommendation Systems
â€¢ PySpark MLlib       Big Data ML

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   FRAMEWORK COMPARISON                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Framework        | Type              | Best For                        | Learning Curve | Production   | Community    |
| ---------------- | ----------------- | ------------------------------- | -------------- | ------------ | ------------ |
| **Scikit-learn** | Classical ML      | Tabular data, traditional ML    | â­ Easy        | âœ… Good      | â­â­â­ Huge  |
| **TensorFlow**   | Deep Learning     | Production, mobile, large scale | â­â­â­ Hard    | âœ… Excellent | â­â­â­ Huge  |
| **Keras**        | Deep Learning     | Prototyping, beginners          | â­ Easy        | âœ… Good      | â­â­â­ Large |
| **PyTorch**      | Deep Learning     | Research, flexibility           | â­â­ Moderate  | âœ… Good      | â­â­â­ Huge  |
| **XGBoost**      | Gradient Boosting | Structured data, competitions   | â­â­ Moderate  | âœ… Excellent | â­â­â­ Large |
| **LightGBM**     | Gradient Boosting | Large datasets, speed           | â­â­ Moderate  | âœ… Excellent | â­â­ Medium  |
| **CatBoost**     | Gradient Boosting | Categorical features            | â­â­ Moderate  | âœ… Good      | â­â­ Medium  |
| **FastAI**       | Deep Learning     | Quick prototyping               | â­ Easy        | âš ï¸ Fair      | â­â­ Growing |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   CHOOSING A FRAMEWORK                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Decision Tree:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

What type of data?
â”œâ”€ Tabular/Structured
â”‚  â”œâ”€ Small/Medium (<100K rows)
â”‚  â”‚  â””â”€ Scikit-learn âœ…
â”‚  â”œâ”€ Large (>100K rows)
â”‚  â”‚  â””â”€ XGBoost / LightGBM âœ…
â”‚  â””â”€ Many categorical features
â”‚     â””â”€ CatBoost âœ…
â”‚
â”œâ”€ Images
â”‚  â”œâ”€ Quick prototyping
â”‚  â”‚  â””â”€ FastAI âœ…
â”‚  â”œâ”€ Custom architectures
â”‚  â”‚  â””â”€ PyTorch âœ…
â”‚  â””â”€ Production deployment
â”‚     â””â”€ TensorFlow âœ…
â”‚
â”œâ”€ Text
â”‚  â”œâ”€ Modern NLP (Transformers)
â”‚  â”‚  â””â”€ Hugging Face âœ…
â”‚  â”œâ”€ Classical NLP
â”‚  â”‚  â””â”€ Scikit-learn âœ…
â”‚  â””â”€ Custom models
â”‚     â””â”€ PyTorch âœ…
â”‚
â””â”€ Time Series
   â”œâ”€ Traditional forecasting
   â”‚  â””â”€ Statsmodels âœ…
   â”œâ”€ Complex patterns
   â”‚  â””â”€ TensorFlow / PyTorch âœ…
   â””â”€ Tabular time series
      â””â”€ XGBoost / LightGBM âœ…

Use Case Recommendations:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Prototyping: Scikit-learn, Keras, FastAI
âœ… Research: PyTorch, JAX
âœ… Production: TensorFlow, XGBoost, Scikit-learn
âœ… Kaggle/Competitions: XGBoost, LightGBM, CatBoost
âœ… Edge devices: TensorFlow Lite, ONNX
âœ… Big Data: PySpark MLlib, Dask-ML

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## ğŸ“Š Scikit-learn

</div>

### The Swiss Army Knife of Machine Learning ğŸ”§

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCIKIT-LEARN FUNDAMENTALS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Scikit-learn: Most popular Python ML library
- Consistent API across algorithms
- Extensive documentation
- Production-ready
- Integrates with NumPy/Pandas
- 15+ years of development
"""

# Installation
# pip install scikit-learn

import numpy as np
import pandas as pd
from sklearn import datasets
import matplotlib.pyplot as plt

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUPERVISED LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CLASSIFICATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, roc_auc_score, roc_curve)

# Load dataset
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X, y = data.data, data.target

print(f"Dataset: {data.DESCR[:200]}...")
print(f"Features: {data.feature_names[:5]}...")
print(f"Shape: {X.shape}")
print(f"Classes: {data.target_names}")

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train multiple classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=10000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

results = {}
for name, clf in classifiers.items():
    # Train
    clf.fit(X_train_scaled, y_train)

    # Predict
    y_pred = clf.predict(X_test_scaled)
    y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1] if hasattr(clf, 'predict_proba') else None

    # Evaluate
    accuracy = accuracy_score(y_test, y_pred)

    results[name] = {
        'model': clf,
        'accuracy': accuracy,
        'predictions': y_pred
    }

    print(f"\n{name}:")
    print(f"Accuracy: {accuracy:.4f}")

    if y_pred_proba is not None:
        auc = roc_auc_score(y_test, y_pred_proba)
        print(f"AUC-ROC: {auc:.4f}")

# Best model
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']
print(f"\nğŸ† Best Model: {best_model_name}")

# Detailed evaluation
print("\nClassification Report:")
print(classification_report(y_test, results[best_model_name]['predictions'],
                          target_names=data.target_names))

# Confusion Matrix
cm = confusion_matrix(y_test, results[best_model_name]['predictions'])
print("\nConfusion Matrix:")
print(cm)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. REGRESSION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Load dataset
from sklearn.datasets import load_diabetes
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train regressors
regressors = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=1.0),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

print("\n" + "="*60)
print("REGRESSION MODELS")
print("="*60)

for name, reg in regressors.items():
    # Train
    reg.fit(X_train, y_train)

    # Predict
    y_pred = reg.predict(X_test)

    # Evaluate
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\n{name}:")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE:  {mae:.2f}")
    print(f"  RÂ²:   {r2:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UNSUPERVISED LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CLUSTERING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Generate sample data
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=42)

# K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
y_kmeans = kmeans.fit_predict(X)

print("\n" + "="*60)
print("CLUSTERING")
print("="*60)
print(f"\nK-Means:")
print(f"  Inertia: {kmeans.inertia_:.2f}")
print(f"  Silhouette Score: {silhouette_score(X, y_kmeans):.4f}")

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
y_dbscan = dbscan.fit_predict(X)
print(f"\nDBSCAN:")
print(f"  Number of clusters: {len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)}")
print(f"  Number of noise points: {list(y_dbscan).count(-1)}")

# Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=4)
y_hierarchical = hierarchical.fit_predict(X)
print(f"\nHierarchical Clustering:")
print(f"  Silhouette Score: {silhouette_score(X, y_hierarchical):.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. DIMENSIONALITY REDUCTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE

# Load high-dimensional data
digits = datasets.load_digits()
X = digits.data

print("\n" + "="*60)
print("DIMENSIONALITY REDUCTION")
print("="*60)
print(f"Original dimensions: {X.shape}")

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
print(f"\nPCA:")
print(f"  Reduced dimensions: {X_pca.shape}")
print(f"  Explained variance: {pca.explained_variance_ratio_.sum():.4f}")

# t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)
print(f"\nt-SNE:")
print(f"  Reduced dimensions: {X_tsne.shape}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL SELECTION & VALIDATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.model_selection import (cross_val_score, GridSearchCV,
                                    RandomizedSearchCV, StratifiedKFold)

# Load dataset
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cross-Validation
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model = RandomForestClassifier(random_state=42)

# Simple cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print("\n" + "="*60)
print("CROSS-VALIDATION")
print("="*60)
print(f"CV Scores: {cv_scores}")
print(f"Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores_stratified = cross_val_score(model, X_train, y_train, cv=skf)
print(f"\nStratified CV Scores: {cv_scores_stratified}")
print(f"Mean: {cv_scores_stratified.mean():.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hyperparameter Tuning - Grid Search
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("\n" + "="*60)
print("GRID SEARCH")
print("="*60)
grid_search.fit(X_train, y_train)

print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")
print(f"Test score: {grid_search.score(X_test, y_test):.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hyperparameter Tuning - Random Search
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from scipy.stats import randint

param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=20,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

print("\n" + "="*60)
print("RANDOM SEARCH")
print("="*60)
random_search.fit(X_train, y_train)

print(f"\nBest parameters: {random_search.best_params_}")
print(f"Best CV score: {random_search.best_score_:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif

# Create pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('feature_selection', SelectKBest(f_classif, k=10)),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Train pipeline
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("\n" + "="*60)
print("PIPELINE")
print("="*60)
print(f"Pipeline accuracy: {accuracy:.4f}")

# Pipeline with GridSearch
param_grid = {
    'feature_selection__k': [5, 10, 15, 20],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [5, 10, None]
}

grid_pipeline = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)
grid_pipeline.fit(X_train, y_train)

print(f"\nBest pipeline parameters: {grid_pipeline.best_params_}")
print(f"Best score: {grid_pipeline.best_score_:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FEATURE ENGINEERING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler,
                                   LabelEncoder, OneHotEncoder, PolynomialFeatures)
from sklearn.impute import SimpleImputer

# Sample data with missing values and categories
df = pd.DataFrame({
    'age': [25, 30, None, 40, 35],
    'income': [50000, 60000, 55000, None, 70000],
    'category': ['A', 'B', 'A', 'C', 'B'],
    'target': [0, 1, 0, 1, 1]
})

print("\n" + "="*60)
print("FEATURE ENGINEERING")
print("="*60)
print("\nOriginal data:")
print(df)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
df[['age', 'income']] = imputer.fit_transform(df[['age', 'income']])

print("\nAfter imputation:")
print(df)

# Encode categorical variables
label_encoder = LabelEncoder()
df['category_encoded'] = label_encoder.fit_transform(df['category'])

# One-hot encoding
onehot = pd.get_dummies(df['category'], prefix='cat')
df = pd.concat([df, onehot], axis=1)

print("\nAfter encoding:")
print(df.head())

# Create polynomial features
X = df[['age', 'income']].values
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

print(f"\nOriginal features: {X.shape}")
print(f"Polynomial features: {X_poly.shape}")
print(f"Feature names: {poly.get_feature_names_out(['age', 'income'])}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL PERSISTENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import joblib
import pickle

# Train a model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save with joblib (recommended)
joblib.dump(model, 'model.pkl')

# Load model
loaded_model = joblib.load('model.pkl')

# Verify
print("\n" + "="*60)
print("MODEL PERSISTENCE")
print("="*60)
print(f"Original model score: {model.score(X_test, y_test):.4f}")
print(f"Loaded model score: {loaded_model.score(X_test, y_test):.4f}")

# Save with pickle
with open('model_pickle.pkl', 'wb') as f:
    pickle.dump(model, f)

# Load with pickle
with open('model_pickle.pkl', 'rb') as f:
    model_pickle = pickle.load(f)

print(f"Pickle model score: {model_pickle.score(X_test, y_test):.4f}")

print("\nâœ… Scikit-learn complete!")
```

---

<div align="center">

## ğŸ§  TensorFlow & Keras

</div>

### Deep Learning for Production ğŸ­

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TENSORFLOW & KERAS FUNDAMENTALS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
TensorFlow: Google's deep learning framework
Keras: High-level API for TensorFlow
- Production-ready
- Mobile and web deployment
- Distributed training
- TensorFlow Serving
"""

# Installation
# pip install tensorflow

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {keras.__version__}")

# Check GPU
print(f"GPU available: {tf.config.list_physical_devices('GPU')}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEURAL NETWORKS WITH KERAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. SIMPLE NEURAL NETWORK (MNIST)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Load data
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

print(f"\nDataset shapes:")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")

# Preprocess
X_train = X_train.reshape(-1, 784) / 255.0
X_test = X_test.reshape(-1, 784) / 255.0

# One-hot encode labels
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

# Build model - Sequential API
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Compile
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Model summary
print("\n" + "="*60)
print("MODEL ARCHITECTURE")
print("="*60)
model.summary()

# Train
history = model.fit(
    X_train, y_train_cat,
    epochs=5,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# Evaluate
test_loss, test_acc = model.evaluate(X_test, y_test_cat)
print(f"\nTest accuracy: {test_acc:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. CONVOLUTIONAL NEURAL NETWORK (CNN)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Load CIFAR-10
(X_train_img, y_train_img), (X_test_img, y_test_img) = keras.datasets.cifar10.load_data()

# Normalize
X_train_img = X_train_img.astype('float32') / 255.0
X_test_img = X_test_img.astype('float32') / 255.0

# One-hot encode
y_train_img = keras.utils.to_categorical(y_train_img, 10)
y_test_img = keras.utils.to_categorical(y_test_img, 10)

# Build CNN
cnn_model = keras.Sequential([
    # Convolutional layers
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),

    # Dense layers
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile
cnn_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\n" + "="*60)
print("CNN ARCHITECTURE")
print("="*60)
cnn_model.summary()

# Train (using subset for demo)
history_cnn = cnn_model.fit(
    X_train_img[:5000], y_train_img[:5000],
    epochs=5,
    batch_size=64,
    validation_split=0.2,
    verbose=1
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. FUNCTIONAL API (COMPLEX MODELS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Multi-input model
input_a = layers.Input(shape=(32,), name='input_a')
input_b = layers.Input(shape=(32,), name='input_b')

# Process first input
x = layers.Dense(16, activation='relu')(input_a)
x = layers.Dropout(0.2)(x)

# Process second input
y = layers.Dense(16, activation='relu')(input_b)
y = layers.Dropout(0.2)(y)

# Concatenate
combined = layers.concatenate([x, y])

# Output layers
z = layers.Dense(32, activation='relu')(combined)
output = layers.Dense(1, activation='sigmoid', name='output')(z)

# Create model
multi_input_model = keras.Model(inputs=[input_a, input_b], outputs=output)

multi_input_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

print("\n" + "="*60)
print("MULTI-INPUT MODEL")
print("="*60)
multi_input_model.summary()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CALLBACKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,
                                       ReduceLROnPlateau, TensorBoard)

# Early stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# Model checkpoint
checkpoint = ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max'
)

# Reduce learning rate on plateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=2,
    min_lr=0.0001
)

# TensorBoard
tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)

# Train with callbacks
model_with_callbacks = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model_with_callbacks.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_callbacks = model_with_callbacks.fit(
    X_train, y_train_cat,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stop, checkpoint, reduce_lr],
    verbose=0
)

print("\n" + "="*60)
print("TRAINING WITH CALLBACKS")
print("="*60)
print(f"Training stopped at epoch: {len(history_callbacks.history['loss'])}")
print(f"Best validation accuracy: {max(history_callbacks.history['val_accuracy']):.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFER LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2

# Load pre-trained model
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Freeze base model
base_model.trainable = False

# Add custom layers
transfer_model = keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # 10 classes
])

transfer_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\n" + "="*60)
print("TRANSFER LEARNING MODEL")
print("="*60)
print(f"Total parameters: {transfer_model.count_params():,}")
print(f"Trainable parameters: {sum([tf.size(w).numpy() for w in transfer_model.trainable_weights]):,}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM TRAINING LOOPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Define model
custom_model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)),
    layers.Dense(10, activation='softmax')
])

# Define loss and optimizer
loss_fn = keras.losses.CategoricalCrossentropy()
optimizer = keras.optimizers.Adam(learning_rate=0.001)

# Metrics
train_acc_metric = keras.metrics.CategoricalAccuracy()

# Custom training step
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = custom_model(x, training=True)
        loss = loss_fn(y, predictions)

    gradients = tape.gradient(loss, custom_model.trainable_weights)
    optimizer.apply_gradients(zip(gradients, custom_model.trainable_weights))

    train_acc_metric.update_state(y, predictions)
    return loss

# Training loop
epochs = 3
batch_size = 128

print("\n" + "="*60)
print("CUSTOM TRAINING LOOP")
print("="*60)

for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")

    # Reset metrics
    train_acc_metric.reset_states()

    # Training
    for i in range(0, len(X_train), batch_size):
        x_batch = X_train[i:i+batch_size]
        y_batch = y_train_cat[i:i+batch_size]

        loss = train_step(x_batch, y_batch)

    # Print metrics
    train_acc = train_acc_metric.result()
    print(f"Training accuracy: {float(train_acc):.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL SAVING & LOADING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Save entire model
model.save('my_model.h5')
model.save('my_model')  # SavedModel format (recommended)

# Load model
loaded_model = keras.models.load_model('my_model.h5')

# Save weights only
model.save_weights('model_weights.h5')

# Load weights
model.load_weights('model_weights.h5')

# Save architecture only
model_json = model.to_json()
with open('model_architecture.json', 'w') as f:
    f.write(model_json)

# Load architecture
with open('model_architecture.json', 'r') as f:
    loaded_model_json = f.read()
loaded_model_from_json = keras.models.model_from_json(loaded_model_json)

print("\nâœ… TensorFlow/Keras complete!")
```

---

<div align="center">

## ğŸ”¥ PyTorch

</div>

### Research-Friendly Deep Learning ğŸ”¬

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYTORCH FUNDAMENTALS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
PyTorch: Facebook's deep learning framework
- Dynamic computation graphs
- Pythonic and intuitive
- Strong research community
- Easy debugging
- Flexible and fast
"""

# Installation
# pip install torch torchvision torchaudio

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Dataset
import torchvision
import torchvision.transforms as transforms
import numpy as np

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TENSORS - PYTORCH BASICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*60)
print("TENSOR OPERATIONS")
print("="*60)

# Create tensors
x = torch.tensor([1, 2, 3, 4, 5])
y = torch.zeros(3, 4)
z = torch.ones(2, 3)
random_tensor = torch.randn(3, 3)

print(f"\nTensor x: {x}")
print(f"Tensor shape: {x.shape}")
print(f"Tensor dtype: {x.dtype}")

# Operations
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(f"\nAddition: {a + b}")
print(f"Multiplication: {a * b}")
print(f"Dot product: {torch.dot(a, b)}")

# Matrix operations
matrix_a = torch.randn(3, 4)
matrix_b = torch.randn(4, 2)
result = torch.mm(matrix_a, matrix_b)  # Matrix multiplication
print(f"\nMatrix multiplication shape: {result.shape}")

# Move to GPU (if available)
if torch.cuda.is_available():
    x_gpu = x.to(device)
    print(f"Tensor on GPU: {x_gpu.device}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEURAL NETWORKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. SIMPLE FEEDFORWARD NETWORK
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# Initialize model
model = SimpleNet(input_size=784, hidden_size=128, num_classes=10)
model = model.to(device)

print("\n" + "="*60)
print("SIMPLE NEURAL NETWORK")
print("="*60)
print(model)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. MNIST CLASSIFICATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Load MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    download=True
)

test_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    transform=transform
)

# Create data loaders
batch_size = 128
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"\nTraining samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")

# Define model
class MNISTNet(nn.Module):
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.fc1 = nn.Linear(784, 512)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(0.2)
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = x.view(-1, 784)  # Flatten
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

mnist_model = MNISTNet().to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(mnist_model.parameters(), lr=0.001)

# Training function
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0
    correct = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

    train_loss /= len(train_loader)
    accuracy = 100. * correct / len(train_loader.dataset)

    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.2f}%')
    return train_loss, accuracy

# Testing function
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')
    return test_loss, accuracy

# Train model
print("\n" + "="*60)
print("TRAINING MNIST MODEL")
print("="*60)

epochs = 5
for epoch in range(1, epochs + 1):
    train_loss, train_acc = train(mnist_model, device, train_loader, optimizer, epoch)

# Test model
print("\n" + "="*60)
print("TESTING")
print("="*60)
test_loss, test_acc = test(mnist_model, device, test_loader)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. CONVOLUTIONAL NEURAL NETWORK (CNN)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        # Fully connected layers
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # Convolutional layers with pooling
        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14
        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7

        # Flatten
        x = x.view(-1, 64 * 7 * 7)

        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        return x

cnn_model = CNN().to(device)

print("\n" + "="*60)
print("CNN ARCHITECTURE")
print("="*60)
print(cnn_model)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. RESIDUAL NETWORK (ResNet)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class SimpleResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)

        self.layer1 = ResidualBlock(64, 64)
        self.layer2 = ResidualBlock(64, 128, stride=2)
        self.layer3 = ResidualBlock(128, 256, stride=2)

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.avg_pool(out)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out

resnet_model = SimpleResNet().to(device)

print("\n" + "="*60)
print("RESNET ARCHITECTURE")
print("="*60)
print(f"Total parameters: {sum(p.numel() for p in resnet_model.parameters()):,}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM DATASETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        label = self.labels[idx]

        if self.transform:
            sample = self.transform(sample)

        return sample, label

# Example usage
X = np.random.randn(1000, 28, 28)
y = np.random.randint(0, 10, 1000)

custom_dataset = CustomDataset(X, y, transform=transforms.ToTensor())
custom_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)

print("\n" + "="*60)
print("CUSTOM DATASET")
print("="*60)
print(f"Dataset size: {len(custom_dataset)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LEARNING RATE SCHEDULING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR

model = SimpleNet(784, 128, 10).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Step LR: Decay LR by factor every step_size epochs
scheduler_step = StepLR(optimizer, step_size=30, gamma=0.1)

# Reduce on plateau: Reduce when metric stops improving
scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)

# Cosine annealing
scheduler_cosine = CosineAnnealingLR(optimizer, T_max=200)

print("\n" + "="*60)
print("LEARNING RATE SCHEDULERS")
print("="*60)
print(f"Initial LR: {optimizer.param_groups[0]['lr']}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL SAVING & LOADING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Save entire model
torch.save(mnist_model, 'mnist_model_complete.pth')

# Save model state dict (recommended)
torch.save(mnist_model.state_dict(), 'mnist_model_state.pth')

# Save checkpoint
checkpoint = {
    'epoch': 5,
    'model_state_dict': mnist_model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': train_loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# Load model state dict
loaded_model = MNISTNet().to(device)
loaded_model.load_state_dict(torch.load('mnist_model_state.pth'))
loaded_model.eval()

# Load checkpoint
checkpoint = torch.load('checkpoint.pth')
mnist_model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

print("\n" + "="*60)
print("MODEL PERSISTENCE")
print("="*60)
print(f"Checkpoint loaded: Epoch {epoch}, Loss {loss:.4f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFER LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Load pre-trained ResNet
pretrained_resnet = torchvision.models.resnet18(pretrained=True)

# Freeze all layers
for param in pretrained_resnet.parameters():
    param.requires_grad = False

# Replace final layer
num_features = pretrained_resnet.fc.in_features
pretrained_resnet.fc = nn.Linear(num_features, 10)  # 10 classes

pretrained_resnet = pretrained_resnet.to(device)

print("\n" + "="*60)
print("TRANSFER LEARNING")
print("="*60)
print(f"Total parameters: {sum(p.numel() for p in pretrained_resnet.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in pretrained_resnet.parameters() if p.requires_grad):,}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYTORCH LIGHTNING (SIMPLIFIED TRAINING)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
# Install: pip install pytorch-lightning

import pytorch_lightning as pl

class LitModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = MNISTNet()
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        acc = (y_hat.argmax(dim=1) == y).float().mean()
        self.log('val_loss', loss)
        self.log('val_acc', acc)

    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=0.001)

# Train
lit_model = LitModel()
trainer = pl.Trainer(max_epochs=5, gpus=1 if torch.cuda.is_available() else 0)
trainer.fit(lit_model, train_loader, test_loader)
"""

print("\nâœ… PyTorch complete!")
```

---

<div align="center">

## âš¡ Gradient Boosting

</div>

### XGBoost, LightGBM, CatBoost ğŸš€

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRADIENT BOOSTING FRAMEWORKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Gradient Boosting: Ensemble learning technique
- XGBoost: Most popular, Kaggle winner
- LightGBM: Microsoft, fastest
- CatBoost: Yandex, handles categorical features
"""

# Installation
# pip install xgboost lightgbm catboost

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.datasets import load_breast_cancer, load_diabetes

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# XGBOOST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import xgboost as xgb
from xgboost import XGBClassifier, XGBRegressor

print("="*60)
print("XGBOOST")
print("="*60)
print(f"XGBoost version: {xgb.__version__}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Classification with XGBoost
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Load data
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Method 1: Scikit-learn API
xgb_clf = XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)

xgb_clf.fit(X_train, y_train)
y_pred = xgb_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nXGBoost Classifier Accuracy: {accuracy:.4f}")

# Method 2: Native API (more features)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

params = {
    'objective': 'binary:logistic',
    'max_depth': 3,
    'learning_rate': 0.1,
    'eval_metric': 'logloss'
}

# Train with early stopping
evals = [(dtrain, 'train'), (dtest, 'test')]
bst = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    evals=evals,
    early_stopping_rounds=10,
    verbose_eval=False
)

print(f"Best iteration: {bst.best_iteration}")
print(f"Best score: {bst.best_score:.4f}")

# Feature importance
feature_importance = xgb_clf.feature_importances_
top_features = sorted(zip(data.feature_names, feature_importance),
                     key=lambda x: x[1], reverse=True)[:5]
print("\nTop 5 Features:")
for feat, imp in top_features:
    print(f"  {feat}: {imp:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Regression with XGBoost
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

diabetes = load_diabetes()
X_reg, y_reg = diabetes.data, diabetes.target
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

xgb_reg = XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

xgb_reg.fit(X_train_reg, y_train_reg)
y_pred_reg = xgb_reg.predict(X_test_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))

print(f"\nXGBoost Regressor RMSE: {rmse:.2f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LIGHTGBM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import lightgbm as lgb
from lightgbm import LGBMClassifier, LGBMRegressor

print("\n" + "="*60)
print("LIGHTGBM")
print("="*60)
print(f"LightGBM version: {lgb.__version__}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Classification with LightGBM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Method 1: Scikit-learn API
lgb_clf = LGBMClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

lgb_clf.fit(X_train, y_train)
y_pred_lgb = lgb_clf.predict(X_test)
accuracy_lgb = accuracy_score(y_test, y_pred_lgb)

print(f"\nLightGBM Classifier Accuracy: {accuracy_lgb:.4f}")

# Method 2: Native API
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

params_lgb = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'max_depth': 3,
    'learning_rate': 0.1,
    'verbose': -1
}

# Train with validation
gbm = lgb.train(
    params_lgb,
    train_data,
    num_boost_round=100,
    valid_sets=[test_data],
    callbacks=[lgb.early_stopping(stopping_rounds=10)]
)

print(f"Best iteration: {gbm.best_iteration}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Regression with LightGBM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

lgb_reg = LGBMRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    random_state=42
)

lgb_reg.fit(X_train_reg, y_train_reg)
y_pred_lgb_reg = lgb_reg.predict(X_test_reg)
rmse_lgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_lgb_reg))

print(f"\nLightGBM Regressor RMSE: {rmse_lgb:.2f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATBOOST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from catboost import CatBoostClassifier, CatBoostRegressor, Pool

print("\n" + "="*60)
print("CATBOOST")
print("="*60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Classification with CatBoost
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat_clf = CatBoostClassifier(
    iterations=100,
    depth=3,
    learning_rate=0.1,
    random_state=42,
    verbose=False
)

cat_clf.fit(X_train, y_train)
y_pred_cat = cat_clf.predict(X_test)
accuracy_cat = accuracy_score(y_test, y_pred_cat)

print(f"\nCatBoost Classifier Accuracy: {accuracy_cat:.4f}")

# With categorical features
df_cat = pd.DataFrame({
    'feature1': np.random.randn(1000),
    'feature2': np.random.randn(1000),
    'category': np.random.choice(['A', 'B', 'C'], 1000),
    'target': np.random.randint(0, 2, 1000)
})

X_cat = df_cat[['feature1', 'feature2', 'category']]
y_cat = df_cat['target']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(
    X_cat, y_cat, test_size=0.2, random_state=42
)

# Specify categorical features
cat_features = ['category']

cat_clf_cat = CatBoostClassifier(
    iterations=100,
    depth=3,
    learning_rate=0.1,
    cat_features=cat_features,
    random_state=42,
    verbose=False
)

cat_clf_cat.fit(X_train_cat, y_train_cat)
y_pred_cat_feat = cat_clf_cat.predict(X_test_cat)
accuracy_cat_feat = accuracy_score(y_test_cat, y_pred_cat_feat)

print(f"CatBoost with Categorical Features Accuracy: {accuracy_cat_feat:.4f}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Regression with CatBoost
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat_reg = CatBoostRegressor(
    iterations=100,
    depth=3,
    learning_rate=0.1,
    random_state=42,
    verbose=False
)

cat_reg.fit(X_train_reg, y_train_reg)
y_pred_cat_reg = cat_reg.predict(X_test_reg)
rmse_cat = np.sqrt(mean_squared_error(y_test_reg, y_pred_cat_reg))

print(f"\nCatBoost Regressor RMSE: {rmse_cat:.2f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*60)
print("GRADIENT BOOSTING COMPARISON")
print("="*60)

comparison = pd.DataFrame({
    'Framework': ['XGBoost', 'LightGBM', 'CatBoost'],
    'Classification Accuracy': [accuracy, accuracy_lgb, accuracy_cat],
    'Regression RMSE': [rmse, rmse_lgb, rmse_cat]
})

print("\n", comparison.to_string(index=False))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYPERPARAMETER TUNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.model_selection import RandomizedSearchCV

print("\n" + "="*60)
print("HYPERPARAMETER TUNING - XGBOOST")
print("="*60)

param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

xgb_search = RandomizedSearchCV(
    XGBClassifier(random_state=42, eval_metric='logloss'),
    param_dist,
    n_iter=10,
    cv=3,
    random_state=42,
    n_jobs=-1
)

xgb_search.fit(X_train, y_train)

print(f"\nBest parameters: {xgb_search.best_params_}")
print(f"Best CV score: {xgb_search.best_score_:.4f}")
print(f"Test score: {xgb_search.score(X_test, y_test):.4f}")

print("\nâœ… Gradient Boosting complete!")
```

---

<div align="center">

## ğŸŒŸ Other Frameworks

</div>

### Specialized ML Frameworks ğŸ› ï¸

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OTHER POPULAR ML FRAMEWORKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. FASTAI - HIGH-LEVEL DEEP LEARNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install fastai

from fastai.vision.all import *

# Load data
path = untar_data(URLs.PETS)
dls = ImageDataLoaders.from_name_func(
    path, get_image_files(path/'images'),
    valid_pct=0.2,
    label_func=lambda x: x[0].isupper(),
    item_tfms=Resize(224)
)

# Create learner
learn = cnn_learner(dls, resnet34, metrics=error_rate)

# Train
learn.fine_tune(2)

# Predict
img = PILImage.create('dog.jpg')
pred, idx, probs = learn.predict(img)
print(f'Prediction: {pred}; Probability: {probs[idx]:.04f}')
"""

print("FastAI: Simplified deep learning API on top of PyTorch")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. HUGGING FACE TRANSFORMERS - NLP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install transformers

from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# Sentiment analysis
classifier = pipeline('sentiment-analysis')
result = classifier("I love this product!")
print(result)

# Text generation
generator = pipeline('text-generation', model='gpt2')
text = generator("Once upon a time", max_length=50)
print(text)

# Named Entity Recognition
ner = pipeline('ner', grouped_entities=True)
entities = ner("Elon Musk founded Tesla in California")
print(entities)

# Question Answering
qa = pipeline('question-answering')
context = "Python is a programming language"
question = "What is Python?"
answer = qa(question=question, context=context)
print(answer)
"""

print("\nHugging Face: State-of-the-art NLP with transformers")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. JAX - HIGH-PERFORMANCE ML
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install jax jaxlib

import jax
import jax.numpy as jnp
from jax import grad, jit, vmap

# Automatic differentiation
def f(x):
    return jnp.sum(x ** 2)

grad_f = grad(f)
print(grad_f(jnp.array([1.0, 2.0, 3.0])))

# JIT compilation
@jit
def fast_function(x):
    return jnp.sum(x ** 2)

# Vectorization
@vmap
def batch_function(x):
    return jnp.sum(x ** 2)
"""

print("JAX: High-performance numerical computing")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. OPTUNA - HYPERPARAMETER OPTIMIZATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install optuna

import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 2, 32)

    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=42
    )

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print(f'Best parameters: {study.best_params}')
print(f'Best accuracy: {study.best_value:.4f}')
"""

print("Optuna: Automated hyperparameter optimization")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. AUTOKERAS - AUTOML
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install autokeras

import autokeras as ak

# Image classification
clf = ak.ImageClassifier(max_trials=10)
clf.fit(x_train, y_train, epochs=10)
accuracy = clf.evaluate(x_test, y_test)

# Text classification
clf_text = ak.TextClassifier(max_trials=10)
clf_text.fit(x_text_train, y_text_train, epochs=10)
"""

print("AutoKeras: Automated machine learning")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. SHAP - MODEL INTERPRETATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install: pip install shap

import shap

# Train model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Explain predictions
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test)
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
"""

print("SHAP: Model interpretability and explainability")

print("\nâœ… Other frameworks overview complete!")
```

---

<div align="center">

## ğŸ“Š Framework Comparison

</div>

### Choosing the Right Framework ğŸ¯

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DETAILED FRAMEWORK COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   CLASSICAL ML FRAMEWORKS                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Aspect            | Scikit-learn | XGBoost   | LightGBM  | CatBoost |
| ----------------- | ------------ | --------- | --------- | -------- |
| **Speed**         | â­â­         | â­â­â­    | â­â­â­â­  | â­â­â­   |
| **Accuracy**      | â­â­â­       | â­â­â­â­  | â­â­â­â­  | â­â­â­â­ |
| **Memory**        | â­â­â­â­     | â­â­â­    | â­â­â­â­  | â­â­â­   |
| **Ease of Use**   | â­â­â­â­     | â­â­â­    | â­â­â­    | â­â­â­â­ |
| **Documentation** | â­â­â­â­     | â­â­â­â­  | â­â­â­    | â­â­â­   |
| **Small Data**    | âœ… Best      | âœ… Good   | âœ… Good   | âœ… Good  |
| **Large Data**    | âŒ Slow      | âœ… Good   | âœ… Best   | âœ… Good  |
| **Categorical**   | âš ï¸ Manual    | âš ï¸ Manual | âš ï¸ Manual | âœ… Auto  |
| **GPU Support**   | âŒ No        | âœ… Yes    | âœ… Yes    | âœ… Yes   |
| **Parallel**      | âœ… Yes       | âœ… Yes    | âœ… Yes    | âœ… Yes   |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   DEEP LEARNING FRAMEWORKS                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Aspect             | TensorFlow     | PyTorch        | Keras     | FastAI      |
| ------------------ | -------------- | -------------- | --------- | ----------- |
| **Learning Curve** | â­â­           | â­â­â­         | â­â­â­â­  | â­â­â­â­    |
| **Flexibility**    | â­â­â­â­       | â­â­â­â­       | â­â­â­    | â­â­        |
| **Production**     | â­â­â­â­       | â­â­â­         | â­â­â­â­  | â­â­        |
| **Research**       | â­â­â­         | â­â­â­â­       | â­â­      | â­â­â­      |
| **Community**      | â­â­â­â­       | â­â­â­â­       | â­â­â­â­  | â­â­â­      |
| **Mobile**         | âœ… TF Lite     | âš ï¸ Limited     | âœ… Via TF | âŒ No       |
| **Web**            | âœ… TF.js       | âš ï¸ Limited     | âœ… Via TF | âŒ No       |
| **Debugging**      | â­â­           | â­â­â­â­       | â­â­â­    | â­â­â­      |
| **Visualization**  | âœ… TensorBoard | âœ… TensorBoard | âœ… Good   | âœ… Built-in |
| **Pre-trained**    | âœ… TF Hub      | âœ… torchvision | âœ… Many   | âœ… Many     |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   USE CASE RECOMMENDATIONS                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tabular Data (Structured):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Small datasets (<10K rows):
  1st Choice: Scikit-learn
  2nd Choice: XGBoost
  Why: Simple, fast training, interpretable

Medium datasets (10K-100K):
  1st Choice: XGBoost
  2nd Choice: LightGBM
  Why: Better accuracy, reasonable speed

Large datasets (>100K rows):
  1st Choice: LightGBM
  2nd Choice: CatBoost
  Why: Fastest training, memory efficient

Many categorical features:
  1st Choice: CatBoost
  2nd Choice: LightGBM
  Why: Automatic handling, no encoding needed

Images:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Quick prototyping:
  1st Choice: FastAI
  2nd Choice: Keras
  Why: High-level API, quick results

Custom architectures:
  1st Choice: PyTorch
  2nd Choice: TensorFlow
  Why: Full control, easy debugging

Production deployment:
  1st Choice: TensorFlow
  2nd Choice: PyTorch
  Why: TF Serving, TF Lite, mature tools

Mobile/Edge devices:
  1st Choice: TensorFlow Lite
  2nd Choice: ONNX
  Why: Optimized for mobile, small size

Text/NLP:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Modern NLP (Transformers):
  1st Choice: Hugging Face
  2nd Choice: PyTorch
  Why: Pre-trained models, easy fine-tuning

Classical NLP:
  1st Choice: Scikit-learn
  2nd Choice: spaCy
  Why: Simple, fast, good for basic tasks

Custom models:
  1st Choice: PyTorch
  2nd Choice: TensorFlow
  Why: Flexibility, research-friendly

Time Series:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Traditional forecasting:
  1st Choice: Statsmodels
  2nd Choice: Prophet
  Why: Statistical methods, interpretable

Complex patterns:
  1st Choice: LightGBM
  2nd Choice: LSTM (PyTorch/TF)
  Why: Captures non-linear patterns

Large-scale:
  1st Choice: LightGBM
  2nd Choice: XGBoost
  Why: Fast, handles large datasets

Research:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1st Choice: PyTorch
2nd Choice: JAX
Why: Dynamic graphs, easy debugging, Pythonic

Industry/Production:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1st Choice: TensorFlow
2nd Choice: XGBoost
Why: Production tools, deployment, serving

Learning/Education:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1st Choice: Scikit-learn
2nd Choice: Keras
Why: Simple API, great documentation

Competitions (Kaggle):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1st Choice: XGBoost
2nd Choice: LightGBM
Why: Consistently win, ensemble-friendly

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   FRAMEWORK ECOSYSTEM                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TensorFlow Ecosystem:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ TensorFlow Core: Main framework
â€¢ Keras: High-level API
â€¢ TensorFlow Lite: Mobile deployment
â€¢ TensorFlow.js: Web deployment
â€¢ TensorFlow Serving: Production serving
â€¢ TensorFlow Extended (TFX): ML pipelines
â€¢ TensorBoard: Visualization

PyTorch Ecosystem:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ PyTorch Core: Main framework
â€¢ torchvision: Computer vision
â€¢ torchaudio: Audio processing
â€¢ torchtext: NLP utilities
â€¢ PyTorch Lightning: High-level wrapper
â€¢ TorchServe: Model serving
â€¢ ONNX: Model export

Scikit-learn Compatible:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ XGBoost: Gradient boosting
â€¢ LightGBM: Fast gradient boosting
â€¢ CatBoost: Categorical boosting
â€¢ Imbalanced-learn: Class imbalance
â€¢ Feature-engine: Feature engineering

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## ğŸš€ Model Deployment

</div>

### From Training to Production ğŸ­

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL DEPLOYMENT STRATEGIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. FLASK API (SIMPLE REST API)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# app.py
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# Load model at startup
model = joblib.load('model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    features = np.array(data['features']).reshape(1, -1)
    prediction = model.predict(features)
    probability = model.predict_proba(features)

    return jsonify({
        'prediction': int(prediction[0]),
        'probability': float(probability[0][1])
    })

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

# Run: python app.py
# Test: curl -X POST http://localhost:5000/predict -H "Content-Type: application/json" -d '{"features": [1, 2, 3, 4]}'
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. FASTAPI (MODERN ASYNC API)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# main.py
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI()

# Load model
model = joblib.load('model.pkl')

class PredictionInput(BaseModel):
    features: list

class PredictionOutput(BaseModel):
    prediction: int
    probability: float

@app.post('/predict', response_model=PredictionOutput)
async def predict(input_data: PredictionInput):
    features = np.array(input_data.features).reshape(1, -1)
    prediction = model.predict(features)
    probability = model.predict_proba(features)

    return {
        'prediction': int(prediction[0]),
        'probability': float(probability[0][1])
    }

@app.get('/health')
async def health():
    return {'status': 'healthy'}

# Run: uvicorn main:app --reload
# Docs: http://localhost:8000/docs
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. TENSORFLOW SERVING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Save model in TensorFlow SavedModel format
import tensorflow as tf

model.save('saved_model/1')  # Version 1

# Docker:
docker run -p 8501:8501 \
  --mount type=bind,source=/path/to/saved_model,target=/models/model \
  -e MODEL_NAME=model \
  tensorflow/serving

# Client request:
import requests
import json

data = json.dumps({
    'signature_name': 'serving_default',
    'instances': [[1.0, 2.0, 3.0, 4.0]]
})

headers = {'content-type': 'application/json'}
response = requests.post(
    'http://localhost:8501/v1/models/model:predict',
    data=data,
    headers=headers
)

predictions = response.json()
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. TORCHSERVE (PYTORCH SERVING)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Install TorchServe
pip install torchserve torch-model-archiver

# Archive model
torch-model-archiver \
  --model-name mnist \
  --version 1.0 \
  --model-file model.py \
  --serialized-file model.pth \
  --handler image_classifier

# Start server
torchserve --start --model-store model_store --models mnist=mnist.mar

# Predict
curl http://localhost:8080/predictions/mnist -T image.jpg
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. DOCKER DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]

# Build:
docker build -t ml-model-api .

# Run:
docker run -p 5000:5000 ml-model-api

# Docker Compose (docker-compose.yml):
version: '3.8'
services:
  api:
    build: .
    ports:
      - "5000:5000"
    environment:
      - MODEL_PATH=/models/model.pkl
    volumes:
      - ./models:/models
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. CLOUD DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# AWS SageMaker:
import sagemaker
from sagemaker.sklearn import SKLearnModel

sklearn_model = SKLearnModel(
    model_data='s3://bucket/model.tar.gz',
    role=role,
    entry_point='inference.py',
    framework_version='0.23-1'
)

predictor = sklearn_model.deploy(
    initial_instance_count=1,
    instance_type='ml.m5.large'
)

# Google Cloud AI Platform:
gcloud ai-platform models create model_name \
  --regions us-central1

gcloud ai-platform versions create v1 \
  --model model_name \
  --runtime-version 2.8 \
  --python-version 3.7 \
  --framework scikit-learn \
  --package-uris gs://bucket/model.tar.gz

# Azure ML:
from azureml.core import Workspace, Model

ws = Workspace.from_config()

model = Model.register(
    workspace=ws,
    model_path='model.pkl',
    model_name='my-model'
)

service = Model.deploy(
    workspace=ws,
    name='my-service',
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config
)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. EDGE DEPLOYMENT (MOBILE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# TensorFlow Lite conversion:
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

# ONNX export (PyTorch):
import torch.onnx

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(
    model,
    dummy_input,
    'model.onnx',
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)

# Core ML (iOS):
import coremltools as ct

coreml_model = ct.convert(
    model,
    inputs=[ct.ImageType(shape=(1, 224, 224, 3))]
)
coreml_model.save('model.mlmodel')
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. BATCH PREDICTION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# batch_predict.py
import pandas as pd
import joblib

def batch_predict(input_file, output_file, model_path):
    # Load model
    model = joblib.load(model_path)

    # Load data
    df = pd.read_csv(input_file)

    # Make predictions
    X = df.drop(['id'], axis=1)
    predictions = model.predict(X)
    probabilities = model.predict_proba(X)

    # Save results
    results = pd.DataFrame({
        'id': df['id'],
        'prediction': predictions,
        'probability': probabilities[:, 1]
    })

    results.to_csv(output_file, index=False)
    print(f'Predictions saved to {output_file}')

if __name__ == '__main__':
    batch_predict('input.csv', 'output.csv', 'model.pkl')

# Airflow DAG for scheduling:
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'batch_prediction',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily'
)

predict_task = PythonOperator(
    task_id='batch_predict',
    python_callable=batch_predict,
    dag=dag
)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9. MODEL MONITORING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# monitoring.py
import logging
from prometheus_client import Counter, Histogram
import time

# Metrics
prediction_counter = Counter('predictions_total', 'Total predictions')
prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency')
prediction_errors = Counter('prediction_errors_total', 'Total prediction errors')

def predict_with_monitoring(features):
    prediction_counter.inc()

    start_time = time.time()
    try:
        prediction = model.predict(features)
        latency = time.time() - start_time
        prediction_latency.observe(latency)

        # Log prediction
        logging.info(f'Prediction: {prediction}, Latency: {latency:.3f}s')

        return prediction
    except Exception as e:
        prediction_errors.inc()
        logging.error(f'Prediction error: {str(e)}')
        raise

# Data drift detection:
from scipy.stats import ks_2samp

def check_data_drift(reference_data, new_data, threshold=0.05):
    drift_detected = {}

    for column in reference_data.columns:
        statistic, p_value = ks_2samp(reference_data[column], new_data[column])
        drift_detected[column] = p_value < threshold

        if drift_detected[column]:
            logging.warning(f'Data drift detected in column: {column}')

    return drift_detected
"""

print("\nâœ… Model deployment strategies complete!")
```

---

<div align="center">

## ğŸ’¡ Best Practices

</div>

### Professional ML Development ğŸ“‹

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ML DEVELOPMENT BEST PRACTICES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   PROJECT ORGANIZATION                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Recommended Structure:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ml-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original, immutable data
â”‚   â”œâ”€â”€ processed/        # Cleaned, processed data
â”‚   â””â”€â”€ features/         # Feature engineered data
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_baseline.ipynb
â”‚   â””â”€â”€ 03_experiments.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ engineering.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ models/               # Trained models
â”œâ”€â”€ reports/              # Reports and visualizations
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   DATA MANAGEMENT                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… DO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Keep raw data immutable
â€¢ Version your datasets (DVC, Git LFS)
â€¢ Document data sources and transformations
â€¢ Validate data quality
â€¢ Handle missing values explicitly
â€¢ Split data properly (train/val/test)
â€¢ Use stratified splits for imbalanced data
â€¢ Set random seeds for reproducibility
â€¢ Check for data leakage
â€¢ Monitor data drift in production

âŒ DON'T:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Modify raw data files
â€¢ Mix train and test data
â€¢ Forget to scale/normalize
â€¢ Ignore class imbalance
â€¢ Overfit to validation set
â€¢ Use test set for development

Example Data Pipeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib

def prepare_data(data_path, test_size=0.2, random_state=42):
    # Load data
    df = pd.read_csv(data_path)

    # Validate
    assert not df.isnull().all().any(), "Column with all NaN"
    assert len(df) > 0, "Empty dataset"

    # Split features and target
    X = df.drop('target', axis=1)
    y = df['target']

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size,
        random_state=random_state,
        stratify=y  # Maintain class distribution
    )

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Save scaler
    joblib.dump(scaler, 'scaler.pkl')

    return X_train_scaled, X_test_scaled, y_train, y_test

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   MODEL DEVELOPMENT                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Development Workflow:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Start with baseline model (simple)
2. Establish evaluation metrics
3. Experiment with features
4. Try different algorithms
5. Tune hyperparameters
6. Ensemble models
7. Evaluate on test set (once!)

âœ… Best Practices:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Use cross-validation
â€¢ Track experiments (MLflow, Weights & Biases)
â€¢ Compare multiple models
â€¢ Tune hyperparameters systematically
â€¢ Use early stopping
â€¢ Save best models
â€¢ Document model architecture
â€¢ Version control models
â€¢ Test on holdout set last
â€¢ Monitor training curves

Example Training Pipeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import mlflow

def train_model(X_train, y_train, params):
    with mlflow.start_run():
        # Log parameters
        mlflow.log_params(params)

        # Train model
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)

        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)

        # Log metrics
        mlflow.log_metric('cv_mean', cv_scores.mean())
        mlflow.log_metric('cv_std', cv_scores.std())

        # Save model
        mlflow.sklearn.log_model(model, 'model')

        return model, cv_scores

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   MODEL EVALUATION                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Evaluation Checklist:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Multiple metrics (accuracy, precision, recall, F1, AUC)
â˜ Confusion matrix
â˜ ROC/PR curves
â˜ Cross-validation scores
â˜ Feature importance
â˜ Error analysis
â˜ Bias/fairness checks
â˜ Performance across subgroups
â˜ Computational cost
â˜ Inference time

Example Evaluation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sklearn.metrics import (classification_report,
                             confusion_matrix, roc_auc_score)
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(model, X_test, y_test):
    # Predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Metrics
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('confusion_matrix.png')

    # AUC-ROC
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nAUC-ROC: {auc:.4f}")

    # Feature Importance
    if hasattr(model, 'feature_importances_'):
        importance = pd.DataFrame({
            'feature': X_test.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nTop 10 Features:")
        print(importance.head(10))

    return {
        'auc': auc,
        'confusion_matrix': cm,
        'predictions': y_pred
    }

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   CODE QUALITY                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Write Clean Code:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Use meaningful variable names
â€¢ Write functions (DRY principle)
â€¢ Add docstrings
â€¢ Use type hints
â€¢ Follow PEP 8
â€¢ Use linters (pylint, flake8)
â€¢ Format code (black)
â€¢ Write unit tests
â€¢ Use version control (git)

Example Clean Code:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import Tuple
import numpy as np
from sklearn.base import BaseEstimator

def train_and_evaluate(
    model: BaseEstimator,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray
) -> Tuple[BaseEstimator, float]:
    """
    Train model and evaluate on test set.

    Parameters
    ----------
    model : BaseEstimator
        Scikit-learn compatible model
    X_train : np.ndarray
        Training features
    y_train : np.ndarray
        Training labels
    X_test : np.ndarray
        Test features
    y_test : np.ndarray
        Test labels

    Returns
    -------
    Tuple[BaseEstimator, float]
        Trained model and test accuracy
    """
    # Train
    model.fit(X_train, y_train)

    # Evaluate
    accuracy = model.score(X_test, y_test)

    return model, accuracy

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   REPRODUCIBILITY                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ensure Reproducibility:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Set random seeds
2. Version dependencies (requirements.txt)
3. Document environment
4. Version data
5. Track experiments
6. Save configurations
7. Document hardware used

Example:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import random
import numpy as np
import torch

def set_seeds(seed=42):
    """Set seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# requirements.txt with versions
"""
scikit-learn==1.3.0
xgboost==2.0.0
numpy==1.24.3
pandas==2.0.3
torch==2.0.1
"""

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   SECURITY                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Security Best Practices:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Sanitize user inputs
âœ… Validate data types and ranges
âœ… Use environment variables for secrets
âœ… Don't commit sensitive data
âœ… Implement rate limiting
âœ… Log access and errors
âœ… Use HTTPS for API
âœ… Validate model outputs
âœ… Monitor for adversarial inputs
âœ… Regular security audits

Example Input Validation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_input(features, expected_shape):
    """Validate input features."""
    if not isinstance(features, np.ndarray):
        raise ValueError("Features must be numpy array")

    if features.shape[1] != expected_shape:
        raise ValueError(f"Expected {expected_shape} features")

    if np.isnan(features).any():
        raise ValueError("Features contain NaN")

    if np.isinf(features).any():
        raise ValueError("Features contain infinity")

    return True

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## ğŸ”§ Real-World Projects

</div>

### End-to-End ML Projects ğŸ’¼

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROJECT 1: IMAGE CLASSIFICATION PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Complete Image Classification Project
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Build production-ready image classifier
Dataset: CIFAR-10 (60K images, 10 classes)
Framework: PyTorch
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import pytorch_lightning as pl

# 1. Data Module
class CIFAR10DataModule(pl.LightningDataModule):
    def __init__(self, batch_size=128):
        super().__init__()
        self.batch_size = batch_size
        self.transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        self.transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

    def setup(self, stage=None):
        self.train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True,
            transform=self.transform_train
        )
        self.test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True,
            transform=self.transform_test
        )

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size,
                         shuffle=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size,
                         num_workers=4)

# 2. Model
class ResNetClassifier(pl.LightningModule):
    def __init__(self, num_classes=10):
        super().__init__()
        self.model = torchvision.models.resnet18(pretrained=True)
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()

        self.log('train_loss', loss)
        self.log('train_acc', acc)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        acc = (logits.argmax(dim=1) == y).float().mean()

        self.log('val_loss', loss)
        self.log('val_acc', acc)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, patience=3
        )
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_loss'
        }

# 3. Training
data_module = CIFAR10DataModule(batch_size=128)
model = ResNetClassifier(num_classes=10)

trainer = pl.Trainer(
    max_epochs=10,
    accelerator='auto',
    callbacks=[
        pl.callbacks.EarlyStopping(monitor='val_loss', patience=5),
        pl.callbacks.ModelCheckpoint(monitor='val_acc', mode='max')
    ]
)

# trainer.fit(model, data_module)

print("âœ… Image Classification Project Template Ready")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROJECT 2: CUSTOMER CHURN PREDICTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Tabular Data Classification
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Predict customer churn
Framework: XGBoost
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

class ChurnPredictor:
    def __init__(self):
        self.model = None
        self.encoders = {}
        self.feature_names = None

    def preprocess(self, df, fit=True):
        # Handle categorical features
        categorical_cols = df.select_dtypes(include='object').columns

        for col in categorical_cols:
            if fit:
                self.encoders[col] = LabelEncoder()
                df[col] = self.encoders[col].fit_transform(df[col])
            else:
                df[col] = self.encoders[col].transform(df[col])

        return df

    def train(self, X_train, y_train, X_val, y_val):
        # Preprocess
        X_train = self.preprocess(X_train, fit=True)
        X_val = self.preprocess(X_val, fit=False)

        self.feature_names = X_train.columns.tolist()

        # Train XGBoost
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

        params = {
            'objective': 'binary:logistic',
            'max_depth': 5,
            'learning_rate': 0.1,
            'eval_metric': 'auc'
        }

        evals = [(dtrain, 'train'), (dval, 'val')]

        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            evals=evals,
            early_stopping_rounds=10,
            verbose_eval=False
        )

        return self

    def predict(self, X):
        X = self.preprocess(X, fit=False)
        dtest = xgb.DMatrix(X)
        return self.model.predict(dtest)

    def get_feature_importance(self):
        importance = self.model.get_score(importance_type='gain')
        return pd.DataFrame({
            'feature': importance.keys(),
            'importance': importance.values()
        }).sort_values('importance', ascending=False)

print("âœ… Churn Prediction Project Template Ready")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROJECT 3: NLP SENTIMENT ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Text Classification with Transformers
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Goal: Sentiment analysis
Framework: Hugging Face Transformers
"""

"""
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                         Trainer, TrainingArguments)
from datasets import load_dataset
import numpy as np

# 1. Load Data
dataset = load_dataset('imdb')

# 2. Load Pre-trained Model
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=2
)

# 3. Tokenize
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length',
                    truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 4. Training Arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    evaluation_strategy='epoch'
)

# 5. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

# 6. Train
trainer.train()

# 7. Evaluate
results = trainer.evaluate()
print(results)
"""

print("âœ… NLP Sentiment Analysis Project Template Ready")

print("\n" + "="*60)
print("All project templates ready for implementation!")
print("="*60)
```

---

<div align="center">

## ğŸ“š Resources & Learning

</div>

### Continue Your ML Journey ğŸš€

```
ğŸ“˜ Official Documentation
   â€¢ Scikit-learn: https://scikit-learn.org/
   â€¢ TensorFlow: https://www.tensorflow.org/
   â€¢ PyTorch: https://pytorch.org/
   â€¢ XGBoost: https://xgboost.readthedocs.io/
   â€¢ LightGBM: https://lightgbm.readthedocs.io/
   â€¢ CatBoost: https://catboost.ai/

ğŸ“— Books
   â€¢ Hands-On Machine Learning (AurÃ©lien GÃ©ron)
   â€¢ Deep Learning (Goodfellow, Bengio, Courville)
   â€¢ Pattern Recognition and Machine Learning (Bishop)
   â€¢ The Hundred-Page Machine Learning Book (Andriy Burkov)

ğŸ“™ Online Courses
   â€¢ FastAI Practical Deep Learning
   â€¢ Coursera ML Specialization (Andrew Ng)
   â€¢ Deep Learning Specialization
   â€¢ Full Stack Deep Learning

ğŸ¥ YouTube Channels
   â€¢ Yannic Kilcher (Paper reviews)
   â€¢ Two Minute Papers
   â€¢ Sentdex
   â€¢ StatQuest

ğŸ’» Practice Platforms
   â€¢ Kaggle: https://www.kaggle.com/
   â€¢ DrivenData: https://www.drivendata.org/
   â€¢ Papers with Code: https://paperswithcode.com/

ğŸ› ï¸ Tools
   â€¢ MLflow: Experiment tracking
   â€¢ Weights & Biases: Experiment tracking
   â€¢ DVC: Data version control
   â€¢ Optuna: Hyperparameter optimization
   â€¢ SHAP: Model interpretability

ğŸ’¬ Communities
   â€¢ r/MachineLearning
   â€¢ r/learnmachinelearning
   â€¢ AI Alignment Forum
   â€¢ Papers with Code
   â€¢ Discord ML communities
```

---

<div align="center">

## ğŸ¯ Summary

</div>

### Key Takeaways ğŸ’¡

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   REMEMBER                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Choose the Right Framework
   â€¢ Scikit-learn: Classical ML, tabular data
   â€¢ XGBoost/LightGBM: Structured data, competitions
   â€¢ TensorFlow: Production, deployment
   â€¢ PyTorch: Research, flexibility

2. Start Simple
   â€¢ Baseline model first
   â€¢ Add complexity gradually
   â€¢ Evaluate thoroughly
   â€¢ Document everything

3. Focus on Data
   â€¢ Quality > Quantity
   â€¢ Feature engineering matters
   â€¢ Validate continuously
   â€¢ Monitor drift

4. Best Practices
   â€¢ Version control everything
   â€¢ Write clean code
   â€¢ Test your code
   â€¢ Track experiments
   â€¢ Reproduce results

5. Production Readiness
   â€¢ API design
   â€¢ Monitoring
   â€¢ Logging
   â€¢ Error handling
   â€¢ Security

"The best model is the one that solves the business problem,
not the one with the highest accuracy." âœ¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

**Built with ğŸ¤– by MrDib, for ML practitioners**

_Remember: "In ML, the model is 10%, the data is 90%"_ ğŸ“Š

**Happy Machine Learning!** ğŸš€
