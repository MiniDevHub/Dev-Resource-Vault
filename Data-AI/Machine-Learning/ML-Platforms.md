<div align="center">

# ğŸš€ ML Platforms - Complete Guide

![ML Platforms](https://img.shields.io/badge/ML-Platforms-purple?style=for-the-badge&logo=databricks)
![Cloud](https://img.shields.io/badge/Cloud-ML-blue?style=for-the-badge&logo=amazonaws)
![Level](https://img.shields.io/badge/Level-Intermediate-orange?style=for-the-badge)

### _From experimentation to production - streamline your ML workflow_ âš¡

**Build, deploy, and manage ML at scale** ğŸŒŸ

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Platform Overview](#-platform-overview)
- [â˜ï¸ Cloud ML Platforms](#ï¸-cloud-ml-platforms)
- [ğŸ¤– AutoML Platforms](#-automl-platforms)
- [ğŸ”„ MLOps Platforms](#-mlops-platforms)
- [ğŸ“Š Experiment Tracking](#-experiment-tracking)
- [ğŸ¯ Feature Stores](#-feature-stores)
- [ğŸš€ Model Serving](#-model-serving)
- [ğŸ¢ End-to-End Platforms](#-end-to-end-platforms)
- [ğŸ“Š Platform Comparison](#-platform-comparison)
- [ğŸ’¡ Best Practices](#-best-practices)
- [ğŸ”§ Implementation Guide](#-implementation-guide)

---

<div align="center">

## ğŸ¯ Platform Overview

</div>

### Understanding ML Platforms ğŸ“–

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WHAT ARE ML PLATFORMS?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ML PLATFORM DEFINITION                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ML Platforms: Integrated environments for end-to-end ML lifecycle
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Streamline ML workflows
â€¢ Manage experiments
â€¢ Version models and data
â€¢ Deploy at scale
â€¢ Monitor performance
â€¢ Collaborate across teams

The ML Lifecycle:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DATA                                                â”‚
â”‚     â”œâ”€ Collection                                       â”‚
â”‚     â”œâ”€ Labeling                                        â”‚
â”‚     â”œâ”€ Versioning                                      â”‚
â”‚     â””â”€ Storage                                         â”‚
â”‚                                                         â”‚
â”‚  2. EXPERIMENTATION                                     â”‚
â”‚     â”œâ”€ Feature engineering                             â”‚
â”‚     â”œâ”€ Model training                                  â”‚
â”‚     â”œâ”€ Hyperparameter tuning                          â”‚
â”‚     â””â”€ Experiment tracking                            â”‚
â”‚                                                         â”‚
â”‚  3. DEPLOYMENT                                          â”‚
â”‚     â”œâ”€ Model serving                                   â”‚
â”‚     â”œâ”€ A/B testing                                     â”‚
â”‚     â”œâ”€ Canary releases                                â”‚
â”‚     â””â”€ Monitoring                                      â”‚
â”‚                                                         â”‚
â”‚  4. MONITORING & MAINTENANCE                            â”‚
â”‚     â”œâ”€ Performance tracking                            â”‚
â”‚     â”œâ”€ Data drift detection                           â”‚
â”‚     â”œâ”€ Model retraining                               â”‚
â”‚     â””â”€ Feedback loops                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   PLATFORM CATEGORIES                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Cloud ML Platforms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ AWS SageMaker
   â€¢ Google Vertex AI
   â€¢ Azure Machine Learning
   â€¢ IBM Watson Studio

2. AutoML Platforms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ H2O.ai
   â€¢ DataRobot
   â€¢ Google AutoML
   â€¢ AutoGluon

3. MLOps Platforms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ MLflow
   â€¢ Kubeflow
   â€¢ Seldon Core
   â€¢ BentoML

4. Experiment Tracking
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Weights & Biases
   â€¢ Neptune.ai
   â€¢ Comet.ml
   â€¢ MLflow Tracking

5. Feature Stores
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Feast
   â€¢ Tecton
   â€¢ Hopsworks
   â€¢ AWS Feature Store

6. Model Serving
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ TensorFlow Serving
   â€¢ TorchServe
   â€¢ NVIDIA Triton
   â€¢ Seldon Core

7. End-to-End Platforms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Databricks
   â€¢ Dataiku
   â€¢ Domino Data Lab
   â€¢ Alteryx

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   WHY USE ML PLATFORMS?                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Benefits:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Faster time to production
âœ… Standardized workflows
âœ… Collaboration tools
âœ… Version control (models, data, code)
âœ… Scalable infrastructure
âœ… Cost optimization
âœ… Monitoring & observability
âœ… Compliance & governance
âœ… Reproducibility
âœ… Team productivity

Challenges Without Platforms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âŒ Manual setup & configuration
âŒ Inconsistent environments
âŒ Lost experiments
âŒ Deployment complexity
âŒ Monitoring gaps
âŒ Scaling difficulties
âŒ Team silos
âŒ Version control issues

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## â˜ï¸ Cloud ML Platforms

</div>

### Enterprise-Grade ML in the Cloud ğŸŒ©ï¸

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AWS SAGEMAKER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
AWS SageMaker: Fully managed ML service
- Notebooks (Jupyter)
- Training (distributed)
- Hyperparameter tuning
- Model deployment
- Monitoring
- Feature Store
- Pipelines
"""

import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn import SKLearn
from sagemaker.tensorflow import TensorFlow
import pandas as pd

print("="*60)
print("AWS SAGEMAKER")
print("="*60)

# Initialize
role = get_execution_role()
sagemaker_session = sagemaker.Session()
bucket = sagemaker_session.default_bucket()

print(f"SageMaker Role: {role}")
print(f"Default Bucket: {bucket}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. TRAINING WITH SAGEMAKER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# train.py (training script)
import argparse
import os
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # Hyperparameters
    parser.add_argument('--n-estimators', type=int, default=100)
    parser.add_argument('--max-depth', type=int, default=5)

    # Data directories
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))

    args = parser.parse_args()

    # Load data
    train_data = pd.read_csv(f'{args.train}/train.csv')
    X_train = train_data.drop('target', axis=1)
    y_train = train_data['target']

    # Train model
    model = RandomForestClassifier(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth
    )
    model.fit(X_train, y_train)

    # Save model
    joblib.dump(model, f'{args.model_dir}/model.joblib')
"""

# Create estimator
sklearn_estimator = SKLearn(
    entry_point='train.py',
    role=role,
    instance_type='ml.m5.xlarge',
    framework_version='1.0-1',
    hyperparameters={
        'n-estimators': 100,
        'max-depth': 5
    }
)

# Training (example - would upload data first)
# sklearn_estimator.fit({'train': 's3://bucket/train-data'})

print("\nâœ… SageMaker Training Example")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. HYPERPARAMETER TUNING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sagemaker.tuner import (HyperparameterTuner, IntegerParameter,
                             ContinuousParameter)

hyperparameter_ranges = {
    'n-estimators': IntegerParameter(50, 300),
    'max-depth': IntegerParameter(3, 15)
}

tuner = HyperparameterTuner(
    sklearn_estimator,
    objective_metric_name='accuracy',
    hyperparameter_ranges=hyperparameter_ranges,
    max_jobs=10,
    max_parallel_jobs=2
)

# tuner.fit({'train': 's3://bucket/train-data'})

print("âœ… Hyperparameter Tuning Configured")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. MODEL DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Deploy model
# predictor = sklearn_estimator.deploy(
#     initial_instance_count=1,
#     instance_type='ml.t2.medium'
# )

# Make predictions
# predictions = predictor.predict(test_data)

# Delete endpoint
# predictor.delete_endpoint()

print("âœ… SageMaker Deployment Example")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. SAGEMAKER PIPELINES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import TrainingStep, ProcessingStep
from sagemaker.workflow.parameters import ParameterInteger

"""
# Define pipeline
n_estimators = ParameterInteger(name="NumEstimators", default_value=100)

# Processing step
processing_step = ProcessingStep(
    name="PreprocessData",
    processor=sklearn_processor,
    inputs=[...],
    outputs=[...],
    code="preprocess.py"
)

# Training step
training_step = TrainingStep(
    name="TrainModel",
    estimator=sklearn_estimator,
    inputs={'train': processing_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri}
)

# Create pipeline
pipeline = Pipeline(
    name="MyMLPipeline",
    parameters=[n_estimators],
    steps=[processing_step, training_step]
)

# Start pipeline
pipeline.upsert(role_arn=role)
execution = pipeline.start()
"""

print("âœ… SageMaker Pipelines Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GOOGLE VERTEX AI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Google Vertex AI: Unified ML platform
- AutoML
- Custom training
- Hyperparameter tuning
- Model deployment
- Feature Store
- Pipelines
- Monitoring
"""

print("\n" + "="*60)
print("GOOGLE VERTEX AI")
print("="*60)

"""
from google.cloud import aiplatform

# Initialize
aiplatform.init(
    project='my-project',
    location='us-central1'
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CUSTOM TRAINING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

job = aiplatform.CustomTrainingJob(
    display_name='my-training-job',
    script_path='train.py',
    container_uri='gcr.io/cloud-aiplatform/training/tf-cpu.2-8:latest',
    requirements=['pandas', 'scikit-learn'],
    model_serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest'
)

# Run training
model = job.run(
    dataset=dataset,
    replica_count=1,
    machine_type='n1-standard-4',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. AUTOML
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

dataset = aiplatform.TabularDataset.create(
    display_name='my-dataset',
    gcs_source=['gs://bucket/data.csv']
)

job = aiplatform.AutoMLTabularTrainingJob(
    display_name='automl-training',
    optimization_prediction_type='classification',
    optimization_objective='maximize-au-prc'
)

model = job.run(
    dataset=dataset,
    target_column='target',
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    budget_milli_node_hours=1000,
    model_display_name='automl-model'
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. MODEL DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

endpoint = model.deploy(
    deployed_model_display_name='my-model',
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=3
)

# Make predictions
predictions = endpoint.predict(instances=[[1, 2, 3, 4]])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. VERTEX AI PIPELINES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from kfp.v2 import dsl
from kfp.v2.dsl import component

@component
def preprocess_data(input_path: str) -> str:
    # Preprocessing logic
    return output_path

@component
def train_model(data_path: str) -> str:
    # Training logic
    return model_path

@dsl.pipeline(name='my-pipeline')
def ml_pipeline():
    preprocess_task = preprocess_data(input_path='gs://bucket/data.csv')
    train_task = train_model(data_path=preprocess_task.output)

# Compile and run
from kfp.v2 import compiler
compiler.Compiler().compile(
    pipeline_func=ml_pipeline,
    package_path='pipeline.json'
)

job = aiplatform.PipelineJob(
    display_name='my-pipeline-job',
    template_path='pipeline.json'
)
job.run()
"""

print("âœ… Vertex AI Examples")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AZURE MACHINE LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Azure ML: Microsoft's ML platform
- Notebooks
- Automated ML
- Designer (drag-and-drop)
- Training (compute clusters)
- Deployment
- Monitoring
- MLOps
"""

print("\n" + "="*60)
print("AZURE MACHINE LEARNING")
print("="*60)

"""
from azureml.core import Workspace, Experiment, Environment
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.train.sklearn import SKLearn

# Connect to workspace
ws = Workspace.from_config()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. CREATE COMPUTE CLUSTER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

compute_config = AmlCompute.provisioning_configuration(
    vm_size='STANDARD_D2_V2',
    max_nodes=4,
    idle_seconds_before_scaledown=300
)

compute_target = ComputeTarget.create(
    ws, 'cpu-cluster', compute_config
)
compute_target.wait_for_completion(show_output=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. TRAINING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

experiment = Experiment(workspace=ws, name='my-experiment')

estimator = SKLearn(
    source_directory='./src',
    entry_script='train.py',
    compute_target=compute_target,
    pip_packages=['pandas', 'scikit-learn']
)

run = experiment.submit(estimator)
run.wait_for_completion(show_output=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. AUTOMATED ML
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from azureml.train.automl import AutoMLConfig

automl_config = AutoMLConfig(
    task='classification',
    primary_metric='AUC_weighted',
    training_data=dataset,
    label_column_name='target',
    n_cross_validations=5,
    enable_early_stopping=True,
    experiment_timeout_hours=1,
    max_concurrent_iterations=4
)

automl_run = experiment.submit(automl_config, show_output=True)
best_run, fitted_model = automl_run.get_output()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. MODEL DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from azureml.core.model import InferenceConfig, Model
from azureml.core.webservice import AciWebservice

# Register model
model = run.register_model(
    model_name='sklearn-model',
    model_path='outputs/model.pkl'
)

# Inference configuration
inference_config = InferenceConfig(
    entry_script='score.py',
    environment=environment
)

# Deployment configuration
deployment_config = AciWebservice.deploy_configuration(
    cpu_cores=1,
    memory_gb=1
)

# Deploy
service = Model.deploy(
    workspace=ws,
    name='my-service',
    models=[model],
    inference_config=inference_config,
    deployment_config=deployment_config
)

service.wait_for_deployment(show_output=True)

# Test service
import json
test_data = json.dumps({'data': [[1, 2, 3, 4]]})
prediction = service.run(test_data)
"""

print("âœ… Azure ML Examples")

print("\n" + "="*60)
print("Cloud ML Platforms Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ¤– AutoML Platforms

</div>

### Automated Machine Learning ğŸ”®

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTOML PLATFORMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
AutoML: Automated machine learning
- Automated feature engineering
- Model selection
- Hyperparameter tuning
- Ensemble methods
- No-code/low-code
"""

print("="*60)
print("AUTOML PLATFORMS")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2O.AI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
H2O.ai: Open-source AutoML platform
- H2O AutoML
- Driverless AI (commercial)
- Model interpretability
- Production deployment
"""

print("\n" + "="*60)
print("H2O.AI")
print("="*60)

# Install: pip install h2o

import h2o
from h2o.automl import H2OAutoML
import pandas as pd

# Initialize H2O
# h2o.init()

"""
# Load data
df = pd.read_csv('data.csv')
h2o_df = h2o.H2OFrame(df)

# Split data
train, test = h2o_df.split_frame(ratios=[0.8])

# Define features and target
x = h2o_df.columns
x.remove('target')
y = 'target'

# Run AutoML
aml = H2OAutoML(
    max_models=20,
    max_runtime_secs=3600,  # 1 hour
    seed=42
)

aml.train(x=x, y=y, training_frame=train)

# View leaderboard
lb = aml.leaderboard
print(lb.head())

# Best model
best_model = aml.leader

# Predictions
predictions = best_model.predict(test)

# Model explanation
best_model.explain(test)

# Save model
h2o.save_model(best_model, path='models/')

# Load model
loaded_model = h2o.load_model('models/best_model')
"""

print("âœ… H2O AutoML Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTOGLUON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
AutoGluon: AutoML for deep learning and tabular data
- Tabular, text, image
- Stacking/ensembling
- Fast and accurate
"""

print("\n" + "="*60)
print("AUTOGLUON")
print("="*60)

# Install: pip install autogluon

from autogluon.tabular import TabularDataset, TabularPredictor
import pandas as pd

"""
# Load data
train_data = TabularDataset('train.csv')
test_data = TabularDataset('test.csv')

# Define target
label = 'target'

# Train AutoML
predictor = TabularPredictor(
    label=label,
    eval_metric='accuracy',
    path='autogluon_models'
).fit(
    train_data=train_data,
    time_limit=3600,  # 1 hour
    presets='best_quality'  # or 'medium_quality', 'optimize_for_deployment'
)

# Leaderboard
leaderboard = predictor.leaderboard(test_data, silent=True)
print(leaderboard)

# Predictions
predictions = predictor.predict(test_data)
probabilities = predictor.predict_proba(test_data)

# Feature importance
feature_importance = predictor.feature_importance(test_data)
print(feature_importance)

# Evaluate
performance = predictor.evaluate(test_data)
print(performance)
"""

print("âœ… AutoGluon Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYCARET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
PyCaret: Low-code ML library
- Classification, regression, clustering
- Model comparison
- Hyperparameter tuning
- Model deployment
"""

print("\n" + "="*60)
print("PYCARET")
print("="*60)

# Install: pip install pycaret

from pycaret.classification import *
import pandas as pd

"""
# Load data
data = pd.read_csv('data.csv')

# Setup
clf = setup(
    data=data,
    target='target',
    session_id=42,
    log_experiment=True,
    experiment_name='my_experiment'
)

# Compare models
best_model = compare_models()

# Create model
rf = create_model('rf')

# Tune model
tuned_rf = tune_model(rf)

# Ensemble model
bagged_rf = ensemble_model(tuned_rf, method='Bagging')

# Plot model
plot_model(tuned_rf, plot='auc')
plot_model(tuned_rf, plot='confusion_matrix')
plot_model(tuned_rf, plot='feature')

# Predict
predictions = predict_model(tuned_rf, data=test_data)

# Save model
save_model(tuned_rf, 'my_model')

# Load model
loaded_model = load_model('my_model')

# Deploy (creates API)
deploy_model(tuned_rf, model_name='my_api', platform='aws')
"""

print("âœ… PyCaret Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TPOT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
TPOT: Tree-based Pipeline Optimization Tool
- Genetic programming
- Pipeline optimization
- Scikit-learn compatible
"""

print("\n" + "="*60)
print("TPOT")
print("="*60)

# Install: pip install tpot

from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

"""
# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Run TPOT
tpot = TPOTClassifier(
    generations=5,
    population_size=50,
    verbosity=2,
    random_state=42,
    n_jobs=-1,
    scoring='accuracy'
)

tpot.fit(X_train, y_train)

# Best pipeline
print(tpot.fitted_pipeline_)

# Evaluate
accuracy = tpot.score(X_test, y_test)
print(f'Accuracy: {accuracy:.4f}')

# Export pipeline
tpot.export('tpot_pipeline.py')

# The exported pipeline can be used like:
# from tpot_pipeline import *
# pipeline.fit(X_train, y_train)
# predictions = pipeline.predict(X_test)
"""

print("âœ… TPOT Example")

print("\n" + "="*60)
print("AutoML Platforms Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ”„ MLOps Platforms

</div>

### End-to-End ML Operations ğŸ”§

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MLOPS PLATFORMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
MLOps: Machine Learning Operations
- Versioning (models, data, code)
- Reproducibility
- CI/CD for ML
- Monitoring
- Collaboration
"""

print("="*60)
print("MLOPS PLATFORMS")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MLFLOW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
MLflow: Open-source ML lifecycle platform
- Tracking: Log experiments
- Projects: Package code
- Models: Package and deploy
- Registry: Manage models
"""

print("\n" + "="*60)
print("MLFLOW")
print("="*60)

# Install: pip install mlflow

import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. EXPERIMENT TRACKING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Set experiment
mlflow.set_experiment("my-experiment")

# Start run
with mlflow.start_run():
    # Log parameters
    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42
    }
    mlflow.log_params(params)

    # Train model
    model = RandomForestClassifier(**params)
    # model.fit(X_train, y_train)

    # Log metrics
    # accuracy = accuracy_score(y_test, model.predict(X_test))
    # mlflow.log_metric('accuracy', accuracy)

    # Log model
    # mlflow.sklearn.log_model(model, 'model')

    # Log artifacts
    # mlflow.log_artifact('plots/confusion_matrix.png')

    print("âœ… Experiment tracked")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. MODEL REGISTRY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Register model
model_uri = f'runs:/{run_id}/model'
mv = mlflow.register_model(model_uri, 'MyModel')

# Transition model stage
from mlflow.tracking import MlflowClient
client = MlflowClient()

client.transition_model_version_stage(
    name='MyModel',
    version=1,
    stage='Production'
)

# Load model
model = mlflow.pyfunc.load_model('models:/MyModel/Production')
predictions = model.predict(X_test)
"""

print("âœ… Model Registry Example")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. MLFLOW PROJECTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# MLproject file
name: my-ml-project

conda_env: conda.yaml

entry_points:
  main:
    parameters:
      n_estimators: {type: int, default: 100}
      max_depth: {type: int, default: 5}
    command: "python train.py --n-estimators {n_estimators} --max-depth {max_depth}"

# Run project
mlflow run . -P n_estimators=200 -P max_depth=10

# Run from Git
mlflow run https://github.com/user/repo -P param=value
"""

print("âœ… MLflow Projects Example")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. MODEL SERVING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Serve model locally
mlflow models serve -m models:/MyModel/Production -p 5000

# Docker deployment
mlflow models build-docker -m models:/MyModel/Production -n my-model

# Cloud deployment (AWS SageMaker)
mlflow deployments create -t sagemaker -m models:/MyModel/Production \
    --name my-model --config config.yaml
"""

print("âœ… MLflow Serving Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KUBEFLOW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Kubeflow: ML toolkit for Kubernetes
- Pipelines
- Training operators
- Serving (KFServing)
- Experiments
- Notebooks
"""

print("\n" + "="*60)
print("KUBEFLOW")
print("="*60)

"""
# Install: pip install kfp

import kfp
from kfp import dsl

# Define component
@dsl.component
def preprocess_data(input_path: str) -> str:
    # Implementation
    return output_path

@dsl.component
def train_model(data_path: str, n_estimators: int) -> str:
    # Implementation
    return model_path

@dsl.component
def evaluate_model(model_path: str, test_data: str) -> float:
    # Implementation
    return accuracy

# Define pipeline
@dsl.pipeline(
    name='ML Pipeline',
    description='End-to-end ML pipeline'
)
def ml_pipeline(input_data: str, n_estimators: int = 100):
    preprocess_task = preprocess_data(input_path=input_data)
    train_task = train_model(
        data_path=preprocess_task.output,
        n_estimators=n_estimators
    )
    evaluate_task = evaluate_model(
        model_path=train_task.output,
        test_data=preprocess_task.output
    )

# Compile pipeline
kfp.compiler.Compiler().compile(
    pipeline_func=ml_pipeline,
    package_path='pipeline.yaml'
)

# Run pipeline
client = kfp.Client(host='http://kubeflow-pipelines:8888')
run = client.create_run_from_pipeline_func(
    ml_pipeline,
    arguments={'input_data': 's3://bucket/data', 'n_estimators': 100}
)
"""

print("âœ… Kubeflow Pipelines Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BENTOML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
BentoML: Model serving made easy
- Package models
- API creation
- Docker containers
- Cloud deployment
"""

print("\n" + "="*60)
print("BENTOML")
print("="*60)

# Install: pip install bentoml

import bentoml
from bentoml.io import NumpyNdarray
import numpy as np

"""
# Save model
bentoml.sklearn.save_model(
    'my_model',
    model,
    signatures={'predict': {'batchable': True}}
)

# Create service (service.py)
import bentoml
from bentoml.io import NumpyNdarray

model_runner = bentoml.sklearn.get('my_model:latest').to_runner()
svc = bentoml.Service('classifier', runners=[model_runner])

@svc.api(input=NumpyNdarray(), output=NumpyNdarray())
def predict(input_array):
    result = model_runner.predict.run(input_array)
    return result

# Serve locally
# bentoml serve service:svc

# Build container
# bentoml containerize classifier:latest

# Deploy to cloud
# bentoml deploy classifier:latest --platform aws_lambda
"""

print("âœ… BentoML Example")

print("\n" + "="*60)
print("MLOps Platforms Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ“Š Experiment Tracking

</div>

### Track, Compare, and Reproduce Experiments ğŸ“ˆ

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPERIMENT TRACKING PLATFORMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Experiment Tracking: Record and compare ML experiments
- Hyperparameters
- Metrics
- Artifacts
- Code versions
- Collaboration
"""

print("="*60)
print("EXPERIMENT TRACKING")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WEIGHTS & BIASES (WANDB)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Weights & Biases: Experiment tracking and visualization
- Real-time metrics
- Hyperparameter tuning
- Model versioning
- Collaboration
- Reports
"""

print("\n" + "="*60)
print("WEIGHTS & BIASES")
print("="*60)

# Install: pip install wandb

import wandb
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

"""
# Initialize
wandb.init(
    project='my-project',
    config={
        'n_estimators': 100,
        'max_depth': 10,
        'learning_rate': 0.01
    }
)

# Access config
config = wandb.config

# Train model
model = RandomForestClassifier(
    n_estimators=config.n_estimators,
    max_depth=config.max_depth
)
model.fit(X_train, y_train)

# Log metrics
for epoch in range(10):
    train_loss = np.random.random()
    val_loss = np.random.random()

    wandb.log({
        'epoch': epoch,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'learning_rate': config.learning_rate
    })

# Log predictions
wandb.log({'predictions': wandb.Table(
    data=[[x, y] for x, y in zip(y_test, y_pred)],
    columns=['actual', 'predicted']
)})

# Log confusion matrix
wandb.sklearn.plot_confusion_matrix(y_test, y_pred, labels=['0', '1'])

# Log feature importance
wandb.sklearn.plot_feature_importances(model, feature_names)

# Save model
wandb.save('model.pkl')

# Finish run
wandb.finish()
"""

print("âœ… W&B Basic Tracking")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HYPERPARAMETER SWEEPS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Define sweep configuration
sweep_config = {
    'method': 'bayes',  # 'random', 'grid', 'bayes'
    'metric': {
        'name': 'val_accuracy',
        'goal': 'maximize'
    },
    'parameters': {
        'n_estimators': {
            'values': [50, 100, 200]
        },
        'max_depth': {
            'min': 3,
            'max': 15
        },
        'learning_rate': {
            'distribution': 'log_uniform_values',
            'min': 0.0001,
            'max': 0.1
        }
    }
}

# Initialize sweep
sweep_id = wandb.sweep(sweep_config, project='my-project')

# Train function
def train():
    run = wandb.init()
    config = wandb.config

    # Train model with config
    model = train_model(config)

    # Evaluate
    val_accuracy = evaluate(model)
    wandb.log({'val_accuracy': val_accuracy})

# Run sweep
wandb.agent(sweep_id, train, count=20)
"""

print("âœ… W&B Hyperparameter Sweeps")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PYTORCH INTEGRATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
import torch
import torch.nn as nn

# Initialize
wandb.init(project='pytorch-project')

# Watch model
model = MyNeuralNetwork()
wandb.watch(model, log='all', log_freq=100)

# Training loop
for epoch in range(epochs):
    for batch in dataloader:
        # Forward pass
        outputs = model(batch)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Log metrics
        wandb.log({'loss': loss.item()})

# Save model
torch.save(model.state_dict(), 'model.pth')
wandb.save('model.pth')
"""

print("âœ… W&B with PyTorch")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEPTUNE.AI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Neptune.ai: Metadata store for MLOps
- Experiment tracking
- Model registry
- Monitoring
- Team collaboration
"""

print("\n" + "="*60)
print("NEPTUNE.AI")
print("="*60)

# Install: pip install neptune-client

import neptune.new as neptune

"""
# Initialize
run = neptune.init_run(
    project='workspace/project',
    api_token='YOUR_API_TOKEN'
)

# Log parameters
params = {
    'n_estimators': 100,
    'max_depth': 10,
    'learning_rate': 0.01
}
run['parameters'] = params

# Log metrics
for epoch in range(100):
    run['train/loss'].log(train_loss)
    run['val/loss'].log(val_loss)
    run['val/accuracy'].log(accuracy)

# Log model
run['model'].upload('model.pkl')

# Log plots
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot(history['loss'])
run['plots/loss'].upload(fig)

# Log dataset version
run['dataset/version'] = 'v1.2.0'

# Add tags
run['sys/tags'].add(['baseline', 'random-forest'])

# Stop logging
run.stop()
"""

print("âœ… Neptune.ai Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMET.ML
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Comet.ml: Experiment management platform
- Tracking
- Comparison
- Monitoring
- Model registry
"""

print("\n" + "="*60)
print("COMET.ML")
print("="*60)

# Install: pip install comet-ml

from comet_ml import Experiment

"""
# Create experiment
experiment = Experiment(
    api_key='YOUR_API_KEY',
    project_name='my-project',
    workspace='my-workspace'
)

# Log parameters
experiment.log_parameters({
    'n_estimators': 100,
    'max_depth': 10,
    'learning_rate': 0.01
})

# Log metrics
for epoch in range(100):
    experiment.log_metric('loss', loss, epoch=epoch)
    experiment.log_metric('accuracy', accuracy, epoch=epoch)

# Log model
experiment.log_model('model', 'model.pkl')

# Log confusion matrix
experiment.log_confusion_matrix(y_true, y_pred)

# Log dataset
experiment.log_dataset_hash(X_train)

# End experiment
experiment.end()
"""

print("âœ… Comet.ml Example")

print("\n" + "="*60)
print("Experiment Tracking Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ¯ Feature Stores

</div>

### Centralized Feature Management ğŸ—„ï¸

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FEATURE STORES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Feature Store: Centralized repository for ML features
- Consistent features (training & serving)
- Feature reusability
- Feature versioning
- Point-in-time correctness
- Real-time & batch features
"""

print("="*60)
print("FEATURE STORES")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FEAST (FEATURE STORE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Feast: Open-source feature store
- Online serving
- Offline retrieval
- Point-in-time joins
- Feature versioning
"""

print("\n" + "="*60)
print("FEAST")
print("="*60)

# Install: pip install feast

from feast import FeatureStore, Entity, Feature, FeatureView, FileSource
from feast.types import Float32, Int64, String
from datetime import timedelta

"""
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. DEFINE FEATURES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Define entity (e.g., customer)
customer = Entity(
    name='customer',
    join_keys=['customer_id']
)

# Define feature view
customer_features_view = FeatureView(
    name='customer_features',
    entities=[customer],
    ttl=timedelta(days=1),
    schema=[
        Feature(name='age', dtype=Int64),
        Feature(name='income', dtype=Float32),
        Feature(name='credit_score', dtype=Int64),
        Feature(name='city', dtype=String)
    ],
    source=FileSource(
        path='data/customer_features.parquet',
        timestamp_field='event_timestamp'
    )
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. REGISTER FEATURES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Initialize feature store
fs = FeatureStore(repo_path='.')

# Apply (register) features
# feast apply (from command line)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. OFFLINE RETRIEVAL (TRAINING)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from feast import FeatureStore
import pandas as pd

fs = FeatureStore(repo_path='.')

# Define entities
entity_df = pd.DataFrame({
    'customer_id': [1001, 1002, 1003],
    'event_timestamp': [
        pd.Timestamp('2024-01-01'),
        pd.Timestamp('2024-01-01'),
        pd.Timestamp('2024-01-01')
    ]
})

# Get historical features
training_df = fs.get_historical_features(
    entity_df=entity_df,
    features=[
        'customer_features:age',
        'customer_features:income',
        'customer_features:credit_score'
    ]
).to_df()

print(training_df)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ONLINE RETRIEVAL (SERVING)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Materialize features to online store
# feast materialize-incremental $(date +%Y-%m-%d)

# Get online features
entity_rows = [
    {'customer_id': 1001},
    {'customer_id': 1002}
]

online_features = fs.get_online_features(
    features=[
        'customer_features:age',
        'customer_features:income',
        'customer_features:credit_score'
    ],
    entity_rows=entity_rows
).to_dict()

print(online_features)
"""

print("âœ… Feast Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TECTON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Tecton: Enterprise feature platform
- Real-time features
- Batch features
- Streaming features
- Feature monitoring
- Feature serving
"""

print("\n" + "="*60)
print("TECTON")
print("="*60)

"""
from tecton import Entity, FeatureView, BatchSource, materialization_context
from tecton.types import Field, String, Int64, Float64
from datetime import datetime, timedelta

# Define entity
customer = Entity(
    name='customer',
    join_keys=[Field('customer_id', Int64)]
)

# Define batch source
customer_source = BatchSource(
    name='customer_transactions',
    batch_config=FileConfig(
        uri='s3://bucket/transactions',
        file_format='parquet',
        timestamp_field='timestamp'
    )
)

# Define feature view
@batch_feature_view(
    sources=[customer_source],
    entities=[customer],
    mode='pandas',
    online=True,
    offline=True,
    feature_start_time=datetime(2024, 1, 1),
    batch_schedule=timedelta(days=1)
)
def customer_transaction_features(customer_source):
    return customer_source.groupby('customer_id').agg({
        'amount': ['sum', 'mean', 'count'],
        'is_fraud': 'sum'
    })

# Get features for training
training_data = customer_transaction_features.get_historical_features(
    spine=training_events
)

# Get features for serving
features = customer_transaction_features.get_online_features(
    join_keys={'customer_id': 1001}
)
"""

print("âœ… Tecton Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HOPSWORKS FEATURE STORE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Hopsworks: Feature store with Python API
- Feature groups
- Training datasets
- Online serving
- Feature monitoring
"""

print("\n" + "="*60)
print("HOPSWORKS")
print("="*60)

"""
import hopsworks

# Connect to Hopsworks
project = hopsworks.login()
fs = project.get_feature_store()

# Create feature group
customer_fg = fs.create_feature_group(
    name='customer_features',
    version=1,
    description='Customer demographic features',
    primary_key=['customer_id'],
    event_time='timestamp'
)

# Insert features
customer_fg.insert(customer_df)

# Create training dataset
feature_view = fs.create_feature_view(
    name='customer_model_features',
    version=1,
    query=customer_fg.select(['age', 'income', 'credit_score'])
)

# Get training data
X_train, y_train = feature_view.training_data(
    training_dataset_version=1
)

# Get batch data
batch_data = feature_view.get_batch_data(
    start_time='2024-01-01',
    end_time='2024-01-31'
)

# Initialize online serving
feature_view.init_serving(training_dataset_version=1)

# Get online features
features = feature_view.get_feature_vector(
    entry={'customer_id': 1001}
)
"""

print("âœ… Hopsworks Example")

print("\n" + "="*60)
print("Feature Stores Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸš€ Model Serving

</div>

### Deploy Models at Scale ğŸŒ

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL SERVING PLATFORMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Model Serving: Deploy ML models for inference
- Low latency
- High throughput
- Scalability
- Model versioning
- A/B testing
"""

print("="*60)
print("MODEL SERVING")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SELDON CORE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Seldon Core: ML deployment on Kubernetes
- Multi-framework support
- Canary deployments
- A/B testing
- Explainers
- Outlier detection
"""

print("\n" + "="*60)
print("SELDON CORE")
print("="*60)

"""
# 1. Create model class
class MyModel:
    def __init__(self):
        self.model = load_model()

    def predict(self, X, features_names=None):
        return self.model.predict(X)

# 2. Create Seldon deployment (YAML)
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: my-model
spec:
  predictors:
  - name: default
    replicas: 3
    graph:
      name: classifier
      type: MODEL
      children: []
    componentSpecs:
    - spec:
        containers:
        - name: classifier
          image: mymodel:v1.0

# 3. Deploy
kubectl apply -f deployment.yaml

# 4. Send predictions
curl -X POST http://seldon-core/api/v1.0/predictions \
  -H 'Content-Type: application/json' \
  -d '{"data": {"ndarray": [[1, 2, 3, 4]]}}'
"""

print("âœ… Seldon Core Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KSERVE (KFSERVING)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
KServe: Serverless inference on Kubernetes
- Auto-scaling
- Canary rollouts
- Explainability
- Monitoring
"""

print("\n" + "="*60)
print("KSERVE")
print("="*60)

"""
# Inference service YAML
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
spec:
  predictor:
    sklearn:
      storageUri: gs://bucket/model
      resources:
        limits:
          cpu: 1
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 1Gi

# Deploy
kubectl apply -f inference_service.yaml

# Predict
curl -X POST \
  http://sklearn-iris.default.example.com/v1/models/sklearn-iris:predict \
  -d '{"instances": [[6.8, 2.8, 4.8, 1.4]]}'
"""

print("âœ… KServe Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAY SERVE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Ray Serve: Scalable model serving
- Python-first
- Framework agnostic
- Dynamic batching
- Composition
"""

print("\n" + "="*60)
print("RAY SERVE")
print("="*60)

# Install: pip install ray[serve]

from ray import serve
import numpy as np

"""
# Define deployment
@serve.deployment(num_replicas=2)
class Classifier:
    def __init__(self):
        # Load model
        self.model = load_model()

    def __call__(self, request):
        data = request.query_params['data']
        prediction = self.model.predict(data)
        return {'prediction': prediction.tolist()}

# Start Ray Serve
serve.start()

# Deploy
Classifier.deploy()

# Query
import requests
response = requests.get(
    'http://localhost:8000/Classifier',
    params={'data': [1, 2, 3, 4]}
)
print(response.json())
"""

print("âœ… Ray Serve Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NVIDIA TRITON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
NVIDIA Triton: Multi-framework serving
- TensorFlow, PyTorch, ONNX
- GPU acceleration
- Dynamic batching
- Model ensemble
- Multiple protocols (HTTP, gRPC)
"""

print("\n" + "="*60)
print("NVIDIA TRITON")
print("="*60)

"""
# Model repository structure
model_repository/
â””â”€â”€ my_model/
    â”œâ”€â”€ config.pbtxt
    â””â”€â”€ 1/
        â””â”€â”€ model.onnx

# config.pbtxt
name: "my_model"
platform: "onnxruntime_onnx"
max_batch_size: 8
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 4 ]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]

# Start server
docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 \
  -v /path/to/model_repository:/models \
  nvcr.io/nvidia/tritonserver:latest \
  tritonserver --model-repository=/models

# Client request
import tritonclient.http as httpclient
import numpy as np

client = httpclient.InferenceServerClient(url='localhost:8000')

inputs = httpclient.InferInput('input', [1, 4], 'FP32')
inputs.set_data_from_numpy(np.array([[1, 2, 3, 4]], dtype=np.float32))

outputs = httpclient.InferRequestedOutput('output')

results = client.infer('my_model', inputs=[inputs], outputs=[outputs])
output_data = results.as_numpy('output')
"""

print("âœ… Triton Example")

print("\n" + "="*60)
print("Model Serving Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ¢ End-to-End Platforms

</div>

### Complete ML Platforms ğŸ¯

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END-TO-END ML PLATFORMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
End-to-End Platforms: Complete ML lifecycle management
- Data preparation
- Model development
- Deployment
- Monitoring
- Collaboration
"""

print("="*60)
print("END-TO-END PLATFORMS")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATABRICKS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Databricks: Unified analytics platform
- Collaborative notebooks
- MLflow integration
- Delta Lake
- AutoML
- Model serving
"""

print("\n" + "="*60)
print("DATABRICKS")
print("="*60)

"""
from databricks import feature_store
from databricks.feature_store import FeatureStoreClient
import mlflow

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. FEATURE ENGINEERING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fs = FeatureStoreClient()

# Create feature table
fs.create_table(
    name='customer_features',
    primary_keys=['customer_id'],
    df=customer_df,
    description='Customer demographic features'
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. MODEL TRAINING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Get features
training_set = fs.create_training_set(
    df=labels_df,
    feature_lookups=[
        FeatureLookup(
            table_name='customer_features',
            lookup_key='customer_id'
        )
    ],
    label='target'
)

training_df = training_set.load_df()

# Train with MLflow
with mlflow.start_run():
    model = train_model(training_df)

    # Log model with feature metadata
    fs.log_model(
        model,
        'model',
        flavor=mlflow.sklearn,
        training_set=training_set
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. MODEL SERVING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Register model
model_uri = f'runs:/{run_id}/model'
model_details = mlflow.register_model(model_uri, 'CustomerModel')

# Deploy to serving endpoint
# Via Databricks UI or API

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. BATCH INFERENCE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Score batch
batch_df = fs.score_batch(
    model_uri=f'models:/CustomerModel/Production',
    df=new_customers_df
)
"""

print("âœ… Databricks Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATAIKU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Dataiku: Collaborative data science platform
- Visual workflows
- Code notebooks
- AutoML
- Deployment
- Monitoring
"""

print("\n" + "="*60)
print("DATAIKU")
print("="*60)

"""
import dataiku
from dataiku import pandasutils as pdu

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. DATA PREPARATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Get dataset
dataset = dataiku.Dataset('my_dataset')
df = dataset.get_dataframe()

# Prepare data
df_prepared = prepare_features(df)

# Write back
output_dataset = dataiku.Dataset('prepared_data')
output_dataset.write_with_schema(df_prepared)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. MODEL TRAINING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from dataiku.ml import Model

# Train model
model = Model()
model.fit(X_train, y_train)

# Save model
model.save('my_model')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. DEPLOYMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Deploy as API endpoint
# Via Dataiku UI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. SCORING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Load model
model = Model.load('my_model')

# Score new data
predictions = model.predict(new_data)
"""

print("âœ… Dataiku Example")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DOMINO DATA LAB
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Domino: Enterprise MLOps platform
- Workspaces
- Experiments
- Model deployment
- Model monitoring
- Collaboration
"""

print("\n" + "="*60)
print("DOMINO DATA LAB")
print("="*60)

"""
from domino import Domino

# Initialize
domino = Domino('username/project')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. RUN EXPERIMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Submit training job
run = domino.runs_start(['python', 'train.py'],
                       title='Experiment 1',
                       tier='gpu')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. MODEL API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Deploy model as API
# Via Domino UI or CLI

# Call API
import requests
response = requests.post(
    'https://app.dominodatalab.com/v1/username/project/endpoint',
    json={'data': [1, 2, 3, 4]},
    headers={'X-Domino-Api-Key': 'YOUR_API_KEY'}
)
predictions = response.json()
"""

print("âœ… Domino Example")

print("\n" + "="*60)
print("End-to-End Platforms Overview Complete")
print("="*60)
```

---

<div align="center">

## ğŸ“Š Platform Comparison

</div>

### Choose the Right Platform ğŸ¯

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMPREHENSIVE PLATFORM COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   CLOUD PLATFORMS COMPARISON               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Feature           | AWS SageMaker    | Google Vertex AI | Azure ML       | IBM Watson   |
| ----------------- | ---------------- | ---------------- | -------------- | ------------ |
| **Ease of Use**   | â­â­â­           | â­â­â­â­         | â­â­â­         | â­â­         |
| **Cost**          | $$$              | $$$              | $$$            | $$$$         |
| **AutoML**        | âœ… Autopilot     | âœ… Built-in      | âœ… Built-in    | âœ… Built-in  |
| **Notebooks**     | âœ… Studio        | âœ… Workbench     | âœ… Compute     | âœ… Studio    |
| **Feature Store** | âœ… Yes           | âœ… Yes           | âŒ No          | âŒ No        |
| **Pipelines**     | âœ… Pipelines     | âœ… Pipelines     | âœ… Designer    | âœ… Flows     |
| **Serving**       | âœ… Endpoints     | âœ… Endpoints     | âœ… Endpoints   | âœ… Endpoints |
| **Monitoring**    | âœ… Model Monitor | âœ… Monitoring    | âœ… Monitoring  | âœ… OpenScale |
| **GPU Support**   | âœ… Excellent     | âœ… Excellent     | âœ… Excellent   | âœ… Good      |
| **Integration**   | â­â­â­â­ AWS     | â­â­â­â­ GCP     | â­â­â­â­ Azure | â­â­â­ IBM   |
| **Best For**      | AWS users        | GCP users        | Azure users    | Enterprise   |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   AUTOML PLATFORMS                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Platform          | Type              | Speed    | Accuracy   | Cost     | Best For           |
| ----------------- | ----------------- | -------- | ---------- | -------- | ------------------ |
| **H2O.ai**        | Open + Commercial | â­â­â­â­ | â­â­â­â­   | Free/$$$ | Enterprise, Custom |
| **DataRobot**     | Commercial        | â­â­â­â­ | â­â­â­â­â­ | $$$$     | Enterprise         |
| **AutoGluon**     | Open-source       | â­â­â­â­ | â­â­â­â­â­ | Free     | Research, Kaggle   |
| **PyCaret**       | Open-source       | â­â­â­   | â­â­â­     | Free     | Quick prototyping  |
| **TPOT**          | Open-source       | â­â­     | â­â­â­â­   | Free     | Research           |
| **Google AutoML** | Cloud             | â­â­â­â­ | â­â­â­â­   | $$$      | GCP users          |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   MLOPS PLATFORMS                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<div align="center">

| Platform     | Learning Curve | Features   | Community       | Best For         |
| ------------ | -------------- | ---------- | --------------- | ---------------- |
| **MLflow**   | â­ Easy        | â­â­â­â­   | â­â­â­â­â­ Huge | General purpose  |
| **Kubeflow** | â­â­â­ Hard    | â­â­â­â­â­ | â­â­â­â­ Large  | Kubernetes users |
| **BentoML**  | â­â­ Moderate  | â­â­â­     | â­â­â­ Growing  | Model serving    |
| **Seldon**   | â­â­â­ Hard    | â­â­â­â­   | â­â­â­ Medium   | K8s deployment   |
| **Metaflow** | â­â­ Moderate  | â­â­â­     | â­â­â­ Medium   | Netflix stack    |

</div>

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   DECISION MATRIX                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Choose Based On Your Needs:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For Startups/Small Teams:
  1st: MLflow + Cloud ML (SageMaker/Vertex AI)
  2nd: Weights & Biases + Cloud
  Why: Low cost, easy to start, scales later

For Mid-Size Companies:
  1st: Databricks
  2nd: Cloud ML Platform + MLflow
  Why: Balance of features and cost

For Enterprises:
  1st: Databricks or Dataiku
  2nd: Domino Data Lab
  Why: Governance, collaboration, support

For Research Teams:
  1st: Weights & Biases
  2nd: MLflow
  Why: Experiment tracking, collaboration

For Heavy AWS Users:
  1st: SageMaker
  2nd: SageMaker + MLflow
  Why: Native AWS integration

For Heavy GCP Users:
  1st: Vertex AI
  2nd: Vertex AI + Kubeflow
  Why: Native GCP integration

For Kubernetes-Native:
  1st: Kubeflow
  2nd: Seldon Core
  Why: K8s-native, flexible

For AutoML Focus:
  1st: H2O.ai or DataRobot
  2nd: Google AutoML
  Why: Best AutoML capabilities

For Budget-Conscious:
  1st: MLflow (open-source)
  2nd: AutoGluon + Feast
  Why: Free, community-driven

For Rapid Prototyping:
  1st: Google Colab + W&B
  2nd: Kaggle + Neptune
  Why: Zero setup, free GPUs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## ğŸ’¡ Best Practices

</div>

### Platform Implementation Guidelines ğŸ“‹

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ML PLATFORM BEST PRACTICES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   PLATFORM SELECTION                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Evaluation Criteria:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Team size and skill level
â˜ Budget constraints
â˜ Existing infrastructure (cloud provider)
â˜ Scalability requirements
â˜ Security and compliance needs
â˜ Integration requirements
â˜ Support and maintenance
â˜ Vendor lock-in concerns
â˜ Feature requirements
â˜ Time to value

Questions to Ask:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ What's our current cloud provider?
â€¢ Do we have Kubernetes expertise?
â€¢ What's our monthly ML budget?
â€¢ How many models will we deploy?
â€¢ Do we need AutoML?
â€¢ What frameworks do we use?
â€¢ Do we need feature stores?
â€¢ How critical is monitoring?
â€¢ What's our team size?
â€¢ Do we need governance tools?

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   IMPLEMENTATION STRATEGY                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Phase 1: Experimentation (Month 1-2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Start with experiment tracking (MLflow/W&B)
âœ… Use cloud notebooks (Colab/SageMaker Studio)
âœ… Version control code (Git)
âœ… Document experiments
âœ… Establish baselines

Phase 2: Infrastructure (Month 3-4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Set up CI/CD pipelines
âœ… Implement model registry
âœ… Configure training infrastructure
âœ… Set up data versioning (DVC)
âœ… Establish monitoring

Phase 3: Production (Month 5-6)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Deploy first models
âœ… Implement serving infrastructure
âœ… Set up model monitoring
âœ… Establish retraining pipelines
âœ… Create dashboards

Phase 4: Scale (Month 7+)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Add feature store
âœ… Implement A/B testing
âœ… Optimize costs
âœ… Automate workflows
âœ… Team training

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   COST OPTIMIZATION                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Strategies:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Use spot instances for training
âœ… Auto-scale serving endpoints
âœ… Schedule notebook shutdowns
âœ… Archive old experiments
âœ… Use appropriate instance types
âœ… Leverage free tiers
âœ… Monitor usage regularly
âœ… Set budget alerts
âœ… Optimize model size
âœ… Use model compression

Cost Comparison (Monthly):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Minimal Setup:
  â€¢ MLflow (self-hosted): $50-100
  â€¢ Cloud compute: $200-500
  â€¢ Storage: $50-100
  Total: ~$300-700/month

Mid-Size Team:
  â€¢ Databricks: $2,000-5,000
  â€¢ Cloud compute: $1,000-3,000
  â€¢ Storage: $200-500
  Total: ~$3,200-8,500/month

Enterprise:
  â€¢ Platform license: $10,000-50,000
  â€¢ Cloud compute: $5,000-20,000
  â€¢ Storage: $1,000-5,000
  Total: ~$16,000-75,000/month

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   SECURITY & GOVERNANCE                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Security Checklist:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â˜ Role-based access control (RBAC)
â˜ Secrets management (AWS Secrets Manager, etc.)
â˜ Data encryption (at rest and in transit)
â˜ Network isolation (VPC)
â˜ Audit logging
â˜ Model versioning
â˜ Data lineage tracking
â˜ Compliance (GDPR, HIPAA)
â˜ Vulnerability scanning
â˜ Regular security audits

Governance:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Model approval workflows
âœ… Experiment documentation
âœ… Model cards
âœ… Reproducibility requirements
âœ… Data quality checks
âœ… Bias detection
âœ… Performance thresholds
âœ… Rollback procedures
âœ… Incident response plans

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   MONITORING & OBSERVABILITY               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What to Monitor:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Performance:
  â€¢ Accuracy/precision/recall
  â€¢ Prediction latency
  â€¢ Throughput
  â€¢ Error rates

Data Quality:
  â€¢ Feature distributions
  â€¢ Missing values
  â€¢ Data drift
  â€¢ Outliers

System Health:
  â€¢ CPU/Memory usage
  â€¢ API response times
  â€¢ Queue lengths
  â€¢ Uptime

Business Metrics:
  â€¢ Model impact on KPIs
  â€¢ Cost per prediction
  â€¢ ROI
  â€¢ User satisfaction

Tools:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Grafana + Prometheus
â€¢ Datadog
â€¢ New Relic
â€¢ CloudWatch (AWS)
â€¢ Stackdriver (GCP)
â€¢ Application Insights (Azure)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   TEAM COLLABORATION                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Best Practices:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Shared experiment tracking
âœ… Code reviews for model code
âœ… Documentation standards
âœ… Regular sync meetings
âœ… Knowledge sharing sessions
âœ… Onboarding processes
âœ… Runbooks for common tasks
âœ… Clear ownership
âœ… Communication channels

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

## ğŸ”§ Implementation Guide

</div>

### Step-by-Step Platform Setup ğŸ› ï¸

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMPLETE PLATFORM IMPLEMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
Example: Setting up an end-to-end ML platform
Stack: AWS SageMaker + MLflow + Feast
"""

print("="*60)
print("IMPLEMENTATION GUIDE")
print("="*60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 1: EXPERIMENT TRACKING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Deploy MLflow on AWS
# 1. Create EC2 instance
# 2. Install MLflow
pip install mlflow boto3 psycopg2-binary

# 3. Start MLflow server
mlflow server \
    --backend-store-uri postgresql://user:pass@db.endpoint:5432/mlflow \
    --default-artifact-root s3://mlflow-artifacts/ \
    --host 0.0.0.0

# 4. Use in code
import mlflow

mlflow.set_tracking_uri('http://mlflow-server:5000')

with mlflow.start_run():
    mlflow.log_param('n_estimators', 100)
    mlflow.log_metric('accuracy', 0.95)
    mlflow.sklearn.log_model(model, 'model')
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 2: TRAINING PIPELINE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# SageMaker training script
# train.py
import argparse
import mlflow
from sklearn.ensemble import RandomForestClassifier
import joblib

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--n-estimators', type=int, default=100)
    parser.add_argument('--mlflow-tracking-uri', type=str)
    args = parser.parse_args()

    # Set MLflow tracking
    mlflow.set_tracking_uri(args.mlflow_tracking_uri)

    with mlflow.start_run():
        # Load data
        X_train, y_train = load_data()

        # Train model
        model = RandomForestClassifier(n_estimators=args.n_estimators)
        model.fit(X_train, y_train)

        # Log to MLflow
        mlflow.log_params({'n_estimators': args.n_estimators})
        mlflow.log_metric('train_accuracy', model.score(X_train, y_train))
        mlflow.sklearn.log_model(model, 'model')

        # Save for SageMaker
        joblib.dump(model, '/opt/ml/model/model.joblib')
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 3: FEATURE STORE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Set up Feast
# feature_store.yaml
project: my_project
registry: s3://feast-registry/registry.db
provider: aws
online_store:
    type: dynamodb
    region: us-west-2
offline_store:
    type: file

# features.py
from feast import Entity, Feature, FeatureView, FileSource
from datetime import timedelta

customer = Entity(name='customer', join_keys=['customer_id'])

customer_features = FeatureView(
    name='customer_features',
    entities=[customer],
    ttl=timedelta(days=1),
    schema=[
        Feature(name='age', dtype=Int64),
        Feature(name='income', dtype=Float32)
    ],
    source=FileSource(
        path='s3://bucket/features.parquet',
        timestamp_field='timestamp'
    )
)

# Apply features
feast apply
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 4: MODEL SERVING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Deploy model with SageMaker
import boto3
import sagemaker

sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

# Create model from MLflow
model_uri = 'runs:/run-id/model'
model_data = mlflow.sagemaker.deploy(
    model_uri=model_uri,
    app_name='my-model',
    execution_role_arn=role,
    image_url='mlflow-pyfunc',
    region_name='us-west-2',
    mode='create'
)

# Endpoint configuration
predictor = sagemaker.Predictor(
    endpoint_name='my-model',
    sagemaker_session=sagemaker_session
)

# Make predictions
predictions = predictor.predict(test_data)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 5: MONITORING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
# Set up monitoring with CloudWatch
import boto3

cloudwatch = boto3.client('cloudwatch')

# Custom metric
cloudwatch.put_metric_data(
    Namespace='ML/Models',
    MetricData=[
        {
            'MetricName': 'PredictionAccuracy',
            'Value': accuracy,
            'Unit': 'Percent',
            'Dimensions': [
                {'Name': 'ModelName', 'Value': 'my-model'},
                {'Name': 'ModelVersion', 'Value': 'v1'}
            ]
        }
    ]
)

# Set up alarms
cloudwatch.put_metric_alarm(
    AlarmName='ModelAccuracyLow',
    ComparisonOperator='LessThanThreshold',
    EvaluationPeriods=1,
    MetricName='PredictionAccuracy',
    Namespace='ML/Models',
    Period=300,
    Statistic='Average',
    Threshold=0.85,
    ActionsEnabled=True,
    AlarmActions=['arn:aws:sns:region:account:topic']
)
"""

print("\n" + "="*60)
print("Implementation Guide Complete")
print("="*60)
```

---

<div align="center">

## ğŸ“š Resources & Learning

</div>

### Continue Your Platform Journey ğŸš€

```
ğŸ“˜ Official Documentation
   â€¢ AWS SageMaker: https://docs.aws.amazon.com/sagemaker/
   â€¢ Google Vertex AI: https://cloud.google.com/vertex-ai/docs
   â€¢ Azure ML: https://docs.microsoft.com/azure/machine-learning/
   â€¢ MLflow: https://mlflow.org/docs/
   â€¢ Kubeflow: https://www.kubeflow.org/docs/
   â€¢ Feast: https://docs.feast.dev/

ğŸ“— Books
   â€¢ Machine Learning Engineering (Andriy Burkov)
   â€¢ Designing Machine Learning Systems (Chip Huyen)
   â€¢ Building Machine Learning Powered Applications (Emmanuel Ameisen)

ğŸ“™ Online Courses
   â€¢ Full Stack Deep Learning
   â€¢ Made With ML (MLOps)
   â€¢ Coursera MLOps Specialization

ğŸ¥ YouTube Channels
   â€¢ Weights & Biases
   â€¢ Made With ML
   â€¢ MLOps Community

ğŸ’» Communities
   â€¢ MLOps Community Slack
   â€¢ r/MachineLearning
   â€¢ r/MLOps
   â€¢ Kubeflow Slack
   â€¢ MLflow Slack

ğŸ› ï¸ Tools & Templates
   â€¢ Cookiecutter Data Science
   â€¢ MLOps Zoomcamp
   â€¢ Awesome MLOps (GitHub)
```

---

<div align="center">

## ğŸ¯ Summary

</div>

### Key Takeaways ğŸ’¡

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   REMEMBER                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Start Simple
   â€¢ Begin with experiment tracking (MLflow/W&B)
   â€¢ Use cloud notebooks initially
   â€¢ Scale infrastructure as needed
   â€¢ Don't over-engineer early

2. Choose Based on Needs
   â€¢ Team size and skills
   â€¢ Budget constraints
   â€¢ Existing infrastructure
   â€¢ Scalability requirements

3. Focus on Core Components
   â€¢ Experiment tracking (essential)
   â€¢ Model registry (important)
   â€¢ Feature store (nice to have)
   â€¢ Serving platform (critical for production)
   â€¢ Monitoring (essential in production)

4. Iterate and Improve
   â€¢ Start with MVP
   â€¢ Get feedback
   â€¢ Add features incrementally
   â€¢ Optimize costs continuously

5. Invest in Team
   â€¢ Training and upskilling
   â€¢ Documentation
   â€¢ Best practices
   â€¢ Collaboration tools

"The best ML platform is the one your team actually uses." âœ¨

Platform Selection Priority:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Experiment Tracking (Day 1)
2. Model Versioning (Week 1)
3. CI/CD for ML (Month 1)
4. Model Serving (Month 2)
5. Monitoring (Month 2)
6. Feature Store (Month 3+)
7. Advanced Automation (Month 6+)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

**Built with ğŸš€ by MrDib, for ML practitioners**

_Remember: "MLOps is about getting ML into production and keeping it there!"_ ğŸŒŸ

**Happy Platform Building!** ğŸ› ï¸
