<div align="center">

# ğŸ’¾ Database Hosting - Complete Guide

### _Managed databases for production applications_ ğŸ—„ï¸

![Database](https://img.shields.io/badge/Database-Production%20Ready-purple?style=for-the-badge)
![Scale](https://img.shields.io/badge/Scale-Auto%20Scaling-green?style=for-the-badge)
![Reliable](https://img.shields.io/badge/Reliable-99.9%25%20Uptime-blue?style=for-the-badge)
![Status](https://img.shields.io/badge/Guide-Complete-gold?style=for-the-badge)

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Choosing Your Database](#-choosing-your-database)
- [ğŸ˜ PostgreSQL Hosting](#-postgresql-hosting)
  - [Supabase](#-supabase)
  - [Neon](#-neon)
  - [Railway PostgreSQL](#-railway-postgresql)
  - [Render PostgreSQL](#-render-postgresql)
- [ğŸ¬ MySQL Hosting](#-mysql-hosting)
  - [PlanetScale](#-planetscale)
  - [Railway MySQL](#-railway-mysql)
- [ğŸƒ MongoDB Hosting](#-mongodb-hosting)
  - [MongoDB Atlas](#-mongodb-atlas)
  - [Railway MongoDB](#-railway-mongodb)
- [âš¡ Redis Hosting](#-redis-hosting)
  - [Upstash](#-upstash)
  - [Redis Cloud](#-redis-cloud)
- [ğŸ”¥ Firebase & Firestore](#-firebase--firestore)
- [ğŸŒ Distributed Databases](#-distributed-databases)
- [ğŸ”„ Database Migration](#-database-migration)
- [ğŸ” Security Best Practices](#-security-best-practices)
- [ğŸ“Š Performance Optimization](#-performance-optimization)
- [ğŸ’° Cost Optimization](#-cost-optimization)
- [ğŸ”§ Backup & Recovery](#-backup--recovery)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ’¡ Best Practices](#-best-practices)

---

<div align="center">

## ğŸ¯ Choosing Your Database

_The ultimate database decision guide_ ğŸŒ²

</div>

### Understanding Database Types

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  DATABASE TYPES EXPLAINED                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. RELATIONAL (SQL) DATABASES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL, MySQL, MariaDB                            â”‚
â”‚                                                        â”‚
â”‚  Structure:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   Users     â”‚  â”‚    Posts    â”‚                     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚  â”‚ id (PK)     â”‚  â”‚ id (PK)     â”‚                     â”‚
â”‚  â”‚ name        â”‚  â”‚ user_id (FK)â”‚â”€â”€â”€â”                 â”‚
â”‚  â”‚ email       â”‚  â”‚ title       â”‚   â”‚                 â”‚
â”‚  â”‚ created_at  â”‚  â”‚ content     â”‚   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                 â”‚
â”‚         â–²                            â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                        â”‚
â”‚  âœ… PROS:                                              â”‚
â”‚  â€¢ Strong consistency (ACID)                          â”‚
â”‚  â€¢ Complex queries (JOINs)                            â”‚
â”‚  â€¢ Data integrity enforced                            â”‚
â”‚  â€¢ Mature ecosystem                                    â”‚
â”‚  â€¢ Transactions support                                â”‚
â”‚                                                        â”‚
â”‚  âŒ CONS:                                              â”‚
â”‚  â€¢ Schema changes can be complex                      â”‚
â”‚  â€¢ Vertical scaling limits                            â”‚
â”‚  â€¢ Not ideal for unstructured data                    â”‚
â”‚                                                        â”‚
â”‚  ğŸ¯ USE WHEN:                                          â”‚
â”‚  â€¢ Data has relationships                             â”‚
â”‚  â€¢ Need ACID transactions                             â”‚
â”‚  â€¢ Complex queries required                           â”‚
â”‚  â€¢ Data structure is well-defined                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. DOCUMENT DATABASES (NoSQL)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB, Firestore, DynamoDB                          â”‚
â”‚                                                        â”‚
â”‚  Structure:                                            â”‚
â”‚  {                                                     â”‚
â”‚    "_id": "user123",                                   â”‚
â”‚    "name": "MrDib",                                    â”‚
â”‚    "email": "email@example.com",                       â”‚
â”‚    "posts": [                                          â”‚
â”‚      {                                                 â”‚
â”‚        "id": "post1",                                  â”‚
â”‚        "title": "My Post",                             â”‚
â”‚        "content": "...",                               â”‚
â”‚        "tags": ["tech", "coding"]                      â”‚
â”‚      }                                                 â”‚
â”‚    ],                                                  â”‚
â”‚    "preferences": {                                    â”‚
â”‚      "theme": "dark",                                  â”‚
â”‚      "notifications": true                             â”‚
â”‚    }                                                   â”‚
â”‚  }                                                     â”‚
â”‚                                                        â”‚
â”‚  âœ… PROS:                                              â”‚
â”‚  â€¢ Flexible schema                                     â”‚
â”‚  â€¢ Horizontal scaling                                  â”‚
â”‚  â€¢ Fast for simple queries                            â”‚
â”‚  â€¢ Good for nested data                               â”‚
â”‚  â€¢ Easy to get started                                â”‚
â”‚                                                        â”‚
â”‚  âŒ CONS:                                              â”‚
â”‚  â€¢ No built-in joins                                  â”‚
â”‚  â€¢ Data duplication common                            â”‚
â”‚  â€¢ Eventual consistency (some)                        â”‚
â”‚  â€¢ Complex transactions harder                        â”‚
â”‚                                                        â”‚
â”‚  ğŸ¯ USE WHEN:                                          â”‚
â”‚  â€¢ Schema changes frequently                          â”‚
â”‚  â€¢ Need to scale horizontally                         â”‚
â”‚  â€¢ Storing varied/nested data                         â”‚
â”‚  â€¢ Real-time applications                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. KEY-VALUE STORES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis, Upstash, DynamoDB                              â”‚
â”‚                                                        â”‚
â”‚  Structure:                                            â”‚
â”‚  Key              â†’ Value                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  user:123:name    â†’ "MrDib"                            â”‚
â”‚  user:123:email   â†’ "email@example.com"                â”‚
â”‚  session:abc123   â†’ {...session data...}              â”‚
â”‚  cache:posts:all  â†’ [...cached posts...]              â”‚
â”‚                                                        â”‚
â”‚  âœ… PROS:                                              â”‚
â”‚  â€¢ Extremely fast (in-memory)                         â”‚
â”‚  â€¢ Simple data model                                   â”‚
â”‚  â€¢ Excellent for caching                              â”‚
â”‚  â€¢ Low latency                                         â”‚
â”‚  â€¢ Highly scalable                                     â”‚
â”‚                                                        â”‚
â”‚  âŒ CONS:                                              â”‚
â”‚  â€¢ No complex queries                                 â”‚
â”‚  â€¢ Limited data types                                  â”‚
â”‚  â€¢ Not for primary storage                            â”‚
â”‚  â€¢ Memory constraints                                  â”‚
â”‚                                                        â”‚
â”‚  ğŸ¯ USE WHEN:                                          â”‚
â”‚  â€¢ Caching frequently accessed data                   â”‚
â”‚  â€¢ Session storage                                     â”‚
â”‚  â€¢ Real-time analytics                                â”‚
â”‚  â€¢ Rate limiting                                       â”‚
â”‚  â€¢ Message queues                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. TIME-SERIES DATABASES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TimescaleDB, InfluxDB                                 â”‚
â”‚                                                        â”‚
â”‚  Structure:                                            â”‚
â”‚  Timestamp              Metric      Value              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  2024-01-01 10:00:00   cpu_usage   45.2               â”‚
â”‚  2024-01-01 10:01:00   cpu_usage   47.8               â”‚
â”‚  2024-01-01 10:02:00   cpu_usage   43.1               â”‚
â”‚                                                        â”‚
â”‚  ğŸ¯ USE WHEN:                                          â”‚
â”‚  â€¢ Storing metrics/logs                               â”‚
â”‚  â€¢ IoT sensor data                                     â”‚
â”‚  â€¢ Application monitoring                             â”‚
â”‚  â€¢ Financial tick data                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DECISION TREE:

START: What are you building?
   â”‚
   â”œâ”€â“ E-commerce / SaaS / Traditional app?
   â”‚  â””â”€âœ… PostgreSQL (Supabase, Neon)
   â”‚     â€¢ Structured data
   â”‚     â€¢ Relationships important
   â”‚     â€¢ Need transactions
   â”‚
   â”œâ”€â“ Real-time chat / Collaborative app?
   â”‚  â””â”€âœ… Firestore or MongoDB
   â”‚     â€¢ Real-time sync
   â”‚     â€¢ Flexible schema
   â”‚     â€¢ Offline support
   â”‚
   â”œâ”€â“ Need caching / sessions?
   â”‚  â””â”€âœ… Redis (Upstash)
   â”‚     â€¢ Fast access
   â”‚     â€¢ Temporary data
   â”‚     â€¢ Rate limiting
   â”‚
   â”œâ”€â“ Mobile app with auth?
   â”‚  â””â”€âœ… Firebase or Supabase
   â”‚     â€¢ Auth included
   â”‚     â€¢ File storage
   â”‚     â€¢ Real-time
   â”‚
   â”œâ”€â“ Analytics / Metrics?
   â”‚  â””â”€âœ… TimescaleDB or InfluxDB
   â”‚     â€¢ Time-series data
   â”‚     â€¢ Aggregations
   â”‚     â€¢ Retention policies
   â”‚
   â””â”€â“ Content-heavy site?
      â””â”€âœ… PostgreSQL with full-text search
         â€¢ Complex queries
         â€¢ Search functionality
         â€¢ JSON support

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

POLYGLOT PERSISTENCE (Using multiple databases):

Modern App Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (React, Vue, etc.)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Layer (Node.js, Python, etc.)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚PostgreSQLâ”‚   â”‚  Redis   â”‚   â”‚   S3    â”‚
   â”‚(Primary) â”‚   â”‚ (Cache)  â”‚   â”‚ (Files) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example:
â€¢ PostgreSQL â†’ User accounts, orders, products
â€¢ Redis â†’ Session data, caching, rate limiting
â€¢ S3/Cloudinary â†’ Images, videos, files
â€¢ Elasticsearch â†’ Full-text search

Benefits:
âœ… Use right tool for each job
âœ… Better performance
âœ… More flexibility
```

### Comparison Matrix

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DATABASE HOSTING COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

POSTGRESQL PROVIDERS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider    â”‚ Free Tier  â”‚ Best Featureâ”‚ Paid From    â”‚ Ratingâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Supabase    â”‚ 500MB      â”‚ Auth+Storageâ”‚ $25/mo       â”‚ â­â­â­â­â­â”‚
â”‚             â”‚ 2 projects â”‚ included    â”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Neon        â”‚ 3GB        â”‚ Serverless  â”‚ $19/mo       â”‚ â­â­â­â­â­â”‚
â”‚             â”‚ Branching  â”‚ Autoscaling â”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Railway     â”‚ Included   â”‚ Integrated  â”‚ Usage-based  â”‚ â­â­â­â­ â”‚
â”‚             â”‚ w/project  â”‚ Platform    â”‚ ~$5/mo       â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Render      â”‚ None (paid)â”‚ Backups     â”‚ $7/mo        â”‚ â­â­â­â­ â”‚
â”‚             â”‚            â”‚ included    â”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ AWS RDS     â”‚ 750hrs/yr  â”‚ Enterprise  â”‚ $15/mo+      â”‚ â­â­â­â­ â”‚
â”‚             â”‚ (1st year) â”‚ features    â”‚              â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

MYSQL PROVIDERS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider    â”‚ Free Tier  â”‚ Best Featureâ”‚ Paid From    â”‚ Ratingâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ PlanetScale â”‚ 5GB        â”‚ DB Branchingâ”‚ $29/mo       â”‚ â­â­â­â­â­â”‚
â”‚             â”‚ 1B reads   â”‚ Schema diffsâ”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Railway     â”‚ Included   â”‚ Easy setup  â”‚ Usage-based  â”‚ â­â­â­â­ â”‚
â”‚             â”‚ w/project  â”‚             â”‚ ~$5/mo       â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ AWS RDS     â”‚ 750hrs/yr  â”‚ Proven      â”‚ $15/mo+      â”‚ â­â­â­â­ â”‚
â”‚             â”‚            â”‚ Reliable    â”‚              â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

MONGODB PROVIDERS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider    â”‚ Free Tier  â”‚ Best Featureâ”‚ Paid From    â”‚ Ratingâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ MongoDB     â”‚ 512MB      â”‚ Official    â”‚ $9/mo        â”‚ â­â­â­â­â­â”‚
â”‚ Atlas       â”‚ Shared     â”‚ Atlas Searchâ”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Railway     â”‚ Included   â”‚ Integrated  â”‚ Usage-based  â”‚ â­â­â­â­ â”‚
â”‚             â”‚ w/project  â”‚             â”‚ ~$5/mo       â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

REDIS PROVIDERS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider    â”‚ Free Tier  â”‚ Best Featureâ”‚ Paid From    â”‚ Ratingâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Upstash     â”‚ 10k cmds   â”‚ Serverless  â”‚ $0.2/100k    â”‚ â­â­â­â­â­â”‚
â”‚             â”‚ /day       â”‚ Pay-per-use â”‚ requests     â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Redis Cloud â”‚ 30MB       â”‚ Official    â”‚ $5/mo        â”‚ â­â­â­â­ â”‚
â”‚             â”‚            â”‚ Redis Labs  â”‚              â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Railway     â”‚ Included   â”‚ Easy        â”‚ Usage-based  â”‚ â­â­â­â­ â”‚
â”‚             â”‚ w/project  â”‚             â”‚ ~$3/mo       â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

REAL-TIME DATABASES:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Provider    â”‚ Free Tier  â”‚ Best Featureâ”‚ Paid From    â”‚ Ratingâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Firebase    â”‚ 1GB storageâ”‚ Real-time   â”‚ Pay-as-go    â”‚ â­â­â­â­â­â”‚
â”‚ Firestore   â”‚ 50k reads  â”‚ Offline syncâ”‚ ~$25/mo      â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ Supabase    â”‚ 500MB      â”‚ PostgreSQL  â”‚ $25/mo       â”‚ â­â­â­â­â­â”‚
â”‚             â”‚ Realtime   â”‚ + Real-time â”‚              â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

RECOMMENDATIONS BY USE CASE:

ğŸ¯ Startup/MVP:
   â†’ Supabase (PostgreSQL + Auth + Storage)
   â†’ PlanetScale (MySQL with branching)

ğŸ’° Budget-Conscious:
   â†’ Railway (all-in-one, pay-as-you-go)
   â†’ Neon (generous free tier)
   â†’ Supabase (free tier is great)

ğŸš€ High Performance:
   â†’ PlanetScale (horizontal scaling)
   â†’ Neon (serverless autoscaling)
   â†’ Upstash Redis (for caching)

ğŸ¢ Enterprise:
   â†’ AWS RDS/Aurora
   â†’ Google Cloud SQL
   â†’ Azure Database

ğŸ”¥ Real-time Apps:
   â†’ Firebase/Firestore
   â†’ Supabase (realtime PostgreSQL)

ğŸ“± Mobile Apps:
   â†’ Firebase (SDK + Auth)
   â†’ Supabase (SDK + Auth)

ğŸ® Gaming:
   â†’ MongoDB Atlas (flexible schema)
   â†’ DynamoDB (low latency)
```

---

<div align="center">

## ğŸ˜ PostgreSQL Hosting

_The world's most advanced open source database_ ğŸ¯

</div>

### âš¡ Supabase

**Open source Firebase alternative powered by PostgreSQL**

```
ğŸŒ Website â†’ https://supabase.com
ğŸ¯ Best For â†’ Full-stack apps needing auth, database, storage
ğŸ’° Pricing  â†’ Free: 500MB DB, 1GB file storage, 50k MAU
             Paid: $25/month for 8GB DB, 100GB storage
âš¡ Features â†’ PostgreSQL, Auth, Storage, Edge Functions, Realtime
ğŸ”§ Support  â†’ JavaScript, Python, Dart, C#, Swift, Kotlin
```

#### ğŸ“¥ Supabase Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INSTALLATION & SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create project at supabase.com
# Get your project URL and anon key

# Install client library
npm install @supabase/supabase-js

# For React/Next.js
npm install @supabase/auth-helpers-nextjs
npm install @supabase/auth-helpers-react

# For authentication UI
npm install @supabase/auth-ui-react @supabase/auth-ui-shared
```

#### ğŸ“ Supabase Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// lib/supabase.js - Client Setup
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { createClient } from "@supabase/supabase-js";

// Environment variables
const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY;

export const supabase = createClient(supabaseUrl, supabaseAnonKey);

// With custom options
export const supabaseWithOptions = createClient(supabaseUrl, supabaseAnonKey, {
  auth: {
    autoRefreshToken: true,
    persistSession: true,
    detectSessionInUrl: true,
  },
  db: {
    schema: "public",
  },
  global: {
    headers: {
      "x-custom-header": "my-app",
    },
  },
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CRUD OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// CREATE
async function createPost(title, content, userId) {
  const { data, error } = await supabase
    .from("posts")
    .insert([
      {
        title,
        content,
        user_id: userId,
        created_at: new Date().toISOString(),
      },
    ])
    .select();

  if (error) throw error;
  return data;
}

// READ
async function getPosts() {
  const { data, error } = await supabase
    .from("posts")
    .select("*")
    .order("created_at", { ascending: false });

  if (error) throw error;
  return data;
}

// READ with JOIN
async function getPostsWithAuthors() {
  const { data, error } = await supabase
    .from("posts")
    .select(
      `
      *,
      author:users (
        id,
        name,
        email,
        avatar_url
      )
    `
    )
    .order("created_at", { ascending: false });

  if (error) throw error;
  return data;
}

// READ with filtering
async function getPublishedPosts() {
  const { data, error } = await supabase
    .from("posts")
    .select("*")
    .eq("published", true)
    .gte("created_at", "2024-01-01")
    .order("created_at", { ascending: false })
    .limit(10);

  if (error) throw error;
  return data;
}

// UPDATE
async function updatePost(id, updates) {
  const { data, error } = await supabase
    .from("posts")
    .update(updates)
    .eq("id", id)
    .select();

  if (error) throw error;
  return data;
}

// DELETE
async function deletePost(id) {
  const { error } = await supabase.from("posts").delete().eq("id", id);

  if (error) throw error;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// REAL-TIME SUBSCRIPTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Subscribe to changes
const subscription = supabase
  .channel("posts_channel")
  .on(
    "postgres_changes",
    {
      event: "*", // INSERT, UPDATE, DELETE, or *
      schema: "public",
      table: "posts",
    },
    (payload) => {
      console.log("Change detected:", payload);

      if (payload.eventType === "INSERT") {
        console.log("New post:", payload.new);
      }

      if (payload.eventType === "UPDATE") {
        console.log("Updated post:", payload.new);
        console.log("Old values:", payload.old);
      }

      if (payload.eventType === "DELETE") {
        console.log("Deleted post:", payload.old);
      }
    }
  )
  .subscribe();

// Unsubscribe
subscription.unsubscribe();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AUTHENTICATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Sign up with email
async function signUp(email, password) {
  const { data, error } = await supabase.auth.signUp({
    email,
    password,
  });

  if (error) throw error;
  return data;
}

// Sign in with email
async function signIn(email, password) {
  const { data, error } = await supabase.auth.signInWithPassword({
    email,
    password,
  });

  if (error) throw error;
  return data;
}

// Sign in with OAuth
async function signInWithGoogle() {
  const { data, error } = await supabase.auth.signInWithOAuth({
    provider: "google",
    options: {
      redirectTo: "http://localhost:3000/auth/callback",
    },
  });

  if (error) throw error;
  return data;
}

// Sign out
async function signOut() {
  const { error } = await supabase.auth.signOut();
  if (error) throw error;
}

// Get current user
async function getCurrentUser() {
  const {
    data: { user },
  } = await supabase.auth.getUser();
  return user;
}

// Listen to auth changes
supabase.auth.onAuthStateChange((event, session) => {
  console.log("Auth event:", event);
  console.log("Session:", session);

  if (event === "SIGNED_IN") {
    console.log("User signed in");
  }

  if (event === "SIGNED_OUT") {
    console.log("User signed out");
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FILE STORAGE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Upload file
async function uploadFile(file, bucket = "avatars") {
  const fileName = `${Date.now()}_${file.name}`;

  const { data, error } = await supabase.storage
    .from(bucket)
    .upload(fileName, file, {
      cacheControl: "3600",
      upsert: false,
    });

  if (error) throw error;
  return data;
}

// Get public URL
function getPublicUrl(path, bucket = "avatars") {
  const { data } = supabase.storage.from(bucket).getPublicUrl(path);

  return data.publicUrl;
}

// Download file
async function downloadFile(path, bucket = "avatars") {
  const { data, error } = await supabase.storage.from(bucket).download(path);

  if (error) throw error;
  return data;
}

// Delete file
async function deleteFile(path, bucket = "avatars") {
  const { error } = await supabase.storage.from(bucket).remove([path]);

  if (error) throw error;
}

// List files
async function listFiles(folder = "", bucket = "avatars") {
  const { data, error } = await supabase.storage.from(bucket).list(folder, {
    limit: 100,
    offset: 0,
    sortBy: { column: "created_at", order: "desc" },
  });

  if (error) throw error;
  return data;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// EDGE FUNCTIONS (Serverless)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Invoke edge function
async function invokeFunction(functionName, payload) {
  const { data, error } = await supabase.functions.invoke(functionName, {
    body: payload,
  });

  if (error) throw error;
  return data;
}

// Example: Send email via edge function
async function sendEmail(to, subject, body) {
  return await invokeFunction("send-email", {
    to,
    subject,
    body,
  });
}
```

#### ğŸ—„ï¸ Supabase Database Schema

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- Create tables in Supabase SQL Editor
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Enable UUID extension
create extension if not exists "uuid-ossp";

-- Users table (extends auth.users)
create table public.profiles (
  id uuid references auth.users on delete cascade not null primary key,
  username text unique,
  full_name text,
  avatar_url text,
  bio text,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null,
  updated_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Enable Row Level Security (RLS)
alter table public.profiles enable row level security;

-- RLS Policies
create policy "Public profiles are viewable by everyone"
  on profiles for select
  using ( true );

create policy "Users can insert their own profile"
  on profiles for insert
  with check ( auth.uid() = id );

create policy "Users can update own profile"
  on profiles for update
  using ( auth.uid() = id );

-- Posts table
create table public.posts (
  id uuid default uuid_generate_v4() primary key,
  user_id uuid references public.profiles(id) on delete cascade not null,
  title text not null,
  content text,
  published boolean default false,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null,
  updated_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Enable RLS
alter table public.posts enable row level security;

-- RLS Policies for posts
create policy "Published posts are viewable by everyone"
  on posts for select
  using ( published = true );

create policy "Users can view their own posts"
  on posts for select
  using ( auth.uid() = user_id );

create policy "Users can insert their own posts"
  on posts for insert
  with check ( auth.uid() = user_id );

create policy "Users can update their own posts"
  on posts for update
  using ( auth.uid() = user_id );

create policy "Users can delete their own posts"
  on posts for delete
  using ( auth.uid() = user_id );

-- Indexes for performance
create index posts_user_id_idx on public.posts(user_id);
create index posts_created_at_idx on public.posts(created_at desc);
create index posts_published_idx on public.posts(published) where published = true;

-- Full-text search
create index posts_title_content_idx on public.posts
  using gin(to_tsvector('english', title || ' ' || coalesce(content, '')));

-- Function to automatically update updated_at
create or replace function public.handle_updated_at()
returns trigger as $$
begin
  new.updated_at = now();
  return new;
end;
$$ language plpgsql;

-- Trigger for updated_at
create trigger handle_updated_at
  before update on public.profiles
  for each row
  execute procedure public.handle_updated_at();

create trigger handle_updated_at
  before update on public.posts
  for each row
  execute procedure public.handle_updated_at();

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- FULL-TEXT SEARCH FUNCTION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

create or replace function search_posts(search_query text)
returns setof posts
language sql
stable
as $$
  select *
  from posts
  where published = true
  and to_tsvector('english', title || ' ' || coalesce(content, '')) @@ plainto_tsquery('english', search_query)
  order by ts_rank(to_tsvector('english', title || ' ' || coalesce(content, '')), plainto_tsquery('english', search_query)) desc;
$$;

-- Use in JavaScript:
-- const { data } = await supabase.rpc('search_posts', { search_query: 'keyword' });
```

#### ğŸ’¡ Supabase Pro Tips

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PERFORMANCE OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Use select() to fetch only needed columns
const { data } = await supabase.from("posts").select("id, title, created_at"); // âœ… Only needed columns
// .select('*'); // âŒ Fetches everything

// 2. Pagination
const pageSize = 10;
const page = 1;

const { data, error } = await supabase
  .from("posts")
  .select("*", { count: "exact" })
  .range((page - 1) * pageSize, page * pageSize - 1);

// 3. Use indexes for filtered queries
// Create index in SQL Editor:
// create index posts_user_published_idx on posts(user_id, published);

// 4. Batch inserts
const { data } = await supabase.from("posts").insert([
  { title: "Post 1", content: "..." },
  { title: "Post 2", content: "..." },
  { title: "Post 3", content: "..." },
]);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ERROR HANDLING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function safeQuery() {
  const { data, error } = await supabase.from("posts").select("*");

  if (error) {
    // Log error
    console.error("Supabase error:", error.message);

    // Handle specific errors
    if (error.code === "PGRST116") {
      console.error("Resource not found");
    }

    // Return or throw
    throw new Error(`Database error: ${error.message}`);
  }

  return data;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SECURITY BEST PRACTICES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Always use RLS (Row Level Security)
// Enable it on all tables

// 2. Use service role key only on server
// Server-side (Next.js API route)
import { createClient } from "@supabase/supabase-js";

const supabaseAdmin = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY // Server-only!
);

// 3. Validate user input
function validatePostData(data) {
  if (!data.title || data.title.length < 3) {
    throw new Error("Title must be at least 3 characters");
  }

  if (data.title.length > 200) {
    throw new Error("Title too long");
  }

  return true;
}

// 4. Use prepared statements (automatic in Supabase)
// All queries are parameterized by default

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CACHING STRATEGY
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Client-side caching with SWR or React Query
import useSWR from "swr";

const fetcher = async () => {
  const { data } = await supabase.from("posts").select("*");
  return data;
};

function Posts() {
  const { data, error, isLoading } = useSWR("posts", fetcher, {
    revalidateOnFocus: false,
    revalidateOnReconnect: false,
  });

  // ...
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATABASE FUNCTIONS (Stored Procedures)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Create in SQL Editor:
/*
create or replace function increment_post_views(post_id uuid)
returns void as $$
  update posts
  set view_count = view_count + 1
  where id = post_id;
$$ language sql volatile;
*/

// Call from JavaScript:
const { data, error } = await supabase.rpc("increment_post_views", {
  post_id: "uuid-here",
});
```

---

### ğŸ’¡ Neon

**Serverless PostgreSQL with instant branching**

```
ğŸŒ Website â†’ https://neon.tech
ğŸ¯ Best For â†’ Serverless apps, development workflows, preview environments
ğŸ’° Pricing  â†’ Free: 3GB storage, 3 projects
             Paid: $19/month for 10GB, autoscaling
âš¡ Features â†’ Serverless, Branching, Autoscaling, Point-in-time restore
ğŸ”§ Support  â†’ Any PostgreSQL client library
```

#### ğŸ“¥ Neon Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INSTALLATION & SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create account at neon.tech
# Create project and get connection string

# Install Neon CLI
npm install -g neonctl

# Login
neonctl auth

# List projects
neonctl projects list

# Create branch (like git branch!)
neonctl branches create --project-id your-project-id --name dev

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION STRING FORMAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Format:
# postgres://[user]:[password]@[endpoint]/[dbname]?sslmode=require

# Example:
# postgres://user:pass@ep-cool-darkness-123456.us-east-2.aws.neon.tech/neondb?sslmode=require
```

#### ğŸ“ Neon Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Node.js with pg (node-postgres)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: { rejectUnauthorized: false }
});

// Query
const { rows } = await pool.query('SELECT * FROM users');

// With parameters
const { rows } = await pool.query(
  'SELECT * FROM users WHERE email = $1',
  ['user@example.com']
);

// Transaction
const client = await pool.connect();
try {
  await client.query('BEGIN');
  await client.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [100, 1]);
  await client.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [100, 2]);
  await client.query('COMMIT');
} catch (e) {
  await client.query('ROLLBACK');
  throw e;
} finally {
  client.release();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Prisma ORM
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// prisma/schema.prisma
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

model User {
  id        Int      @id @default(autoincrement())
  email     String   @unique
  name      String?
  posts     Post[]
  createdAt DateTime @default(now())
}

model Post {
  id        Int      @id @default(autoincrement())
  title     String
  content   String?
  published Boolean  @default(false)
  author    User     @relation(fields: [authorId], references: [id])
  authorId  Int
  createdAt DateTime @default(now())
}

// Generate client
// npx prisma generate

// Use Prisma Client
import { PrismaClient } from '@prisma/client';

const prisma = new PrismaClient();

// Create user
const user = await prisma.user.create({
  data: {
    email: 'user@example.com',
    name: 'MrDib',
    posts: {
      create: [
        { title: 'First Post', content: 'Hello World!' }
      ]
    }
  },
  include: {
    posts: true
  }
});

// Query with relations
const users = await prisma.user.findMany({
  include: {
    posts: {
      where: { published: true }
    }
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Drizzle ORM (Lightweight alternative)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { drizzle } from 'drizzle-orm/node-postgres';
import { pgTable, serial, text, timestamp, boolean } from 'drizzle-orm/pg-core';
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL
});

const db = drizzle(pool);

// Define schema
const users = pgTable('users', {
  id: serial('id').primaryKey(),
  email: text('email').notNull().unique(),
  name: text('name'),
  createdAt: timestamp('created_at').defaultNow()
});

const posts = pgTable('posts', {
  id: serial('id').primaryKey(),
  title: text('title').notNull(),
  content: text('content'),
  published: boolean('published').default(false),
  authorId: serial('author_id').references(() => users.id),
  createdAt: timestamp('created_at').defaultNow()
});

// Query
const allUsers = await db.select().from(users);

// With where
const publishedPosts = await db
  .select()
  .from(posts)
  .where(eq(posts.published, true));

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATABASE BRANCHING (Git for Databases!)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Create development branch
neonctl branches create --project-id your-project --name dev

// Create feature branch from dev
neonctl branches create --project-id your-project --name feature-auth --parent dev

// Get connection string for branch
neonctl connection-string feature-auth

// Use different connection strings per environment:
// .env.development
DATABASE_URL=postgres://...@ep-dev-123.neon.tech/neondb

// .env.production
DATABASE_URL=postgres://...@ep-main-456.neon.tech/neondb

// When feature is ready, merge changes:
// 1. Test in feature branch
// 2. Deploy to production with main branch connection string
// 3. Delete feature branch
neonctl branches delete feature-auth

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// POINT-IN-TIME RESTORE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Restore to specific timestamp
neonctl branches create --project-id your-project \
  --name recovery \
  --parent main \
  --timestamp "2024-01-15T10:30:00Z"

// Use recovery branch to inspect/recover data
```

#### ğŸ’¡ Neon Pro Tips

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AUTOSCALING & COMPUTE OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Neon automatically scales compute based on load
// - Scales down to zero when idle (saves costs!)
// - Scales up when traffic increases

// Connection pooling (recommended)
import { neon, neonConfig } from '@neondatabase/serverless';

// For serverless environments
neonConfig.fetchConnectionCache = true;

const sql = neon(process.env.DATABASE_URL);

// Query (HTTP-based, no persistent connection)
const users = await sql`SELECT * FROM users`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COLD START OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Use HTTP queries for serverless (faster cold starts)
import { neon } from '@neondatabase/serverless';

const sql = neon(process.env.DATABASE_URL);

// Direct SQL execution (no connection overhead)
const result = await sql`
  SELECT * FROM users WHERE email = ${email}
`;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MIGRATION WORKFLOW
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Create branch for migration
neonctl branches create --name migration-001

// 2. Apply migration to branch
DATABASE_URL=<branch-connection-string> npx prisma migrate dev

// 3. Test thoroughly

// 4. Apply to production
DATABASE_URL=<main-connection-string> npx prisma migrate deploy

// 5. Delete migration branch
neonctl branches delete migration-001

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MONITORING & ANALYTICS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Check branch usage
neonctl branches list --project-id your-project

// View metrics in Neon dashboard:
// - Active time
// - Compute time
// - Data transfer
// - Storage size
```

---

### ğŸš‚ Railway PostgreSQL

**Integrated PostgreSQL with Railway platform**

```
ğŸŒ Website â†’ https://railway.app
ğŸ¯ Best For â†’ Full-stack apps on Railway platform
ğŸ’° Pricing  â†’ Usage-based (included in Railway project)
             ~$5-10/month for small DB
âš¡ Features â†’ Automatic backups, metrics, logs
ğŸ”§ Support  â†’ Any PostgreSQL client
```

#### ğŸ“¥ Railway PostgreSQL Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Add PostgreSQL to Railway project
railway add postgresql

# Connection string automatically available as:
# DATABASE_URL

# View variables
railway variables

# Connect locally for debugging
railway run psql

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BACKUP & RESTORE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Backups are automatic on Railway

# Download backup (via UI)
# Project â†’ PostgreSQL â†’ Backups â†’ Download

# Restore from backup
# Project â†’ PostgreSQL â†’ Backups â†’ Restore
```

#### ğŸ“ Railway PostgreSQL Usage

```javascript
// Same as standard PostgreSQL connection
import { Pool } from "pg";

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: { rejectUnauthorized: false },
});

// Railway provides these environment variables automatically:
// - DATABASE_URL (connection string)
// - PGHOST
// - PGPORT
// - PGUSER
// - PGPASSWORD
// - PGDATABASE
```

---

<div align="center">

## ğŸ¬ MySQL Hosting

_Popular relational database for web applications_ ğŸŒ

</div>

### ğŸŒ PlanetScale

**Serverless MySQL with database branching**

```
ğŸŒ Website â†’ https://planetscale.com
ğŸ¯ Best For â†’ Scalable MySQL apps, development workflows
ğŸ’° Pricing  â†’ Free: 5GB storage, 1 billion row reads/month
             Paid: $29/month for 10GB, 10 billion reads
âš¡ Features â†’ Branching, schema migrations, horizontal sharding
ğŸ”§ Support  â†’ Standard MySQL protocol, Prisma
```

#### ğŸ“¥ PlanetScale Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INSTALLATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install CLI
brew install planetscale/tap/pscale

# Or via script
curl -fsSL https://raw.githubusercontent.com/planetscale/cli/main/install.sh | sh

# Login
pscale auth login

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATABASE MANAGEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create database
pscale database create my-database --region us-east

# List databases
pscale database list

# Create branch (like git!)
pscale branch create my-database dev

# Create deploy request (like PR)
pscale deploy-request create my-database dev

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Get connection string
pscale connect my-database main --port 3309

# Connection string format:
# mysql://user:password@host/database?sslmode=require
```

#### ğŸ“ PlanetScale Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Prisma with PlanetScale
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// prisma/schema.prisma
datasource db {
  provider = "mysql"
  url      = env("DATABASE_URL")
  relationMode = "prisma" // Important for PlanetScale!
}

generator client {
  provider = "prisma-client-js"
}

model User {
  id        String   @id @default(cuid())
  email     String   @unique
  name      String?
  posts     Post[]
  createdAt DateTime @default(now())

  @@index([email])
}

model Post {
  id        String   @id @default(cuid())
  title     String
  content   String?  @db.Text
  published Boolean  @default(false)
  authorId  String
  author    User     @relation(fields: [authorId], references: [id])
  createdAt DateTime @default(now())

  @@index([authorId])
  @@index([published])
}

// Important: Use @@index instead of foreign key constraints
// PlanetScale doesn't support foreign keys

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Node.js with mysql2
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import mysql from 'mysql2/promise';

const connection = await mysql.createConnection({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_NAME,
  ssl: {
    rejectUnauthorized: true
  }
});

// Query
const [rows] = await connection.execute(
  'SELECT * FROM users WHERE email = ?',
  ['user@example.com']
);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATABASE BRANCHING WORKFLOW
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Create development branch
pscale branch create my-database dev

// 2. Connect to dev branch
pscale connect my-database dev --port 3309

// 3. Make schema changes on dev branch
// Run migrations with Prisma:
DATABASE_URL="mysql://..." npx prisma db push

// 4. Create deploy request
pscale deploy-request create my-database dev

// 5. Review schema diff in UI
// Shows what will change in production

// 6. Deploy to production
pscale deploy-request deploy my-database <deploy-request-number>

// 7. Delete dev branch (optional)
pscale branch delete my-database dev

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SCHEMA MIGRATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// PlanetScale handles migrations via branching
// No need for migration files!

// Workflow:
// 1. Create branch
// 2. Modify schema on branch (prisma db push)
// 3. Test changes
// 4. Create deploy request
// 5. Review and deploy

// View schema
pscale shell my-database main

// In shell:
SHOW TABLES;
DESCRIBE users;
```

#### ğŸ’¡ PlanetScale Pro Tips

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERFORMANCE OPTIMIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Add indexes for frequently queried columns
# In Prisma schema:
model User {
  email String @unique
  name  String

  @@index([email])
  @@index([name])
}

# 2. Use connection pooling
# PlanetScale has built-in connection pooling

# 3. Enable query insights
# View slow queries in PlanetScale dashboard
# Insights â†’ Query Statistics

# 4. Use read-only regions (Pro plan)
# Reduces latency for global users

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# View database insights
pscale database show my-database

# Monitor queries
# Dashboard â†’ Insights â†’ Queries

# Set up alerts
# Dashboard â†’ Settings â†’ Notifications

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BACKUP & RESTORE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Backups are automatic (daily)
# Restore by creating branch from backup point

# Create branch from backup
pscale branch create my-database recovery \
  --restore --timestamp "2024-01-15T10:00:00Z"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECURITY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create password with specific permissions
pscale password create my-database main password-name \
  --role read-only

# Delete password
pscale password delete my-database main password-id

# IP restrictions (Enterprise plan)
# Dashboard â†’ Settings â†’ IP Allow List
```

---

<div align="center">

## ğŸƒ MongoDB Hosting

_Flexible document database for modern apps_ ğŸ“„

</div>

### ğŸ“Š MongoDB Atlas

**Fully managed MongoDB in the cloud**

```
ğŸŒ Website â†’ https://mongodb.com/cloud/atlas
ğŸ¯ Best For â†’ Flexible schemas, rapid development, mobile apps
ğŸ’° Pricing  â†’ Free: 512MB storage (shared cluster)
             Paid: $9/month for dedicated cluster
âš¡ Features â†’ Atlas Search, Charts, Realm, Data API
ğŸ”§ Support  â†’ Official MongoDB drivers for all languages
```

#### ğŸ“¥ MongoDB Atlas Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Create account at mongodb.com/cloud/atlas
# 2. Create cluster (Free M0 tier available)
# 3. Create database user
# 4. Add IP address to whitelist (0.0.0.0/0 for all)
# 5. Get connection string

# Connection string format:
# mongodb+srv://<username>:<password>@cluster.mongodb.net/<dbname>?retryWrites=true&w=majority

# Install MongoDB driver
npm install mongodb

# Or Mongoose ODM
npm install mongoose
```

#### ğŸ“ MongoDB Atlas Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Native MongoDB Driver
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { MongoClient } from "mongodb";

const client = new MongoClient(process.env.MONGODB_URI);

async function run() {
  try {
    await client.connect();
    const database = client.db("myapp");
    const users = database.collection("users");

    // Insert
    const result = await users.insertOne({
      name: "MrDib",
      email: "email@example.com",
      createdAt: new Date(),
    });

    // Find
    const user = await users.findOne({ email: "email@example.com" });

    // Update
    await users.updateOne(
      { email: "email@example.com" },
      { $set: { name: "Updated Name" } }
    );

    // Delete
    await users.deleteOne({ email: "email@example.com" });

    // Find many with filter
    const allUsers = await users
      .find({
        createdAt: { $gte: new Date("2024-01-01") },
      })
      .toArray();
  } finally {
    await client.close();
  }
}

run().catch(console.error);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Mongoose ODM (Object Data Modeling)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import mongoose from "mongoose";

// Connect
await mongoose.connect(process.env.MONGODB_URI);

// Define Schema
const userSchema = new mongoose.Schema({
  name: {
    type: String,
    required: true,
    trim: true,
  },
  email: {
    type: String,
    required: true,
    unique: true,
    lowercase: true,
    trim: true,
  },
  age: {
    type: Number,
    min: 0,
    max: 120,
  },
  posts: [
    {
      type: mongoose.Schema.Types.ObjectId,
      ref: "Post",
    },
  ],
  createdAt: {
    type: Date,
    default: Date.now,
  },
});

// Add methods
userSchema.methods.getFullName = function () {
  return this.name;
};

// Add static methods
userSchema.statics.findByEmail = function (email) {
  return this.findOne({ email });
};

// Add indexes
userSchema.index({ email: 1 });
userSchema.index({ createdAt: -1 });

// Create Model
const User = mongoose.model("User", userSchema);

// CRUD Operations

// Create
const user = new User({
  name: "MrDib",
  email: "email@example.com",
  age: 25,
});
await user.save();

// Or
const newUser = await User.create({
  name: "MrDib",
  email: "email@example.com",
});

// Read
const users = await User.find();
const user = await User.findById(userId);
const user = await User.findOne({ email: "email@example.com" });

// With conditions
const activeUsers = await User.find({
  age: { $gte: 18 },
  createdAt: { $gte: new Date("2024-01-01") },
});

// With population (joins)
const userWithPosts = await User.findById(userId).populate("posts");

// Update
await User.findByIdAndUpdate(
  userId,
  { name: "Updated Name" },
  { new: true } // Return updated document
);

// Or
user.name = "Updated Name";
await user.save();

// Delete
await User.findByIdAndDelete(userId);

// Or
await User.deleteOne({ email: "email@example.com" });

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AGGREGATION PIPELINE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Complex queries with aggregation
const stats = await User.aggregate([
  // Match stage (filter)
  {
    $match: {
      createdAt: { $gte: new Date("2024-01-01") },
    },
  },
  // Group stage
  {
    $group: {
      _id: "$age",
      count: { $sum: 1 },
      avgAge: { $avg: "$age" },
    },
  },
  // Sort stage
  {
    $sort: { count: -1 },
  },
  // Limit stage
  {
    $limit: 10,
  },
]);

// Lookup (join) example
const usersWithPosts = await User.aggregate([
  {
    $lookup: {
      from: "posts",
      localField: "_id",
      foreignField: "authorId",
      as: "posts",
    },
  },
  {
    $project: {
      name: 1,
      email: 1,
      postCount: { $size: "$posts" },
    },
  },
]);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TEXT SEARCH
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Create text index
userSchema.index({ name: "text", bio: "text" });

// Search
const results = await User.find({
  $text: { $search: "developer nodejs" },
});

// With Atlas Search (advanced)
const searchResults = await User.aggregate([
  {
    $search: {
      index: "default",
      text: {
        query: "developer",
        path: ["name", "bio"],
      },
    },
  },
]);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TRANSACTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const session = await mongoose.startSession();
session.startTransaction();

try {
  // Operations in transaction
  await User.create([{ name: "User 1" }], { session });
  await Post.create([{ title: "Post 1", authorId: userId }], { session });

  // Commit transaction
  await session.commitTransaction();
} catch (error) {
  // Rollback on error
  await session.abortTransaction();
  throw error;
} finally {
  session.endSession();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CHANGE STREAMS (Real-time)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const changeStream = User.watch();

changeStream.on("change", (change) => {
  console.log("Change detected:", change);

  if (change.operationType === "insert") {
    console.log("New user:", change.fullDocument);
  }

  if (change.operationType === "update") {
    console.log("Updated user:", change.documentKey);
  }
});

// Close stream
changeStream.close();
```

#### ğŸ’¡ MongoDB Atlas Pro Tips

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PERFORMANCE OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Create compound indexes for common queries
userSchema.index({ email: 1, createdAt: -1 });

// 2. Use projection to fetch only needed fields
const users = await User.find({}, "name email"); // Only name and email

// 3. Use lean() for read-only queries (faster)
const users = await User.find().lean(); // Returns plain objects

// 4. Use select() to exclude fields
const users = await User.find().select("-password -__v");

// 5. Pagination
const page = 1;
const limit = 10;
const users = await User.find()
  .skip((page - 1) * limit)
  .limit(limit);

// 6. Monitor slow queries in Atlas
// Dashboard â†’ Performance Advisor

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SCHEMA VALIDATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const userSchema = new mongoose.Schema({
  email: {
    type: String,
    required: [true, "Email is required"],
    unique: true,
    validate: {
      validator: function (v) {
        return /^[\w-\.]+@([\w-]+\.)+[\w-]{2,4}$/.test(v);
      },
      message: "Invalid email format",
    },
  },
  age: {
    type: Number,
    min: [0, "Age cannot be negative"],
    max: [120, "Age too high"],
  },
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MIDDLEWARE (Hooks)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Pre-save hook
userSchema.pre("save", async function (next) {
  // Hash password before saving
  if (this.isModified("password")) {
    this.password = await bcrypt.hash(this.password, 10);
  }
  next();
});

// Post-save hook
userSchema.post("save", function (doc) {
  console.log("User saved:", doc._id);
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ATLAS FEATURES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Atlas Search (Elasticsearch-like search)
// Create search index in Atlas UI
// Use $search in aggregation

// 2. Atlas Charts (Data visualization)
// Create charts in Atlas UI

// 3. Atlas Data API (REST API for MongoDB)
// Enable in Atlas UI
// Access via HTTPS endpoint

// 4. Atlas App Services (formerly Realm)
// Backend-as-a-Service
// Authentication, GraphQL, Triggers

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// BACKUP & RESTORE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Backups are automatic in Atlas (continuous)
// Restore via Atlas UI:
// Cluster â†’ Backup â†’ Restore

// Point-in-time restore available on M10+ clusters
```

---

<div align="center">

## âš¡ Redis Hosting

_In-memory data store for caching and real-time apps_ ğŸš€

</div>

### ğŸš€ Upstash

**Serverless Redis with per-request pricing**

```
ğŸŒ Website â†’ https://upstash.com
ğŸ¯ Best For â†’ Serverless apps, edge computing, pay-per-use
ğŸ’° Pricing  â†’ Free: 10k commands/day
             Paid: $0.2 per 100k requests
âš¡ Features â†’ Global replication, REST API, low latency
ğŸ”§ Support  â†’ Redis protocol, REST API, SDKs
```

#### ğŸ“¥ Upstash Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Create account at upstash.com
# 2. Create database
# 3. Get REST URL and token

# Install SDK
npm install @upstash/redis
```

#### ğŸ“ Upstash Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Upstash Redis SDK (Recommended for serverless)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { Redis } from "@upstash/redis";

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL,
  token: process.env.UPSTASH_REDIS_REST_TOKEN,
});

// Basic operations
await redis.set("key", "value");
const value = await redis.get("key");
await redis.del("key");

// With expiration (TTL in seconds)
await redis.setex("session:123", 3600, "session-data"); // Expires in 1 hour

// Increment
await redis.incr("counter");
await redis.incrby("counter", 5);

// Lists
await redis.lpush("queue", "item1", "item2");
await redis.rpush("queue", "item3");
const item = await redis.lpop("queue");
const items = await redis.lrange("queue", 0, -1);

// Sets
await redis.sadd("tags", "javascript", "nodejs", "redis");
const tags = await redis.smembers("tags");
const exists = await redis.sismember("tags", "nodejs");

// Hashes (objects)
await redis.hset("user:123", {
  name: "MrDib",
  email: "email@example.com",
  age: 25,
});
const user = await redis.hgetall("user:123");
const name = await redis.hget("user:123", "name");

// Sorted Sets (with scores)
await redis.zadd("leaderboard", { score: 100, member: "user1" });
await redis.zadd("leaderboard", { score: 200, member: "user2" });
const top = await redis.zrange("leaderboard", 0, 9, { rev: true }); // Top 10

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CACHING PATTERN
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function getCachedData(key, fetchFn, ttl = 3600) {
  // Try cache first
  const cached = await redis.get(key);
  if (cached) {
    return typeof cached === "string" ? JSON.parse(cached) : cached;
  }

  // Fetch data
  const data = await fetchFn();

  // Cache it
  await redis.setex(key, ttl, JSON.stringify(data));

  return data;
}

// Usage
const posts = await getCachedData(
  "posts:all",
  async () => {
    // Fetch from database
    return await db.query("SELECT * FROM posts");
  },
  300 // 5 minutes
);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// RATE LIMITING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { Ratelimit } from "@upstash/ratelimit";

const ratelimit = new Ratelimit({
  redis,
  limiter: Ratelimit.slidingWindow(10, "10 s"), // 10 requests per 10 seconds
  analytics: true,
});

async function checkRateLimit(identifier) {
  const { success, limit, remaining, reset } = await ratelimit.limit(
    identifier
  );

  if (!success) {
    throw new Error("Rate limit exceeded");
  }

  return { remaining, reset };
}

// Usage in API route
app.post("/api/action", async (req, res) => {
  const ip = req.ip;

  try {
    await checkRateLimit(ip);
    // Process request
    res.json({ success: true });
  } catch (error) {
    res.status(429).json({ error: "Too many requests" });
  }
});

// Different rate limit strategies
const ratelimitStrict = new Ratelimit({
  redis,
  limiter: Ratelimit.fixedWindow(5, "1 m"), // 5 requests per minute
});

const ratelimitToken = new Ratelimit({
  redis,
  limiter: Ratelimit.tokenBucket(10, "1 m", 50), // Token bucket algorithm
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SESSION STORAGE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function createSession(userId, sessionData) {
  const sessionId = crypto.randomUUID();
  const key = `session:${sessionId}`;

  await redis.setex(
    key,
    86400, // 24 hours
    JSON.stringify({
      userId,
      ...sessionData,
      createdAt: Date.now(),
    })
  );

  return sessionId;
}

async function getSession(sessionId) {
  const key = `session:${sessionId}`;
  const data = await redis.get(key);

  if (!data) return null;

  return typeof data === "string" ? JSON.parse(data) : data;
}

async function destroySession(sessionId) {
  const key = `session:${sessionId}`;
  await redis.del(key);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DISTRIBUTED LOCKS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function acquireLock(resource, ttl = 10000) {
  const lockKey = `lock:${resource}`;
  const lockId = crypto.randomUUID();

  const acquired = await redis.set(lockKey, lockId, {
    nx: true, // Only set if not exists
    px: ttl, // TTL in milliseconds
  });

  if (!acquired) return null;

  return {
    id: lockId,
    release: async () => {
      // Only release if we still own the lock
      const current = await redis.get(lockKey);
      if (current === lockId) {
        await redis.del(lockKey);
      }
    },
  };
}

// Usage
const lock = await acquireLock("critical-section", 5000);
if (!lock) {
  throw new Error("Could not acquire lock");
}

try {
  // Critical section
  await performCriticalOperation();
} finally {
  await lock.release();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PUB/SUB (Real-time messaging)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Publisher
await redis.publish(
  "notifications",
  JSON.stringify({
    type: "new_message",
    userId: "123",
    message: "Hello!",
  })
);

// Subscriber (in separate process/worker)
const subscriber = redis.duplicate();
await subscriber.subscribe("notifications", (message) => {
  const data = JSON.parse(message);
  console.log("Received:", data);
});
```

#### ğŸ’¡ Upstash Pro Tips

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// BEST PRACTICES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Use pipelines for multiple commands
const pipeline = redis.pipeline();
pipeline.set("key1", "value1");
pipeline.set("key2", "value2");
pipeline.incr("counter");
await pipeline.exec();

// 2. Set appropriate TTLs
// - Sessions: 24 hours
// - Cache: 5-60 minutes
// - Rate limits: Based on window

// 3. Use namespace prefixes
const CACHE_PREFIX = "cache:";
const SESSION_PREFIX = "session:";
const LOCK_PREFIX = "lock:";

await redis.set(`${CACHE_PREFIX}posts`, data);

// 4. Monitor usage
// Dashboard â†’ Metrics
// - Commands per second
// - Memory usage
// - Hit rate

// 5. Use global replication for low latency
// Enable in Upstash dashboard

// 6. Handle errors gracefully
try {
  const value = await redis.get("key");
} catch (error) {
  console.error("Redis error:", error);
  // Fallback to database or return stale data
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COST OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Use longer TTLs for static data
await redis.setex("config", 86400, data); // 24 hours

// 2. Batch operations with pipeline
// Counts as fewer commands

// 3. Use appropriate data structures
// - String: Simple values
// - Hash: Objects
// - Set: Unique items
// - Sorted Set: Ranked data

// 4. Delete expired keys
// Upstash automatically deletes expired keys

// 5. Monitor command count
// Upstash dashboard â†’ Usage
```

---

<div align="center">

## ğŸ”¥ Firebase & Firestore

_Google's real-time database platform_ ğŸ“±

</div>

### ğŸ”¥ Firebase

**Complete app development platform with real-time database**

```
ğŸŒ Website â†’ https://firebase.google.com
ğŸ¯ Best For â†’ Mobile apps, real-time features, rapid development
ğŸ’° Pricing  â†’ Free: 1GB storage, 50k reads/day
             Paid: Pay-as-you-go (Blaze plan)
âš¡ Features â†’ Firestore, Auth, Storage, Functions, Hosting
ğŸ”§ Support  â†’ JavaScript, iOS, Android, Flutter, Unity
```

#### ğŸ“¥ Firebase Setup

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INSTALLATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install Firebase SDK
npm install firebase

# Install Firebase Admin SDK (for server-side)
npm install firebase-admin

# Install Firebase CLI
npm install -g firebase-tools

# Login
firebase login

# Initialize project
firebase init

# Deploy
firebase deploy
```

#### ğŸ“ Firebase Configuration

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Client-side Firebase Setup
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { initializeApp } from "firebase/app";
import {
  getFirestore,
  collection,
  doc,
  getDocs,
  getDoc,
  addDoc,
  setDoc,
  updateDoc,
  deleteDoc,
  query,
  where,
  orderBy,
  limit,
  onSnapshot,
} from "firebase/firestore";
import {
  getAuth,
  createUserWithEmailAndPassword,
  signInWithEmailAndPassword,
  signOut,
  onAuthStateChanged,
} from "firebase/auth";
import { getStorage, ref, uploadBytes, getDownloadURL } from "firebase/storage";

// Firebase config (from Firebase Console)
const firebaseConfig = {
  apiKey: process.env.NEXT_PUBLIC_FIREBASE_API_KEY,
  authDomain: process.env.NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN,
  projectId: process.env.NEXT_PUBLIC_FIREBASE_PROJECT_ID,
  storageBucket: process.env.NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET,
  messagingSenderId: process.env.NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID,
  appId: process.env.NEXT_PUBLIC_FIREBASE_APP_ID,
};

// Initialize Firebase
const app = initializeApp(firebaseConfig);
const db = getFirestore(app);
const auth = getAuth(app);
const storage = getStorage(app);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FIRESTORE CRUD OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// CREATE (add with auto-generated ID)
async function createPost(data) {
  const docRef = await addDoc(collection(db, "posts"), {
    title: data.title,
    content: data.content,
    authorId: data.authorId,
    createdAt: new Date(),
    published: false,
  });

  console.log("Document created with ID:", docRef.id);
  return docRef.id;
}

// CREATE (set with custom ID)
async function createUserProfile(userId, data) {
  await setDoc(doc(db, "users", userId), {
    name: data.name,
    email: data.email,
    createdAt: new Date(),
  });
}

// READ (single document)
async function getPost(postId) {
  const docRef = doc(db, "posts", postId);
  const docSnap = await getDoc(docRef);

  if (docSnap.exists()) {
    return { id: docSnap.id, ...docSnap.data() };
  } else {
    throw new Error("Post not found");
  }
}

// READ (all documents)
async function getAllPosts() {
  const querySnapshot = await getDocs(collection(db, "posts"));
  const posts = [];

  querySnapshot.forEach((doc) => {
    posts.push({ id: doc.id, ...doc.data() });
  });

  return posts;
}

// READ (with query)
async function getPublishedPosts() {
  const q = query(
    collection(db, "posts"),
    where("published", "==", true),
    orderBy("createdAt", "desc"),
    limit(10)
  );

  const querySnapshot = await getDocs(q);
  const posts = [];

  querySnapshot.forEach((doc) => {
    posts.push({ id: doc.id, ...doc.data() });
  });

  return posts;
}

// UPDATE
async function updatePost(postId, updates) {
  const docRef = doc(db, "posts", postId);
  await updateDoc(docRef, {
    ...updates,
    updatedAt: new Date(),
  });
}

// DELETE
async function deletePost(postId) {
  await deleteDoc(doc(db, "posts", postId));
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// REAL-TIME LISTENERS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Listen to single document
function listenToPost(postId, callback) {
  const unsubscribe = onSnapshot(
    doc(db, "posts", postId),
    (doc) => {
      if (doc.exists()) {
        callback({ id: doc.id, ...doc.data() });
      }
    },
    (error) => {
      console.error("Error listening to post:", error);
    }
  );

  return unsubscribe; // Call to stop listening
}

// Listen to collection with query
function listenToPosts(callback) {
  const q = query(
    collection(db, "posts"),
    where("published", "==", true),
    orderBy("createdAt", "desc")
  );

  const unsubscribe = onSnapshot(q, (querySnapshot) => {
    const posts = [];
    querySnapshot.forEach((doc) => {
      posts.push({ id: doc.id, ...doc.data() });
    });
    callback(posts);
  });

  return unsubscribe;
}

// Usage in React
import { useEffect, useState } from "react";

function Posts() {
  const [posts, setPosts] = useState([]);

  useEffect(() => {
    const unsubscribe = listenToPosts(setPosts);
    return () => unsubscribe(); // Cleanup
  }, []);

  return (
    <div>
      {posts.map((post) => (
        <div key={post.id}>{post.title}</div>
      ))}
    </div>
  );
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AUTHENTICATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Sign up
async function signUp(email, password) {
  const userCredential = await createUserWithEmailAndPassword(
    auth,
    email,
    password
  );
  return userCredential.user;
}

// Sign in
async function signIn(email, password) {
  const userCredential = await signInWithEmailAndPassword(
    auth,
    email,
    password
  );
  return userCredential.user;
}

// Sign out
async function logout() {
  await signOut(auth);
}

// Listen to auth state
onAuthStateChanged(auth, (user) => {
  if (user) {
    console.log("User signed in:", user.uid);
  } else {
    console.log("User signed out");
  }
});

// Get current user
const currentUser = auth.currentUser;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FILE STORAGE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Upload file
async function uploadFile(file, path) {
  const storageRef = ref(storage, path);
  const snapshot = await uploadBytes(storageRef, file);
  const downloadURL = await getDownloadURL(snapshot.ref);
  return downloadURL;
}

// Usage
async function uploadAvatar(file, userId) {
  const url = await uploadFile(file, `avatars/${userId}`);

  // Update user profile with avatar URL
  await updateDoc(doc(db, "users", userId), {
    avatarUrl: url,
  });

  return url;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SUBCOLLECTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Add comment to post
async function addComment(postId, commentData) {
  const commentsRef = collection(db, "posts", postId, "comments");
  const docRef = await addDoc(commentsRef, {
    text: commentData.text,
    authorId: commentData.authorId,
    createdAt: new Date(),
  });
  return docRef.id;
}

// Get comments for post
async function getComments(postId) {
  const commentsRef = collection(db, "posts", postId, "comments");
  const q = query(commentsRef, orderBy("createdAt", "desc"));
  const querySnapshot = await getDocs(q);

  const comments = [];
  querySnapshot.forEach((doc) => {
    comments.push({ id: doc.id, ...doc.data() });
  });

  return comments;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// BATCH OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { writeBatch } from "firebase/firestore";

async function batchUpdate() {
  const batch = writeBatch(db);

  // Add multiple operations
  const ref1 = doc(db, "posts", "post1");
  batch.update(ref1, { published: true });

  const ref2 = doc(db, "posts", "post2");
  batch.update(ref2, { published: true });

  const ref3 = doc(db, "posts", "post3");
  batch.delete(ref3);

  // Commit batch (atomic operation)
  await batch.commit();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TRANSACTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { runTransaction } from "firebase/firestore";

async function transferPoints(fromUserId, toUserId, amount) {
  await runTransaction(db, async (transaction) => {
    const fromRef = doc(db, "users", fromUserId);
    const toRef = doc(db, "users", toUserId);

    const fromDoc = await transaction.get(fromRef);
    const toDoc = await transaction.get(toRef);

    if (!fromDoc.exists() || !toDoc.exists()) {
      throw new Error("User not found");
    }

    const fromPoints = fromDoc.data().points;
    const toPoints = toDoc.data().points;

    if (fromPoints < amount) {
      throw new Error("Insufficient points");
    }

    transaction.update(fromRef, { points: fromPoints - amount });
    transaction.update(toRef, { points: toPoints + amount });
  });
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SECURITY RULES (Firestore Rules)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/*
rules_version = '2';
service cloud.firestore {
  match /databases/{database}/documents {
    // Users can read/write their own profile
    match /users/{userId} {
      allow read: if true;
      allow write: if request.auth != null && request.auth.uid == userId;
    }

    // Posts
    match /posts/{postId} {
      // Anyone can read published posts
      allow read: if resource.data.published == true;

      // Authors can read their own posts
      allow read: if request.auth != null
                  && request.auth.uid == resource.data.authorId;

      // Authenticated users can create posts
      allow create: if request.auth != null
                    && request.resource.data.authorId == request.auth.uid;

      // Authors can update their own posts
      allow update: if request.auth != null
                    && request.auth.uid == resource.data.authorId;

      // Authors can delete their own posts
      allow delete: if request.auth != null
                    && request.auth.uid == resource.data.authorId;

      // Comments subcollection
      match /comments/{commentId} {
        allow read: if true;
        allow create: if request.auth != null;
        allow update, delete: if request.auth != null
                              && request.auth.uid == resource.data.authorId;
      }
    }
  }
}
*/
```

#### ğŸ’¡ Firebase Pro Tips

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PERFORMANCE OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Use indexes for complex queries
// Firebase will prompt you to create them
// Or create manually in Firebase Console

// 2. Denormalize data for faster reads
// Instead of joins, duplicate data
const post = {
  id: "post1",
  title: "My Post",
  author: {
    id: "user1",
    name: "MrDib",
    avatar: "url",
  },
};

// 3. Use pagination
import { startAfter, getDocs, query, limit } from "firebase/firestore";

async function getPaginatedPosts(lastDoc = null, pageSize = 10) {
  let q = query(
    collection(db, "posts"),
    orderBy("createdAt", "desc"),
    limit(pageSize)
  );

  if (lastDoc) {
    q = query(q, startAfter(lastDoc));
  }

  const snapshot = await getDocs(q);
  const posts = [];
  let lastVisible = null;

  snapshot.forEach((doc) => {
    posts.push({ id: doc.id, ...doc.data() });
    lastVisible = doc;
  });

  return { posts, lastVisible };
}

// 4. Use offline persistence
import { enableIndexedDbPersistence } from "firebase/firestore";

enableIndexedDbPersistence(db).catch((err) => {
  if (err.code === "failed-precondition") {
    console.log("Multiple tabs open");
  } else if (err.code === "unimplemented") {
    console.log("Browser not supported");
  }
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COST OPTIMIZATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. Minimize reads with listeners
// Instead of polling, use onSnapshot

// 2. Use get() instead of onSnapshot when real-time not needed
const posts = await getDocs(collection(db, "posts"));

// 3. Limit query results
const q = query(collection(db, "posts"), limit(10));

// 4. Use indexes to avoid full collection scans

// 5. Monitor usage in Firebase Console
// Usage and billing â†’ Usage

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SERVER-SIDE (ADMIN SDK)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Use Admin SDK for server-side operations
import admin from "firebase-admin";

admin.initializeApp({
  credential: admin.credential.cert({
    projectId: process.env.FIREBASE_PROJECT_ID,
    clientEmail: process.env.FIREBASE_CLIENT_EMAIL,
    privateKey: process.env.FIREBASE_PRIVATE_KEY.replace(/\\n/g, "\n"),
  }),
});

const db = admin.firestore();

// No security rules apply - full access
const users = await db.collection("users").get();

// Verify ID token
const decodedToken = await admin.auth().verifyIdToken(idToken);
const uid = decodedToken.uid;
```

---

<div align="center">

## ğŸŒ Distributed Databases

_Global scale and high availability_ ğŸŒ

</div>

### Comparison Overview

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DISTRIBUTED DATABASE COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature      â”‚ CockroachDB â”‚ FaunaDB      â”‚ DynamoDB    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type         â”‚ SQL         â”‚ Document     â”‚ Key-Value   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Consistency  â”‚ Strong      â”‚ Strong       â”‚ Eventual/   â”‚
â”‚              â”‚             â”‚              â”‚ Strong      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Multi-region â”‚ âœ… Native   â”‚ âœ… Native    â”‚ âœ… Setup    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Scaling      â”‚ Horizontal  â”‚ Serverless   â”‚ Automatic   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Query Lang   â”‚ SQL         â”‚ FQL/GraphQL  â”‚ PartiQL     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Use Case     â”‚ RDBMS alt   â”‚ Serverless   â”‚ AWS apps    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Free Tier    â”‚ Limited     â”‚ Generous     â”‚ 25GB        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ CHOOSE COCKROACHDB IF:
â€¢ Need SQL with global distribution
â€¢ Replacing PostgreSQL at scale
â€¢ Strong consistency required
â€¢ Complex transactions needed

ğŸ¯ CHOOSE FAUNADB IF:
â€¢ Building serverless apps
â€¢ Need flexible schema
â€¢ GraphQL API preferred
â€¢ Pay-per-use model

ğŸ¯ CHOOSE DYNAMODB IF:
â€¢ Deep AWS integration
â€¢ Key-value access patterns
â€¢ Need predictable low latency
â€¢ AWS expertise available
```

---

<div align="center">

## ğŸ”„ Database Migration

_Moving data safely and efficiently_ ğŸšš

</div>

### Migration Strategies

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SCHEMA MIGRATIONS (SQL)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Using Prisma Migrate
npx prisma migrate dev --name add_users_table
npx prisma migrate deploy

# Using Knex.js
npx knex migrate:make add_users_table
npx knex migrate:latest
npx knex migrate:rollback

# Using Flyway
flyway migrate
flyway undo

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA MIGRATION BETWEEN DATABASES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Export from PostgreSQL
pg_dump -h source-host -U user -d dbname > backup.sql

# Import to new PostgreSQL
psql -h target-host -U user -d dbname < backup.sql

# MongoDB export
mongodump --uri="mongodb://source-uri" --out=/backup

# MongoDB import
mongorestore --uri="mongodb://target-uri" /backup

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MIGRATION TOOLS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# pgLoader (PostgreSQL to anything)
brew install pgloader

# Example: MySQL to PostgreSQL
pgloader mysql://user:pass@localhost/dbname \
         postgresql://user:pass@localhost/dbname

# AWS Database Migration Service (DMS)
# For large-scale migrations
# Supports: Oracle, SQL Server, MySQL, PostgreSQL, MongoDB
```

### Migration Example Scripts

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATA MIGRATION SCRIPT (MongoDB to PostgreSQL)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { MongoClient } from "mongodb";
import { Pool } from "pg";

const mongoClient = new MongoClient(process.env.MONGODB_URI);
const pgPool = new Pool({ connectionString: process.env.DATABASE_URL });

async function migrateUsers() {
  try {
    await mongoClient.connect();
    const mongodb = mongoClient.db("myapp");
    const users = mongodb.collection("users");

    // Get all users from MongoDB
    const cursor = users.find();
    let count = 0;

    while (await cursor.hasNext()) {
      const user = await cursor.next();

      // Insert into PostgreSQL
      await pgPool.query(
        `INSERT INTO users (id, name, email, created_at)
         VALUES ($1, $2, $3, $4)
         ON CONFLICT (id) DO NOTHING`,
        [user._id.toString(), user.name, user.email, user.createdAt]
      );

      count++;
      if (count % 100 === 0) {
        console.log(`Migrated ${count} users...`);
      }
    }

    console.log(`âœ… Migration complete: ${count} users`);
  } catch (error) {
    console.error("âŒ Migration failed:", error);
  } finally {
    await mongoClient.close();
    await pgPool.end();
  }
}

migrateUsers();

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ZERO-DOWNTIME MIGRATION STRATEGY
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/*
DUAL-WRITE PATTERN:

Phase 1: Setup
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â”‚    â†“         â”‚
â”‚  Old DB      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2: Dual Write (start writing to both)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â”‚    â†“   â†“     â”‚
â”‚  Old   New   â”‚
â”‚   DB    DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 3: Backfill (copy old data to new)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â”‚    â†“   â†“     â”‚
â”‚  Old   New   â”‚
â”‚   DB    DB   â”‚
â”‚     â†“       â”‚
â”‚  Migration   â”‚
â”‚   Script     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 4: Read from New (verify)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â”‚    â†“   â†“     â”‚
â”‚  Old   New   â”‚â† Read from here
â”‚   DB    DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 5: Stop dual write
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application  â”‚
â”‚       â†“      â”‚
â”‚      New     â”‚
â”‚       DB     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 6: Remove old DB
*/

// Implementation example
class DualWriteUserRepository {
  constructor(oldDb, newDb) {
    this.oldDb = oldDb;
    this.newDb = newDb;
    this.readFromNew = process.env.READ_FROM_NEW === "true";
  }

  async create(userData) {
    // Write to both databases
    const [oldResult, newResult] = await Promise.all([
      this.oldDb.users.insertOne(userData),
      this.newDb.query("INSERT INTO users ..."),
    ]);

    return this.readFromNew ? newResult : oldResult;
  }

  async findById(id) {
    if (this.readFromNew) {
      // Try new DB first
      const result = await this.newDb.query(
        "SELECT * FROM users WHERE id = $1",
        [id]
      );
      if (result.rows.length > 0) return result.rows[0];

      // Fallback to old DB
      return await this.oldDb.users.findOne({ _id: id });
    } else {
      return await this.oldDb.users.findOne({ _id: id });
    }
  }
}
```

---

<div align="center">

## ğŸ” Security Best Practices

_Protect your data_ ğŸ›¡ï¸

</div>

### Security Checklist

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION SECURITY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Always use SSL/TLS connections
   postgresql://user:pass@host/db?sslmode=require

âœ… Never commit connection strings to Git
   Use environment variables

âœ… Rotate database passwords regularly
   Update every 90 days minimum

âœ… Use connection pooling with limits
   Prevent connection exhaustion attacks

âœ… Restrict database access by IP
   Whitelist only necessary IPs

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTHENTICATION & AUTHORIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Use separate database users per service
   app-user (read/write)
   readonly-user (read only)
   admin-user (full access)

âœ… Grant minimum required permissions
   GRANT SELECT, INSERT ON users TO app_user;

âœ… Never use root/admin credentials in apps
   Create dedicated service accounts

âœ… Implement Row-Level Security (RLS)
   PostgreSQL/Supabase feature

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SQL INJECTION PREVENTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# âŒ NEVER do this (vulnerable to SQL injection)
const query = `SELECT * FROM users WHERE email = '${userInput}'`;

# âœ… ALWAYS use parameterized queries
const { rows } = await pool.query(
  'SELECT * FROM users WHERE email = $1',
  [userInput]
);

# âœ… Use ORMs with built-in protection
const user = await prisma.user.findUnique({
  where: { email: userInput }
});

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA ENCRYPTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Encrypt data at rest
   Enable in database settings

âœ… Encrypt data in transit (SSL/TLS)
   Enforce encrypted connections

âœ… Encrypt sensitive fields (app-level)
   Passwords, SSN, credit cards

# Example: Encrypt sensitive data
import crypto from 'crypto';

const algorithm = 'aes-256-gcm';
const key = Buffer.from(process.env.ENCRYPTION_KEY, 'hex');

function encrypt(text) {
  const iv = crypto.randomBytes(16);
  const cipher = crypto.createCipheriv(algorithm, key, iv);

  let encrypted = cipher.update(text, 'utf8', 'hex');
  encrypted += cipher.final('hex');

  const authTag = cipher.getAuthTag();

  return {
    encrypted,
    iv: iv.toString('hex'),
    authTag: authTag.toString('hex')
  };
}

function decrypt(encrypted, iv, authTag) {
  const decipher = crypto.createDecipheriv(
    algorithm,
    key,
    Buffer.from(iv, 'hex')
  );

  decipher.setAuthTag(Buffer.from(authTag, 'hex'));

  let decrypted = decipher.update(encrypted, 'hex', 'utf8');
  decrypted += decipher.final('utf8');

  return decrypted;
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BACKUP & DISASTER RECOVERY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Enable automatic backups
   Daily minimum, hourly for critical data

âœ… Test restore procedures regularly
   Verify backups actually work

âœ… Store backups in different region/provider
   Protection against regional outages

âœ… Encrypt backup files
   Prevent data leaks from backups

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING & AUDITING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Enable query logging
   Track suspicious queries

âœ… Monitor failed login attempts
   Detect brute force attacks

âœ… Set up alerts for unusual activity
   Large data exports, schema changes

âœ… Regular security audits
   Review permissions, users, queries

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COMPLIANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… GDPR compliance (if EU users)
   - Right to deletion
   - Right to export data
   - Data retention policies

âœ… PCI DSS (if handling payment data)
   - Encrypt card data
   - Secure network
   - Regular testing

âœ… HIPAA (if healthcare data)
   - Access controls
   - Audit trails
   - Encryption
```

---

<div align="center">

## ğŸ“Š Performance Optimization

_Make your database blazing fast_ âš¡

</div>

### Optimization Strategies

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- INDEXING (Most Important!)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Single column index
CREATE INDEX idx_users_email ON users(email);

-- Composite index (order matters!)
CREATE INDEX idx_posts_user_created ON posts(user_id, created_at DESC);

-- Partial index (smaller, faster)
CREATE INDEX idx_posts_published ON posts(published)
WHERE published = true;

-- Full-text search index (PostgreSQL)
CREATE INDEX idx_posts_search ON posts
USING gin(to_tsvector('english', title || ' ' || content));

-- View index usage
-- PostgreSQL
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- QUERY OPTIMIZATION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Use EXPLAIN to analyze queries
EXPLAIN ANALYZE
SELECT * FROM posts
WHERE user_id = 123
AND published = true
ORDER BY created_at DESC;

-- Avoid SELECT *
-- âŒ Bad
SELECT * FROM users;

-- âœ… Good
SELECT id, name, email FROM users;

-- Use LIMIT for large result sets
SELECT * FROM posts
WHERE published = true
ORDER BY created_at DESC
LIMIT 20;

-- Use COUNT(*) efficiently
-- âŒ Slow for large tables
SELECT COUNT(*) FROM posts;

-- âœ… Faster (if approximate is okay)
SELECT reltuples::bigint AS estimate
FROM pg_class
WHERE relname = 'posts';

-- Avoid N+1 queries
-- âŒ Bad (N+1 queries)
const posts = await db.query('SELECT * FROM posts');
for (const post of posts) {
    post.author = await db.query(
        'SELECT * FROM users WHERE id = $1',
        [post.user_id]
    );
}

-- âœ… Good (1 query with JOIN)
SELECT
    posts.*,
    users.name as author_name,
    users.email as author_email
FROM posts
JOIN users ON posts.user_id = users.id;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CONNECTION POOLING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import { Pool } from 'pg';

const pool = new Pool({
    connectionString: process.env.DATABASE_URL,
    max: 20,                    // Maximum connections
    idleTimeoutMillis: 30000,   // Close idle connections after 30s
    connectionTimeoutMillis: 2000, // Timeout for new connections
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CACHING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Application-level caching with Redis
import { Redis } from '@upstash/redis';

const redis = new Redis({
    url: process.env.UPSTASH_REDIS_REST_URL,
    token: process.env.UPSTASH_REDIS_REST_TOKEN
});

async function getCachedPosts() {
    const cacheKey = 'posts:recent';

    // Try cache first
    const cached = await redis.get(cacheKey);
    if (cached) {
        return JSON.parse(cached);
    }

    // Query database
    const posts = await db.query(`
        SELECT * FROM posts
        WHERE published = true
        ORDER BY created_at DESC
        LIMIT 20
    `);

    // Cache for 5 minutes
    await redis.setex(cacheKey, 300, JSON.stringify(posts));

    return posts;
}

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- PAGINATION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Offset pagination (simple but slow for large offsets)
SELECT * FROM posts
ORDER BY created_at DESC
LIMIT 20 OFFSET 100;

-- Cursor-based pagination (faster)
SELECT * FROM posts
WHERE created_at < '2024-01-01 12:00:00'
ORDER BY created_at DESC
LIMIT 20;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DENORMALIZATION (For Read-Heavy Workloads)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Instead of joining every time
SELECT posts.*, users.name
FROM posts
JOIN users ON posts.user_id = users.id;

-- Store author name in posts table
CREATE TABLE posts (
    id SERIAL PRIMARY KEY,
    title TEXT,
    user_id INT,
    author_name TEXT, -- Denormalized!
    created_at TIMESTAMP
);

-- Update when user changes name (trade-off)
UPDATE posts SET author_name = 'New Name' WHERE user_id = 123;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MONITORING SLOW QUERIES
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- PostgreSQL: Log slow queries
-- In postgresql.conf:
-- log_min_duration_statement = 1000  # Log queries > 1 second

-- Query to find slow queries
SELECT
    calls,
    total_time,
    mean_time,
    query
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;
```

---

<div align="center">

## ğŸ’° Cost Optimization

_Save money without sacrificing performance_ ğŸ’¸

</div>

### Cost-Saving Strategies

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RIGHT-SIZING YOUR DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Don't over-provision
# Start small, scale up when needed

# PostgreSQL/MySQL:
# - Small project: 512MB-1GB RAM
# - Medium project: 2-4GB RAM
# - Large project: 8GB+ RAM

# Monitor actual usage:
# - CPU utilization (should be <70% avg)
# - Memory usage (should have headroom)
# - Storage growth rate

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STORAGE OPTIMIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Archive old data
# Move historical data to cheaper storage (S3, etc.)

# PostgreSQL: Create archive table
CREATE TABLE posts_archive (LIKE posts INCLUDING ALL);

# Move old posts (older than 2 years)
INSERT INTO posts_archive
SELECT * FROM posts
WHERE created_at < NOW() - INTERVAL '2 years';

DELETE FROM posts
WHERE created_at < NOW() - INTERVAL '2 years';

# 2. Vacuum database regularly
# PostgreSQL
VACUUM ANALYZE posts;

# 3. Delete unused indexes
# Find unused indexes
SELECT schemaname, tablename, indexname
FROM pg_stat_user_indexes
WHERE idx_scan = 0
AND indexname NOT LIKE '%_pkey';

# Drop unused index
DROP INDEX IF EXISTS idx_unused;

# 4. Compress old data
# Use PostgreSQL TOAST compression
ALTER TABLE posts
ALTER COLUMN content SET STORAGE EXTENDED;

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION OPTIMIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Use connection pooling (PgBouncer for PostgreSQL)
# Reduces connection overhead

# For serverless: Use HTTP-based queries
# - Neon serverless driver
# - Upstash Redis REST API

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# READ REPLICAS (Only if needed!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Use read replicas for:
# - Separating read from write load
# - Running analytics queries
# - Geographic distribution

# Don't use read replicas if:
# - You have low traffic
# - Caching would be cheaper
# - Your app is simple

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHOOSE THE RIGHT DATABASE TYPE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Serverless databases (Neon, PlanetScale) for:
# - Variable traffic
# - Development environments
# - Pay for what you use

# Dedicated databases for:
# - Consistent high traffic
# - Predictable workloads
# - Better cost at scale

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING & ALERTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Set up billing alerts
# - 50% of budget
# - 80% of budget
# - 100% of budget

# Monitor key metrics:
# - Storage growth rate
# - Query volume
# - Connection count
# - Cache hit rate
```

---

<div align="center">

## ğŸ”§ Backup & Recovery

_Protect your data_ ğŸ’¾

</div>

### Backup Strategies

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# POSTGRESQL BACKUPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Full backup (pg_dump)
pg_dump -h hostname -U username -d database > backup.sql

# With compression
pg_dump -h hostname -U username -d database | gzip > backup.sql.gz

# Custom format (faster restore)
pg_dump -h hostname -U username -Fc database > backup.dump

# Restore from backup
psql -h hostname -U username -d database < backup.sql
# Or custom format
pg_restore -h hostname -U username -d database backup.dump

# Backup specific table
pg_dump -h hostname -U username -d database -t users > users_backup.sql

# Automated backup script
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups"
DATABASE="myapp"

pg_dump -h $DB_HOST -U $DB_USER -d $DATABASE | \
gzip > $BACKUP_DIR/${DATABASE}_${DATE}.sql.gz

# Keep only last 7 days
find $BACKUP_DIR -name "*.sql.gz" -mtime +7 -delete

# Upload to S3
aws s3 cp $BACKUP_DIR/${DATABASE}_${DATE}.sql.gz \
s3://my-backups/postgres/

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONGODB BACKUPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Backup entire database
mongodump --uri="mongodb://user:pass@host/database" --out=/backup

# Backup specific collection
mongodump --uri="mongodb://..." --collection=users --out=/backup

# Restore
mongorestore --uri="mongodb://..." /backup

# Automated backup
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
mongodump --uri="$MONGODB_URI" --out=/backup/mongo_$DATE
tar -czf /backup/mongo_$DATE.tar.gz /backup/mongo_$DATE
rm -rf /backup/mongo_$DATE

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MANAGED DATABASE BACKUPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Most managed services (Supabase, Railway, etc.) have:
# - Automatic daily backups
# - Point-in-time recovery
# - One-click restore

# Supabase: Project â†’ Database â†’ Backups
# Railway: Project â†’ Database â†’ Backups
# PlanetScale: Database â†’ Backups

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DISASTER RECOVERY PLAN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Backup frequency
#    - Critical data: Hourly
#    - Normal data: Daily
#    - Archives: Weekly

# 2. Backup retention
#    - Daily: Keep 7 days
#    - Weekly: Keep 4 weeks
#    - Monthly: Keep 12 months

# 3. Test restores monthly
#    Verify backups are restorable

# 4. Document recovery procedures
#    Step-by-step guide

# 5. Calculate Recovery Point Objective (RPO)
#    How much data loss is acceptable?
#    - RPO = 0: Continuous replication
#    - RPO = 1 hour: Hourly backups
#    - RPO = 24 hours: Daily backups

# 6. Calculate Recovery Time Objective (RTO)
#    How quickly must service be restored?
#    - RTO = 1 hour: Hot standby
#    - RTO = 4 hours: Warm standby
#    - RTO = 24 hours: Cold backup
```

---

<div align="center">

## ğŸ› Troubleshooting

_Common issues and solutions_ ğŸ”§

</div>

### Common Problems

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION ISSUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Problem: "connection refused"
# Solutions:
# 1. Check if database is running
# 2. Verify connection string
# 3. Check firewall/security groups
# 4. Ensure SSL settings are correct

# Problem: "too many connections"
# Solutions:
# 1. Implement connection pooling
# 2. Close connections properly
# 3. Increase max_connections (if needed)
# 4. Find and kill idle connections

# PostgreSQL: Find idle connections
SELECT pid, usename, state, query
FROM pg_stat_activity
WHERE state = 'idle'
AND state_change < NOW() - INTERVAL '30 minutes';

# Kill idle connections
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state = 'idle'
AND state_change < NOW() - INTERVAL '30 minutes';

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PERFORMANCE ISSUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Problem: Slow queries
# Solutions:
# 1. Add indexes
# 2. Optimize query
# 3. Increase resources
# 4. Implement caching

# Find slow queries (PostgreSQL)
SELECT
    calls,
    mean_time,
    max_time,
    query
FROM pg_stat_statements
WHERE mean_time > 1000  -- More than 1 second
ORDER BY mean_time DESC;

# Problem: High CPU usage
# Solutions:
# 1. Optimize expensive queries
# 2. Add indexes
# 3. Reduce query frequency
# 4. Scale up resources

# Problem: Running out of storage
# Solutions:
# 1. Archive old data
# 2. Delete unnecessary data
# 3. Vacuum database
# 4. Increase storage

# PostgreSQL: Check table sizes
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

# MongoDB: Check collection sizes
db.stats()
db.collection.stats()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA CORRUPTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Problem: Data inconsistency
# Solutions:
# 1. Restore from backup
# 2. Use transactions properly
# 3. Implement foreign key constraints
# 4. Add validation at application level

# PostgreSQL: Check for corruption
SELECT * FROM pg_stat_database_conflicts;

# Problem: Lost data after crash
# Solutions:
# 1. Enable WAL (Write-Ahead Logging)
# 2. Use synchronous commits
# 3. Verify backup integrity
# 4. Test disaster recovery

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MIGRATION ISSUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Problem: Migration fails
# Solutions:
# 1. Test migration on staging first
# 2. Use transactions (if supported)
# 3. Create rollback plan
# 4. Migrate in batches

# Problem: Downtime during migration
# Solutions:
# 1. Use blue-green deployment
# 2. Implement dual-write pattern
# 3. Use database replication
# 4. Schedule during low-traffic

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTHENTICATION ERRORS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Problem: "authentication failed"
# Solutions:
# 1. Verify credentials
# 2. Check user permissions
# 3. Reset password
# 4. Check IP whitelist

# PostgreSQL: Check user permissions
SELECT * FROM pg_roles WHERE rolname = 'username';

# Grant permissions
GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO username;

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEBUGGING TIPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Enable query logging
# PostgreSQL: log_statement = 'all'
# MongoDB: db.setProfilingLevel(2)

# 2. Use EXPLAIN to analyze queries
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';

# 3. Monitor connection pool
console.log('Pool stats:', pool.totalCount, pool.idleCount);

# 4. Check database logs
# Supabase: Project â†’ Logs
# Railway: Service â†’ Logs
# AWS RDS: CloudWatch Logs

# 5. Test connection locally
psql -h hostname -U username -d database
mongo "mongodb://connection-string"

# 6. Use database health check queries
-- PostgreSQL
SELECT 1;
SELECT version();

-- MongoDB
db.adminCommand({ ping: 1 })

# 7. Monitor resource usage
-- PostgreSQL: Current connections
SELECT count(*) FROM pg_stat_activity;

-- PostgreSQL: Database size
SELECT pg_database_size('database_name');

-- PostgreSQL: Cache hit ratio (should be >99%)
SELECT
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit) as heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
FROM pg_statio_user_tables;
```

---

<div align="center">

## ğŸ’¡ Best Practices

_Production-ready database management_ â­

</div>

### Development Best Practices

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CONNECTION MANAGEMENT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Use connection pooling
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// âœ… DO: Close connections properly
async function queryDatabase() {
  const client = await pool.connect();
  try {
    const result = await client.query('SELECT * FROM users');
    return result.rows;
  } finally {
    client.release(); // Always release!
  }
}

// âŒ DON'T: Create new connection for each query
// This exhausts connection limits
async function badQuery() {
  const client = new Client({ connectionString: process.env.DATABASE_URL });
  await client.connect();
  const result = await client.query('SELECT * FROM users');
  await client.end();
  return result;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// QUERY PATTERNS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Use parameterized queries (prevents SQL injection)
const { rows } = await pool.query(
  'SELECT * FROM users WHERE email = $1',
  [userEmail]
);

// âŒ DON'T: Concatenate user input into queries
const query = `SELECT * FROM users WHERE email = '${userEmail}'`; // DANGEROUS!

// âœ… DO: Use transactions for multi-step operations
const client = await pool.connect();
try {
  await client.query('BEGIN');
  await client.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromId]);
  await client.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId]);
  await client.query('COMMIT');
} catch (e) {
  await client.query('ROLLBACK');
  throw e;
} finally {
  client.release();
}

// âœ… DO: Fetch only needed columns
const { rows } = await pool.query('SELECT id, name, email FROM users');

// âŒ DON'T: Fetch everything if you don't need it
const { rows } = await pool.query('SELECT * FROM users');

// âœ… DO: Use indexes for frequently queried columns
// Add index in migration:
CREATE INDEX idx_users_email ON users(email);

// âœ… DO: Implement pagination
async function getPaginatedUsers(page = 1, limit = 20) {
  const offset = (page - 1) * limit;
  const { rows } = await pool.query(
    'SELECT * FROM users ORDER BY created_at DESC LIMIT $1 OFFSET $2',
    [limit, offset]
  );
  return rows;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ERROR HANDLING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Handle database errors gracefully
async function safeQuery() {
  try {
    const { rows } = await pool.query('SELECT * FROM users');
    return rows;
  } catch (error) {
    console.error('Database error:', error.message);

    // Handle specific errors
    if (error.code === '23505') {
      throw new Error('Duplicate entry');
    }

    if (error.code === '23503') {
      throw new Error('Foreign key constraint violation');
    }

    // Generic error
    throw new Error('Database operation failed');
  }
}

// âœ… DO: Implement retry logic for transient errors
async function queryWithRetry(query, params, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await pool.query(query, params);
    } catch (error) {
      // Retry on connection errors
      if (error.code === 'ECONNREFUSED' || error.code === 'ETIMEDOUT') {
        if (i === maxRetries - 1) throw error;
        await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
        continue;
      }
      throw error;
    }
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SCHEMA DESIGN
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Normalize data appropriately
-- Good: Normalized structure
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  name VARCHAR(255)
);

CREATE TABLE posts (
  id SERIAL PRIMARY KEY,
  user_id INT REFERENCES users(id),
  title TEXT NOT NULL,
  content TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);

// âœ… DO: Add constraints
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  age INT CHECK (age >= 0 AND age <= 120),
  created_at TIMESTAMP DEFAULT NOW() NOT NULL
);

// âœ… DO: Use appropriate data types
-- String: VARCHAR(n), TEXT
-- Numbers: INTEGER, BIGINT, DECIMAL
-- Date/Time: TIMESTAMP, DATE, TIME
-- JSON: JSONB (PostgreSQL)
-- Boolean: BOOLEAN
-- UUID: UUID

// âœ… DO: Add indexes for foreign keys
CREATE INDEX idx_posts_user_id ON posts(user_id);

// âœ… DO: Use UNIQUE constraints
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  email VARCHAR(255) UNIQUE NOT NULL,
  username VARCHAR(50) UNIQUE NOT NULL
);

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ENVIRONMENT-SPECIFIC CONFIGS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Use different databases per environment
// .env.development
DATABASE_URL=postgresql://localhost/myapp_dev

// .env.test
DATABASE_URL=postgresql://localhost/myapp_test

// .env.production
DATABASE_URL=postgresql://prod-host/myapp_prod

// âœ… DO: Use migrations for schema changes
// Never modify production database manually

// âœ… DO: Test migrations on staging first
// Before applying to production

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SECURITY PRACTICES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Store connection strings in environment variables
const connectionString = process.env.DATABASE_URL;

// âŒ DON'T: Commit credentials to Git
// Add .env to .gitignore

// âœ… DO: Use SSL/TLS in production
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: process.env.NODE_ENV === 'production'
    ? { rejectUnauthorized: false }
    : false
});

// âœ… DO: Rotate credentials regularly
// Change passwords every 90 days

// âœ… DO: Use read-only connections for read-only operations
const readOnlyPool = new Pool({
  connectionString: process.env.READ_REPLICA_URL
});

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MONITORING & LOGGING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… DO: Log slow queries
const start = Date.now();
const result = await pool.query('SELECT * FROM users');
const duration = Date.now() - start;

if (duration > 1000) {
  console.warn(`Slow query (${duration}ms):`, 'SELECT * FROM users');
}

// âœ… DO: Monitor connection pool
setInterval(() => {
  console.log('Pool stats:', {
    total: pool.totalCount,
    idle: pool.idleCount,
    waiting: pool.waitingCount
  });
}, 60000);

// âœ… DO: Set up alerts for errors
// Use Sentry, Datadog, or similar

// âœ… DO: Track query performance
import { Counter, Histogram } from 'prom-client';

const queryCounter = new Counter({
  name: 'db_queries_total',
  help: 'Total number of database queries'
});

const queryDuration = new Histogram({
  name: 'db_query_duration_seconds',
  help: 'Database query duration'
});

async function monitoredQuery(query, params) {
  queryCounter.inc();
  const start = Date.now();

  try {
    return await pool.query(query, params);
  } finally {
    queryDuration.observe((Date.now() - start) / 1000);
  }
}
```

### Deployment Checklist

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PRE-PRODUCTION CHECKLIST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Database Setup
   â˜ Created production database
   â˜ Set up SSL/TLS
   â˜ Configured firewall rules
   â˜ Whitelisted application IPs
   â˜ Created database users with minimal permissions

âœ… Schema & Migrations
   â˜ All migrations tested in staging
   â˜ Rollback scripts prepared
   â˜ Indexes created for common queries
   â˜ Foreign key constraints in place
   â˜ Check constraints validated

âœ… Security
   â˜ Connection strings in environment variables
   â˜ No credentials in code
   â˜ SSL/TLS enabled
   â˜ Strong passwords set
   â˜ Row-level security configured (if needed)
   â˜ Backup encryption enabled

âœ… Performance
   â˜ Connection pooling configured
   â˜ Indexes created
   â˜ Query optimization done
   â˜ Caching strategy implemented
   â˜ Load testing completed

âœ… Backup & Recovery
   â˜ Automatic backups enabled
   â˜ Backup retention policy set
   â˜ Restore procedure tested
   â˜ Point-in-time recovery available
   â˜ Backups stored in different region

âœ… Monitoring
   â˜ Query logging enabled
   â˜ Slow query alerts set up
   â˜ Connection monitoring configured
   â˜ Storage alerts configured
   â˜ Error tracking enabled (Sentry, etc.)

âœ… Documentation
   â˜ Connection instructions documented
   â˜ Schema documented
   â˜ Migration process documented
   â˜ Backup/restore procedures documented
   â˜ Troubleshooting guide created

âœ… Testing
   â˜ Unit tests passing
   â˜ Integration tests passing
   â˜ Load tests completed
   â˜ Disaster recovery tested
   â˜ Failover tested (if applicable)
```

---

<div align="center">

## ğŸ¯ Quick Reference

_Essential commands at your fingertips_ âš¡

</div>

### PostgreSQL Quick Reference

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CONNECTION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Connect to database
psql -h hostname -U username -d database

-- Connection string format
postgresql://username:password@hostname:5432/database?sslmode=require

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DATABASE OPERATIONS
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- List databases
\l

-- Connect to database
\c database_name

-- Show current database
SELECT current_database();

-- Database size
SELECT pg_size_pretty(pg_database_size('database_name'));

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- TABLE OPERATIONS
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- List tables
\dt

-- Describe table
\d table_name

-- Table size
SELECT pg_size_pretty(pg_total_relation_size('table_name'));

-- Row count
SELECT COUNT(*) FROM table_name;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- INDEX OPERATIONS
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- List indexes
\di

-- Create index
CREATE INDEX idx_name ON table_name(column_name);

-- Drop index
DROP INDEX idx_name;

-- Unused indexes
SELECT schemaname, tablename, indexname
FROM pg_stat_user_indexes
WHERE idx_scan = 0;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- USER & PERMISSIONS
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- List users
\du

-- Create user
CREATE USER username WITH PASSWORD 'password';

-- Grant permissions
GRANT SELECT, INSERT, UPDATE ON table_name TO username;

-- Revoke permissions
REVOKE ALL ON table_name FROM username;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- QUERY OPTIMIZATION
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Analyze query
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';

-- Vacuum table
VACUUM ANALYZE table_name;

-- Reindex
REINDEX TABLE table_name;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MONITORING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Active connections
SELECT * FROM pg_stat_activity;

-- Slow queries
SELECT query, mean_time
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;

-- Database statistics
SELECT * FROM pg_stat_database;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- BACKUP & RESTORE
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Backup
pg_dump -h hostname -U username database > backup.sql

-- Restore
psql -h hostname -U username database < backup.sql

-- Backup with compression
pg_dump database | gzip > backup.sql.gz

-- Restore compressed
gunzip -c backup.sql.gz | psql database
```

### MongoDB Quick Reference

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CONNECTION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Connect
mongosh "mongodb://username:password@hostname/database"

// Connection string format
mongodb+srv://username:password@cluster.mongodb.net/database

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// DATABASE OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Show databases
show dbs

// Use database
use database_name

// Current database
db.getName()

// Database stats
db.stats()

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COLLECTION OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Show collections
show collections

// Collection stats
db.collection_name.stats()

// Count documents
db.collection_name.countDocuments()

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// CRUD OPERATIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Insert
db.users.insertOne({ name: "MrDib", email: "email@example.com" })
db.users.insertMany([{ name: "User1" }, { name: "User2" }])

// Find
db.users.find()
db.users.findOne({ email: "email@example.com" })
db.users.find({ age: { $gte: 18 } })

// Update
db.users.updateOne({ _id: ObjectId("...") }, { $set: { name: "New Name" } })
db.users.updateMany({ active: false }, { $set: { status: "inactive" } })

// Delete
db.users.deleteOne({ _id: ObjectId("...") })
db.users.deleteMany({ createdAt: { $lt: new Date("2020-01-01") } })

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// INDEXES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// List indexes
db.collection_name.getIndexes()

// Create index
db.collection_name.createIndex({ field: 1 })  // 1 = ascending, -1 = descending

// Create compound index
db.collection_name.createIndex({ field1: 1, field2: -1 })

// Create unique index
db.collection_name.createIndex({ email: 1 }, { unique: true })

// Drop index
db.collection_name.dropIndex("index_name")

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AGGREGATION
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Basic aggregation
db.users.aggregate([
  { $match: { age: { $gte: 18 } } },
  { $group: { _id: "$country", count: { $sum: 1 } } },
  { $sort: { count: -1 } }
])

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// BACKUP & RESTORE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Backup
mongodump --uri="mongodb://..." --out=/backup

// Restore
mongorestore --uri="mongodb://..." /backup

// Backup specific collection
mongodump --uri="mongodb://..." --collection=users --out=/backup
```

### Redis Quick Reference

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONNECTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Connect to Redis
redis-cli -h hostname -p 6379 -a password

# Test connection
PING

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STRINGS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Set value
SET key value

# Get value
GET key

# Set with expiration (seconds)
SETEX key 3600 value

# Increment
INCR counter
INCRBY counter 5

# Delete
DEL key

# Check if exists
EXISTS key

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LISTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Push to list
LPUSH list value
RPUSH list value

# Pop from list
LPOP list
RPOP list

# Get range
LRANGE list 0 -1

# List length
LLEN list

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Add to set
SADD set member

# Get all members
SMEMBERS set

# Check membership
SISMEMBER set member

# Set size
SCARD set

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HASHES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Set hash field
HSET hash field value

# Get hash field
HGET hash field

# Get all hash
HGETALL hash

# Delete hash field
HDEL hash field

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SORTED SETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Add to sorted set
ZADD leaderboard 100 player1

# Get range
ZRANGE leaderboard 0 -1
ZREVRANGE leaderboard 0 9  # Top 10

# Get score
ZSCORE leaderboard player1

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KEYS & EXPIRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# List all keys (careful in production!)
KEYS *

# Set expiration
EXPIRE key 3600

# Check TTL
TTL key

# Remove expiration
PERSIST key

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFO & MONITORING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Server info
INFO

# Memory info
INFO memory

# Monitor commands in real-time
MONITOR

# Slow log
SLOWLOG GET 10
```

---

<div align="center">

## ğŸ“ Learning Resources

_Master database management_ ğŸ“š

</div>

### Official Documentation

```
ğŸ˜ PostgreSQL
   â†’ https://postgresql.org/docs

ğŸ¬ MySQL
   â†’ https://dev.mysql.com/doc

ğŸƒ MongoDB
   â†’ https://docs.mongodb.com

âš¡ Redis
   â†’ https://redis.io/docs

ğŸ”¥ Firebase
   â†’ https://firebase.google.com/docs

âš¡ Supabase
   â†’ https://supabase.com/docs

ğŸŒ PlanetScale
   â†’ https://planetscale.com/docs

ğŸ’¡ Neon
   â†’ https://neon.tech/docs
```

### Books & Courses

```
ğŸ“š Books:
   â€¢ "PostgreSQL: Up and Running" by Regina Obe
   â€¢ "Designing Data-Intensive Applications" by Martin Kleppmann
   â€¢ "MongoDB: The Definitive Guide" by Shannon Bradshaw
   â€¢ "High Performance MySQL" by Baron Schwartz

ğŸ“ Courses:
   â€¢ Prisma's Data Guide
   â€¢ MongoDB University (Free)
   â€¢ PostgreSQL Tutorial (postgresqltutorial.com)
   â€¢ Use The Index, Luke (indexing guide)
```

### Community & Support

```
ğŸ’¬ Forums & Communities:
   â€¢ Stack Overflow
   â€¢ Reddit /r/PostgreSQL, /r/mongodb
   â€¢ Discord servers (Supabase, PlanetScale)
   â€¢ Database Administrators Stack Exchange

ğŸ¥ YouTube Channels:
   â€¢ Hussein Nasser
   â€¢ Fireship
   â€¢ Traversy Media
```

---

<div align="center">

## ğŸ‰ Conclusion

</div>

**Congratulations!** You now have a comprehensive guide to database hosting! ğŸš€

### Key Takeaways:

âœ… **Choose Wisely**: Select database based on your use case
âœ… **Start Simple**: Use managed services, scale later
âœ… **Security First**: Encrypt, backup, monitor
âœ… **Optimize Early**: Indexes, caching, connection pooling
âœ… **Monitor Always**: Know what's happening in production
âœ… **Backup Everything**: Test restores regularly
âœ… **Document Well**: Future you will be grateful

### Recommended Stack by Project Type:

```
ğŸ¯ Simple Web App:
   â†’ Supabase (PostgreSQL + Auth + Storage)
   â†’ Upstash Redis (Caching)

ğŸš€ SaaS Application:
   â†’ Neon or PlanetScale (Primary DB)
   â†’ Upstash Redis (Sessions/Cache)
   â†’ S3 (File storage)

ğŸ“± Mobile App:
   â†’ Firebase/Firestore (Real-time + Auth)
   â†’ Cloud Storage (Files)

ğŸ¢ Enterprise:
   â†’ AWS RDS/Aurora (PostgreSQL/MySQL)
   â†’ ElastiCache (Redis)
   â†’ S3 (Storage)

ğŸ’° Budget-Conscious:
   â†’ Railway (All-in-one)
   â†’ Or Supabase Free Tier
```

### Final Checklist:

```bash
âœ… Database created and accessible
âœ… Connection string secure (environment variable)
âœ… SSL/TLS enabled
âœ… Indexes created for common queries
âœ… Backup strategy in place
âœ… Monitoring configured
âœ… Security rules/permissions set
âœ… Performance tested
âœ… Documentation written
âœ… Team trained
```

---

<div align="center">

**Built with ğŸ’¾ by MrDib for Database Engineers**

_May your queries be fast, your data be safe, and your backups always restore!_ âš¡

**Now go build something with reliable data storage!** ğŸ‰

---

**Found this helpful? Star the repo â­ and share with fellow developers!**

_Happy Database Hosting!_ ğŸš€

</div>
