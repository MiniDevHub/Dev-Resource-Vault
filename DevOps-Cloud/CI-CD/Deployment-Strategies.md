<div align="center">

# ğŸš€ Deployment Strategies

### _Ship code like a pro - safely, quickly, and without breaking production_ ğŸ’ª

![Deployment](https://img.shields.io/badge/Deploy-Zero_Downtime-green?style=for-the-badge)
![Strategies](https://img.shields.io/badge/Strategies-7+-blue?style=for-the-badge)
![Reliability](https://img.shields.io/badge/Reliability-99.99%25-gold?style=for-the-badge)

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Understanding Deployment Strategies](#-understanding-deployment-strategies)
- [ğŸ”„ Recreate Deployment](#-recreate-deployment)
- [ğŸŒŠ Rolling Deployment](#-rolling-deployment)
- [ğŸ’™ğŸ’š Blue-Green Deployment](#-blue-green-deployment)
- [ğŸ¤ Canary Deployment](#-canary-deployment)
- [ğŸ§ª A/B Testing](#-ab-testing)
- [ğŸ­ Shadow Deployment](#-shadow-deployment)
- [ğŸ”€ Feature Flags](#-feature-flags)
- [ğŸ—„ï¸ Database Migrations](#ï¸-database-migrations)
- [ğŸ”™ Rollback Strategies](#-rollback-strategies)
- [ğŸ“Š Monitoring & Observability](#-monitoring--observability)
- [ğŸ› ï¸ Tools & Platforms](#ï¸-tools--platforms)
- [ğŸ’¡ Best Practices](#-best-practices)

---

<div align="center">

## ğŸ¯ Understanding Deployment Strategies

_Choose the right strategy for your use case_ ğŸ¯

</div>

### Strategy Comparison Matrix

<div align="center">

| Strategy        | Downtime | Complexity | Rollback Speed | Resource Usage | Best For      |
| :-------------- | :------: | :--------: | :------------: | :------------: | :------------ |
| **Recreate**    |  âŒ Yes  |     â­     |     âš¡âš¡âš¡     |       ğŸ’°       | Dev/Test      |
| **Rolling**     |  âœ… No   |    â­â­    |      âš¡âš¡      |      ğŸ’°ğŸ’°      | Most apps     |
| **Blue-Green**  |  âœ… No   |   â­â­â­   |   âš¡âš¡âš¡âš¡âš¡   |    ğŸ’°ğŸ’°ğŸ’°ğŸ’°    | Critical apps |
| **Canary**      |  âœ… No   |  â­â­â­â­  |    âš¡âš¡âš¡âš¡    |      ğŸ’°ğŸ’°      | Large scale   |
| **A/B Testing** |  âœ… No   | â­â­â­â­â­ |     âš¡âš¡âš¡     |      ğŸ’°ğŸ’°      | Features      |
| **Shadow**      |  âœ… No   | â­â­â­â­â­ |      N/A       |     ğŸ’°ğŸ’°ğŸ’°     | Testing       |

</div>

---

### Decision Tree

```
ğŸ¯ Which Deployment Strategy Should You Use?

Can you afford downtime?
â”œâ”€ No
â”‚  â”œâ”€ Need instant rollback?
â”‚  â”‚  â”œâ”€ Yes + Have resources?
â”‚  â”‚  â”‚  â””â”€ âœ… Blue-Green (instant rollback, 2x resources)
â”‚  â”‚  â””â”€ No
â”‚  â”‚     â””â”€ âœ… Rolling (gradual, efficient)
â”‚  â”œâ”€ Testing new features?
â”‚  â”‚  â””â”€ âœ… Canary (gradual % of traffic)
â”‚  â”œâ”€ Need user feedback?
â”‚  â”‚  â””â”€ âœ… A/B Testing (specific user segments)
â”‚  â””â”€ Testing production load?
â”‚     â””â”€ âœ… Shadow (mirror traffic)
â””â”€ Yes (Dev/Test only!)
   â””â”€ âœ… Recreate (simplest, but downtime)

Still unsure?
â””â”€ Start with Rolling
   Most balanced approach for production
```

---

<div align="center">

## ğŸ”„ Recreate Deployment

_The simplest strategy (with downtime)_ â¸ï¸

</div>

### How It Works

```
Timeline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Running    â”‚ Downtime  â”‚   Running    â”‚
â”‚   v1.0 â–ˆâ–ˆâ–ˆâ–ˆ  â”‚           â”‚  v2.0 â–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚              â”‚  [STOP]   â”‚              â”‚
â”‚              â”‚  [START]  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     t=0          t=1-2min       t=3min

Process:
1. Stop all v1.0 instances
2. Deploy v2.0 code
3. Start all v2.0 instances
4. âš ï¸ Users experience downtime during steps 1-3
```

---

### Kubernetes Implementation

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RECREATE DEPLOYMENT - KUBERNETES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
spec:
  replicas: 3
  strategy:
    type: Recreate # All old pods terminated before new ones start
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
        version: v2.0.0
    spec:
      containers:
        - name: app
          image: myregistry/my-app:v2.0.0
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
```

---

### Docker Compose Implementation

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RECREATE - DOCKER COMPOSE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

version: "3.8"

services:
  app:
    image: myapp:v2.0.0
    deploy:
      replicas: 3
      update_config:
        parallelism: 0 # Stop all instances
        delay: 0s
      restart_policy:
        condition: on-failure
    ports:
      - "8080:8080"
    environment:
      - NODE_ENV=production
```

**Deployment Command:**

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RECREATE DEPLOYMENT SCRIPT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#!/bin/bash
set -e

echo "ğŸ›‘ Stopping current version..."
kubectl delete deployment my-app --grace-period=30

echo "â³ Waiting for graceful shutdown..."
sleep 30

echo "ğŸš€ Deploying new version..."
kubectl apply -f deployment-v2.yaml

echo "â³ Waiting for deployment to be ready..."
kubectl wait --for=condition=available --timeout=300s deployment/my-app

echo "âœ… Deployment complete!"

# Verify
kubectl get pods -l app=my-app
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Simple to implement
â€¢ Clean state (no version mixing)
â€¢ Predictable behavior
â€¢ Easy to understand
â€¢ No extra infrastructure needed

âŒ CONS:
â€¢ Downtime (usually 1-5 minutes)
â€¢ Bad user experience
â€¢ Risk if new version has issues
â€¢ Not suitable for production
â€¢ Can't test in production

ğŸ¯ USE WHEN:
âœ“ Development environments
âœ“ Testing environments
âœ“ Internal tools with maintenance windows
âœ“ Batch jobs
âœ“ Simplicity > availability

âŒ DON'T USE FOR:
âœ— Customer-facing production apps
âœ— 24/7 services
âœ— High-availability requirements
âœ— SLA commitments
```

> **âš ï¸ Reality Check:** Recreate is fine for dev/test, but if you're using it in production, you're basically telling users "we don't care about your time." Don't be that company!

---

<div align="center">

## ğŸŒŠ Rolling Deployment

_Gradual replacement with zero downtime_ ğŸ”„

</div>

### How It Works

```
Timeline (4 replicas):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Start   â”‚  Step 1  â”‚  Step 2  â”‚  Step 3  â”‚  Step 4  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ v1 â–ˆâ–ˆâ–ˆâ–ˆ  â”‚ v1 â–ˆâ–ˆâ–ˆ   â”‚ v1 â–ˆâ–ˆ    â”‚ v1 â–ˆ     â”‚ v2 â–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â”‚          â”‚ v2 â–ˆ     â”‚ v2 â–ˆâ–ˆ    â”‚ v2 â–ˆâ–ˆâ–ˆ   â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Process:
1. Deploy 1 instance of v2.0
2. Wait for health check âœ“
3. Remove 1 instance of v1.0
4. Repeat steps 1-3 until complete
5. âœ… No downtime - always have instances serving traffic
```

---

### Kubernetes Implementation

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ROLLING DEPLOYMENT - KUBERNETES
# Production-ready configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: production
  labels:
    app: my-app
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # Max 1 pod above desired count (25%)
      maxUnavailable: 1 # Max 1 pod can be unavailable (25%)

  # How long to wait before considering deployment failed
  progressDeadlineSeconds: 600

  # Min seconds pod should be ready without crashes
  minReadySeconds: 30

  selector:
    matchLabels:
      app: my-app

  template:
    metadata:
      labels:
        app: my-app
        version: v2.0.0
    spec:
      # Graceful shutdown
      terminationGracePeriodSeconds: 60

      containers:
        - name: app
          image: myregistry/my-app:v2.0.0
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          # Resource limits
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"

          # Liveness probe (restart if fails)
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3

          # Readiness probe (remove from service if fails)
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3

          # Startup probe (for slow-starting apps)
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 30 # 5 minutes to start

          # Environment variables
          env:
            - name: NODE_ENV
              value: "production"
            - name: LOG_LEVEL
              value: "info"

          # Graceful shutdown handler
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "sleep 15"] # Wait for load balancer to deregister
```

---

### Advanced Rolling Deployment Script

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADVANCED ROLLING DEPLOYMENT SCRIPT
# With health checks, monitoring, and automatic rollback
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -euo pipefail

# Configuration
APP_NAME="my-app"
NAMESPACE="production"
NEW_VERSION="v2.0.0"
MAX_WAIT_TIME=600  # 10 minutes
HEALTH_CHECK_URL="https://api.example.com/health"
ERROR_THRESHOLD=0.05  # 5% error rate threshold

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check deployment health
check_deployment_health() {
    local deployment=$1
    local namespace=$2

    # Check if deployment is available
    if ! kubectl rollout status deployment/$deployment -n $namespace --timeout=${MAX_WAIT_TIME}s; then
        log_error "Deployment rollout failed!"
        return 1
    fi

    # Check pod health
    local ready_pods=$(kubectl get deployment $deployment -n $namespace -o jsonpath='{.status.readyReplicas}')
    local desired_pods=$(kubectl get deployment $deployment -n $namespace -o jsonpath='{.spec.replicas}')

    if [ "$ready_pods" != "$desired_pods" ]; then
        log_error "Not all pods are ready ($ready_pods/$desired_pods)"
        return 1
    fi

    log_info "All pods are healthy ($ready_pods/$desired_pods)"
    return 0
}

# Function to check application metrics
check_application_metrics() {
    log_info "Checking application metrics..."

    # Get error rate from Prometheus/monitoring system
    # This is a simplified example
    local error_rate=$(curl -s "$HEALTH_CHECK_URL/metrics" | grep error_rate | awk '{print $2}')

    if (( $(echo "$error_rate > $ERROR_THRESHOLD" | bc -l) )); then
        log_error "Error rate too high: $error_rate (threshold: $ERROR_THRESHOLD)"
        return 1
    fi

    log_info "Error rate is acceptable: $error_rate"
    return 0
}

# Function to perform rollback
perform_rollback() {
    log_error "ğŸš¨ ROLLING BACK DEPLOYMENT! ğŸš¨"

    kubectl rollout undo deployment/$APP_NAME -n $NAMESPACE

    log_info "Waiting for rollback to complete..."
    kubectl rollout status deployment/$APP_NAME -n $NAMESPACE --timeout=300s

    log_info "âœ… Rollback completed successfully"
}

# Main deployment function
main() {
    log_info "ğŸš€ Starting rolling deployment of $APP_NAME:$NEW_VERSION"

    # 1. Update deployment
    log_info "Updating deployment manifest..."
    kubectl set image deployment/$APP_NAME $APP_NAME=myregistry/$APP_NAME:$NEW_VERSION -n $NAMESPACE

    # 2. Watch rollout
    log_info "Watching rollout progress..."
    if ! check_deployment_health "$APP_NAME" "$NAMESPACE"; then
        perform_rollback
        exit 1
    fi

    # 3. Wait for stabilization
    log_info "Waiting 60s for deployment to stabilize..."
    sleep 60

    # 4. Check application health
    if ! check_application_metrics; then
        perform_rollback
        exit 1
    fi

    # 5. Run smoke tests
    log_info "Running smoke tests..."
    if ! ./scripts/smoke-tests.sh; then
        log_error "Smoke tests failed!"
        perform_rollback
        exit 1
    fi

    # 6. Success!
    log_info "âœ… Deployment completed successfully!"

    # 7. Notify team
    curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
      -H 'Content-Type: application/json' \
      -d "{\"text\":\"âœ… $APP_NAME:$NEW_VERSION deployed successfully!\"}"

    # 8. Document deployment
    kubectl annotate deployment/$APP_NAME -n $NAMESPACE \
      "deployment.kubernetes.io/deployed-by=$(whoami)" \
      "deployment.kubernetes.io/deployed-at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
      "deployment.kubernetes.io/git-commit=$(git rev-parse HEAD)"
}

# Cleanup on exit
trap 'log_warn "Deployment interrupted!"' INT TERM

# Run main function
main "$@"
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Zero downtime
â€¢ Gradual rollout reduces risk
â€¢ Easy to understand
â€¢ Built-in to most platforms
â€¢ Automatic rollback on failure
â€¢ Cost-effective (no 2x resources)

âŒ CONS:
â€¢ Both versions run simultaneously
â€¢ Rollback takes time (not instant)
â€¢ Database compatibility required
â€¢ Can't easily test full new version first
â€¢ Complex if app doesn't support mixed versions

ğŸ¯ USE WHEN:
âœ“ Standard production deployments
âœ“ Stateless applications
âœ“ Backward-compatible changes
âœ“ Limited resources (can't afford 2x)
âœ“ Need zero downtime

âš ï¸ CHALLENGES:
â€¢ Database schema changes
â€¢ Breaking API changes
â€¢ Incompatible versions
â€¢ Shared state
```

---

### Health Check Examples

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HEALTH CHECK ENDPOINTS - EXPRESS.JS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const express = require("express");
const app = express();

// Liveness probe - is the app running?
app.get("/health", (req, res) => {
  // Basic check - if this endpoint responds, app is alive
  res.status(200).json({ status: "ok", timestamp: Date.now() });
});

// Readiness probe - is the app ready to serve traffic?
app.get("/ready", async (req, res) => {
  try {
    // Check database connection
    await database.ping();

    // Check required services
    await cache.ping();
    await messageQueue.ping();

    // Check if migrations are complete
    const migrationsComplete = await checkMigrations();

    if (!migrationsComplete) {
      return res.status(503).json({
        status: "not_ready",
        reason: "migrations_pending",
      });
    }

    // All checks passed
    res.status(200).json({
      status: "ready",
      checks: {
        database: "ok",
        cache: "ok",
        messageQueue: "ok",
        migrations: "ok",
      },
    });
  } catch (error) {
    // Not ready
    res.status(503).json({
      status: "not_ready",
      error: error.message,
    });
  }
});

// Startup probe - is the app starting up?
// Useful for apps that take a while to initialize
app.get("/startup", (req, res) => {
  if (app.locals.startupComplete) {
    res.status(200).json({ status: "started" });
  } else {
    res.status(503).json({ status: "starting" });
  }
});

// Mark startup as complete after initialization
app.listen(8080, async () => {
  await initializeApp();
  app.locals.startupComplete = true;
  console.log("App started and ready");
});
```

---

<div align="center">

## ğŸ’™ğŸ’š Blue-Green Deployment

_Instant rollback with full environment switching_ ğŸ›ï¸

</div>

### How It Works

```
Setup:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Blue (Current)    â”‚   Green (New)       â”‚
â”‚                     â”‚                     â”‚
â”‚   v1.0 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚   v2.0 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚
â”‚   (Production)      â”‚   (Staging/Ready)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²
           â”‚
    Traffic Routes Here

Switch:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Blue (Old)        â”‚   Green (Current)   â”‚
â”‚                     â”‚                     â”‚
â”‚   v1.0 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚   v2.0 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚
â”‚   (Idle)            â”‚   (Production)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                              â”‚
                       Traffic Routes Here

Process:
1. Deploy v2.0 to Green environment
2. Test Green thoroughly
3. Switch traffic from Blue to Green (instant!)
4. Monitor for issues
5. If issues: switch back to Blue (instant!)
6. If good: Blue becomes next staging environment
```

---

### Kubernetes Implementation

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLUE-GREEN DEPLOYMENT - KUBERNETES
# Complete setup with service switching
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Blue Deployment (Current Production)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-blue
  namespace: production
  labels:
    app: my-app
    environment: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: blue
  template:
    metadata:
      labels:
        app: my-app
        version: blue
    spec:
      containers:
        - name: app
          image: myregistry/my-app:v1.0.0
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
          env:
            - name: ENVIRONMENT
              value: "blue"

# Green Deployment (New Version)
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
  namespace: production
  labels:
    app: my-app
    environment: green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
        - name: app
          image: myregistry/my-app:v2.0.0
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
          env:
            - name: ENVIRONMENT
              value: "green"

# Production Service (Routes to Blue or Green)
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
  namespace: production
  labels:
    app: my-app
spec:
  type: LoadBalancer
  selector:
    app: my-app
    version: blue # Change to 'green' to switch traffic
  ports:
    - name: http
      port: 80
      targetPort: 8080
      protocol: TCP
  sessionAffinity: ClientIP # Optional: sticky sessions
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800 # 3 hours

# Internal Testing Services (Always available)
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-blue
  namespace: production
spec:
  selector:
    app: my-app
    version: blue
  ports:
    - port: 80
      targetPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: my-app-green
  namespace: production
spec:
  selector:
    app: my-app
    version: green
  ports:
    - port: 80
      targetPort: 8080
```

---

### Blue-Green Deployment Script

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLUE-GREEN DEPLOYMENT SCRIPT
# Production-grade with extensive checks
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -euo pipefail

# Configuration
APP_NAME="my-app"
NAMESPACE="production"
NEW_VERSION="v2.0.0"
SERVICE_NAME="my-app-service"
MONITORING_PERIOD=300  # 5 minutes
ERROR_THRESHOLD=0.05

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Get current active environment
get_active_environment() {
    kubectl get service $SERVICE_NAME -n $NAMESPACE \
      -o jsonpath='{.spec.selector.version}'
}

# Deploy to inactive environment
deploy_to_inactive() {
    local active_env=$(get_active_environment)
    local target_env="green"

    if [ "$active_env" = "green" ]; then
        target_env="blue"
    fi

    log_info "Active environment: $active_env"
    log_info "Deploying $NEW_VERSION to $target_env environment..."

    # Update deployment
    kubectl set image deployment/$APP_NAME-$target_env \
      app=myregistry/$APP_NAME:$NEW_VERSION \
      -n $NAMESPACE

    # Wait for rollout
    kubectl rollout status deployment/$APP_NAME-$target_env \
      -n $NAMESPACE --timeout=600s

    log_info "âœ… Deployment to $target_env complete"
    echo "$target_env"
}

# Run comprehensive tests
run_tests() {
    local env=$1
    local endpoint="http://my-app-$env.$NAMESPACE.svc.cluster.local"

    log_info "Running tests against $env environment..."

    # Health check
    log_info "1. Health check..."
    if ! curl -f "$endpoint/health" > /dev/null 2>&1; then
        log_error "Health check failed!"
        return 1
    fi

    # Smoke tests
    log_info "2. Smoke tests..."
    if ! ./scripts/smoke-tests.sh "$endpoint"; then
        log_error "Smoke tests failed!"
        return 1
    fi

    # Integration tests
    log_info "3. Integration tests..."
    if ! ./scripts/integration-tests.sh "$endpoint"; then
        log_error "Integration tests failed!"
        return 1
    fi

    # Performance tests
    log_info "4. Performance tests..."
    if ! ./scripts/performance-tests.sh "$endpoint"; then
        log_error "Performance tests failed!"
        return 1
    fi

    log_info "âœ… All tests passed!"
    return 0
}

# Switch traffic
switch_traffic() {
    local target_env=$1
    local active_env=$(get_active_environment)

    log_warn "ğŸ”„ Switching traffic from $active_env to $target_env..."

    # Update service selector
    kubectl patch service $SERVICE_NAME -n $NAMESPACE -p \
      "{\"spec\":{\"selector\":{\"version\":\"$target_env\"}}}"

    log_info "âœ… Traffic switched to $target_env"
    log_info "Waiting for load balancer to propagate (30s)..."
    sleep 30
}

# Monitor deployment
monitor_deployment() {
    local env=$1
    local start_time=$(date +%s)
    local end_time=$((start_time + MONITORING_PERIOD))

    log_info "ğŸ“Š Monitoring $env environment for $MONITORING_PERIOD seconds..."

    while [ $(date +%s) -lt $end_time ]; do
        # Check error rate
        local error_rate=$(kubectl logs -l version=$env -n $NAMESPACE --tail=100 | \
          grep -c "ERROR" || echo "0")

        # Check pod status
        local ready_pods=$(kubectl get deployment $APP_NAME-$env -n $NAMESPACE \
          -o jsonpath='{.status.readyReplicas}')
        local desired_pods=$(kubectl get deployment $APP_NAME-$env -n $NAMESPACE \
          -o jsonpath='{.spec.replicas}')

        if [ "$ready_pods" != "$desired_pods" ]; then
            log_error "Pod count mismatch: $ready_pods/$desired_pods"
            return 1
        fi

        log_info "Status: $ready_pods/$desired_pods pods ready, error_rate: $error_rate"

        sleep 30
    done

    log_info "âœ… Monitoring complete - no issues detected"
    return 0
}

# Rollback function
rollback() {
    local target_env=$1

    log_error "ğŸš¨ ROLLING BACK TO $target_env! ğŸš¨"

    switch_traffic "$target_env"

    log_info "âœ… Rollback complete"
}

# Cleanup old environment
cleanup_old_environment() {
    local old_env=$1

    log_info "ğŸ§¹ Cleaning up $old_env environment..."

    # Scale down to save resources (optional)
    kubectl scale deployment/$APP_NAME-$old_env --replicas=1 -n $NAMESPACE

    log_info "âœ… Cleanup complete"
}

# Main function
main() {
    log_info "ğŸš€ Starting Blue-Green deployment of $APP_NAME:$NEW_VERSION"

    # Get current state
    local active_env=$(get_active_environment)
    log_info "Current active environment: $active_env"

    # Step 1: Deploy to inactive environment
    local target_env=$(deploy_to_inactive)

    # Step 2: Run tests
    if ! run_tests "$target_env"; then
        log_error "Tests failed on $target_env - aborting deployment"
        exit 1
    fi

    # Step 3: Manual approval (optional)
    log_warn "âš ï¸  About to switch traffic from $active_env to $target_env"
    read -p "Continue? (yes/no): " confirm
    if [ "$confirm" != "yes" ]; then
        log_warn "Deployment cancelled by user"
        exit 0
    fi

    # Step 4: Switch traffic
    switch_traffic "$target_env"

    # Step 5: Monitor
    if ! monitor_deployment "$target_env"; then
        rollback "$active_env"
        exit 1
    fi

    # Step 6: Success!
    log_info "âœ… Deployment successful!"

    # Step 7: Cleanup
    cleanup_old_environment "$active_env"

    # Step 8: Notify
    curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK \
      -H 'Content-Type: application/json' \
      -d "{\"text\":\"âœ… $APP_NAME:$NEW_VERSION deployed via Blue-Green strategy!\"}"
}

# Trap signals
trap 'log_warn "Deployment interrupted!"' INT TERM

# Run
main "$@"
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Instant rollback (just switch service back)
â€¢ Full environment testing before switch
â€¢ Zero downtime
â€¢ Clean separation of versions
â€¢ Can compare versions side-by-side
â€¢ Reduced risk

âŒ CONS:
â€¢ 2x infrastructure cost (during deployment)
â€¢ Database migrations complexity
â€¢ Stateful applications challenging
â€¢ Need to handle data sync between environments
â€¢ Resource intensive

ğŸ¯ USE WHEN:
âœ“ Critical production applications
âœ“ Can afford 2x resources temporarily
âœ“ Need instant rollback capability
âœ“ Want to test full environment first
âœ“ High-risk deployments

ğŸ’° COST CONSIDERATION:
â€¢ Need 2x compute resources during deployment
â€¢ But only temporarily (minutes to hours)
â€¢ After switch, can scale down old environment

ğŸ¯ PERFECT FOR:
â€¢ Financial systems
â€¢ Healthcare applications
â€¢ E-commerce (during peak)
â€¢ Mission-critical apps
â€¢ Compliance requirements
```

---

<div align="center">

## ğŸ¤ Canary Deployment

_Test with real users gradually_ ğŸ“Š

</div>

### How It Works

```
Traffic Distribution Over Time:

Stage 1 (10% canary):
v1.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (90%)
v2.0: â–ˆâ–ˆ (10%)
      Monitor metrics...

Stage 2 (25% canary):
v1.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (75%)
v2.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (25%)
      Monitor metrics...

Stage 3 (50% canary):
v1.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (50%)
v2.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (50%)
      Monitor metrics...

Stage 4 (75% canary):
v1.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (25%)
v2.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (75%)
      Monitor metrics...

Stage 5 (100% canary):
v1.0: (0%)
v2.0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (100%)
      Success!

Each stage: Monitor for errors, latency, business metrics
If issues detected at any stage â†’ Rollback immediately
```

---

### Kubernetes + Flagger Implementation

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CANARY DEPLOYMENT - FLAGGER
# Automated canary with metrics-based promotion
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Install Flagger first:
# kubectl apply -k github.com/fluxcd/flagger//kustomize/linkerd

apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: my-app
  namespace: production
spec:
  # Target deployment
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app

  # HPA reference (optional)
  autoscalerRef:
    apiVersion: autoscaling/v2
    kind: HorizontalPodAutoscaler
    name: my-app

  # Service configuration
  service:
    port: 80
    targetPort: 8080
    gateways:
      - istio-system/public-gateway
    hosts:
      - app.example.com
    trafficPolicy:
      tls:
        mode: DISABLE

  # Canary analysis
  analysis:
    # Schedule interval (how often to check metrics)
    interval: 1m

    # Max number of failed checks before rollback
    threshold: 5

    # Max traffic percentage routed to canary
    maxWeight: 50

    # Traffic increment per iteration
    stepWeight: 10

    # Metrics to check
    metrics:
      # Request success rate (from Prometheus)
      - name: request-success-rate
        thresholdRange:
          min: 99
        interval: 1m

      # Request duration (from Prometheus)
      - name: request-duration
        thresholdRange:
          max: 500
        interval: 1m

      # Custom metric query
      - name: error-rate
        templateRef:
          name: error-rate
          namespace: flagger
        thresholdRange:
          max: 1
        interval: 1m

    # Webhooks for testing
    webhooks:
      # Pre-rollout webhook (smoke tests)
      - name: smoke-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 5m
        metadata:
          type: bash
          cmd: "curl -sd 'test' http://my-app-canary/token | grep token"

      # Load test during canary
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: "hey -z 1m -q 10 -c 2 http://my-app-canary/"

      # Post-rollout notification
      - name: send-notification
        type: post-rollout
        url: http://notification-service/
        metadata:
          message: "Canary deployment completed"

---
# Prometheus metric template (custom metrics)
apiVersion: flagger.app/v1beta1
kind: MetricTemplate
metadata:
  name: error-rate
  namespace: flagger
spec:
  provider:
    type: prometheus
    address: http://prometheus.monitoring:9090
  query: |
    100 - sum(
      rate(
        http_requests_total{
          kubernetes_namespace="{{ namespace }}",
          kubernetes_pod_name=~"{{ target }}-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)",
          status!~"5.."
        }[{{ interval }}]
      )
    )
    /
    sum(
      rate(
        http_requests_total{
          kubernetes_namespace="{{ namespace }}",
          kubernetes_pod_name=~"{{ target }}-[0-9a-zA-Z]+(-[0-9a-zA-Z]+)"
        }[{{ interval }}]
      )
    )
    * 100
```

---

### Manual Canary with Nginx

```nginx
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CANARY DEPLOYMENT - NGINX
# Manual traffic splitting
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

upstream backend {
    # Initial: 90% to v1, 10% to v2
    server app-v1:8080 weight=90 max_fails=3 fail_timeout=30s;
    server app-v2:8080 weight=10 max_fails=3 fail_timeout=30s;

    # Stage 2: 75% to v1, 25% to v2
    # server app-v1:8080 weight=75;
    # server app-v2:8080 weight=25;

    # Stage 3: 50/50
    # server app-v1:8080 weight=50;
    # server app-v2:8080 weight=50;

    # Final: 100% to v2
    # server app-v2:8080 weight=100;

    # Connection settings
    keepalive 32;
}

server {
    listen 80;
    server_name app.example.com;

    location / {
        proxy_pass http://backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;

        # Add version header for debugging
        add_header X-Backend-Version $upstream_addr;
    }
}
```

---

### Canary Deployment Script

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MANUAL CANARY DEPLOYMENT SCRIPT
# Gradual traffic increase with monitoring
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -euo pipefail

# Configuration
APP_NAME="my-app"
NEW_VERSION="v2.0.0"
CANARY_STAGES=(10 25 50 75 100)
MONITORING_DURATION=300  # 5 minutes per stage
ERROR_THRESHOLD=0.05
LATENCY_THRESHOLD_MS=500

log_info() { echo "[INFO] $1"; }
log_error() { echo "[ERROR] $1" >&2; }

# Deploy canary version
deploy_canary() {
    log_info "Deploying canary version $NEW_VERSION..."

    kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${APP_NAME}-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: $APP_NAME
      version: canary
  template:
    metadata:
      labels:
        app: $APP_NAME
        version: canary
    spec:
      containers:
      - name: app
        image: myregistry/$APP_NAME:$NEW_VERSION
        ports:
        - containerPort: 8080
EOF

    kubectl wait --for=condition=available deployment/${APP_NAME}-canary --timeout=300s
    log_info "âœ… Canary deployed"
}

# Update traffic split
update_traffic_split() {
    local canary_percent=$1
    local stable_percent=$((100 - canary_percent))

    log_info "Setting traffic: ${stable_percent}% stable, ${canary_percent}% canary"

    # This depends on your service mesh (Istio, Linkerd, etc.)
    # Example with Istio VirtualService:
    kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: $APP_NAME
spec:
  hosts:
  - $APP_NAME
  http:
  - route:
    - destination:
        host: $APP_NAME
        subset: stable
      weight: $stable_percent
    - destination:
        host: $APP_NAME
        subset: canary
      weight: $canary_percent
EOF
}

# Monitor metrics
monitor_canary() {
    local duration=$1
    local end_time=$(($(date +%s) + duration))

    log_info "Monitoring for ${duration}s..."

    while [ $(date +%s) -lt $end_time ]; do
        # Query Prometheus for error rate
        local error_rate=$(curl -s 'http://prometheus:9090/api/v1/query' \
          --data-urlencode 'query=rate(http_requests_total{version="canary",status=~"5.."}[5m])' | \
          jq -r '.data.result[0].value[1]')

        # Query Prometheus for latency
        local latency=$(curl -s 'http://prometheus:9090/api/v1/query' \
          --data-urlencode 'query=histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{version="canary"}[5m]))' | \
          jq -r '.data.result[0].value[1]')

        local latency_ms=$(echo "$latency * 1000" | bc)

        log_info "Canary metrics: error_rate=$error_rate, p99_latency=${latency_ms}ms"

        # Check thresholds
        if (( $(echo "$error_rate > $ERROR_THRESHOLD" | bc -l) )); then
            log_error "Error rate too high: $error_rate"
            return 1
        fi

        if (( $(echo "$latency_ms > $LATENCY_THRESHOLD_MS" | bc -l) )); then
            log_error "Latency too high: ${latency_ms}ms"
            return 1
        fi

        sleep 30
    done

    log_info "âœ… Monitoring passed"
    return 0
}

# Rollback
rollback() {
    log_error "ğŸš¨ ROLLING BACK CANARY! ğŸš¨"

    # Set traffic to 100% stable
    update_traffic_split 0

    # Delete canary deployment
    kubectl delete deployment ${APP_NAME}-canary

    log_info "âœ… Rollback complete"
}

# Promote canary
promote_canary() {
    log_info "ğŸ‰ Promoting canary to stable..."

    # Update stable deployment to new version
    kubectl set image deployment/$APP_NAME app=myregistry/$APP_NAME:$NEW_VERSION

    # Wait for rollout
    kubectl rollout status deployment/$APP_NAME

    # Delete canary deployment
    kubectl delete deployment ${APP_NAME}-canary

    # Reset traffic to 100% stable
    kubectl delete virtualservice $APP_NAME

    log_info "âœ… Promotion complete!"
}

# Main
main() {
    log_info "ğŸš€ Starting canary deployment: $APP_NAME:$NEW_VERSION"

    # Deploy canary
    deploy_canary

    # Gradual rollout
    for stage in "${CANARY_STAGES[@]}"; do
        log_info "ğŸ“Š Stage: ${stage}% canary traffic"

        # Update traffic
        update_traffic_split "$stage"

        # Monitor
        if ! monitor_canary "$MONITORING_DURATION"; then
            rollback
            exit 1
        fi

        # Pause between stages (optional)
        if [ "$stage" -lt 100 ]; then
            log_info "Pausing 60s before next stage..."
            sleep 60
        fi
    done

    # Promote
    promote_canary

    log_info "âœ… Canary deployment successful!"
}

main "$@"
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Very low risk (gradual rollout)
â€¢ Real production testing
â€¢ Can detect issues early with small %
â€¢ Automatic rollback possible
â€¢ Great for large-scale apps
â€¢ Users barely notice issues

âŒ CONS:
â€¢ Complex setup (need service mesh)
â€¢ Requires metrics/monitoring
â€¢ Slow rollout (hours)
â€¢ Need traffic shaping
â€¢ Stateful apps are tricky

ğŸ¯ USE WHEN:
âœ“ Large user base
âœ“ Can't afford any downtime
âœ“ Have good monitoring
âœ“ High-risk changes
âœ“ Want gradual validation

ğŸ“Š REQUIREMENTS:
â€¢ Service mesh (Istio, Linkerd) or load balancer
â€¢ Metrics system (Prometheus)
â€¢ Monitoring/alerting
â€¢ Automated testing

ğŸ¯ PERFECT FOR:
â€¢ Netflix-scale applications
â€¢ Financial services
â€¢ Healthcare systems
â€¢ Large SaaS platforms
```

---

<div align="center">

## ğŸ§ª A/B Testing

_Test features with specific user segments_ ğŸ²

</div>

### How It Works

```
User Traffic Split:

Control Group (50%):
â”œâ”€â”€ See Version A (current)
â”œâ”€â”€ Track conversion metrics
â””â”€â”€ Baseline performance

Treatment Group (50%):
â”œâ”€â”€ See Version B (new feature)
â”œâ”€â”€ Track conversion metrics
â””â”€â”€ Compare to baseline

Analysis:
â”œâ”€â”€ Conversion rates
â”œâ”€â”€ User engagement
â”œâ”€â”€ Revenue impact
â”œâ”€â”€ User satisfaction
â””â”€â”€ Statistical significance

Decision:
â””â”€â”€ Keep version with better metrics
```

---

### Application-Level A/B Testing

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// A/B TESTING - APPLICATION LEVEL
// Feature flag based implementation
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const express = require("express");
const app = express();

// A/B testing configuration
const experiments = {
  "new-checkout-flow": {
    enabled: true,
    variants: {
      control: { weight: 50 }, // 50% see old version
      treatment: { weight: 50 }, // 50% see new version
    },
    // Specific users/segments
    overrides: {
      "vip-users": "treatment", // VIPs always see new
      "internal-team": "treatment", // Team always sees new
    },
  },
};

// Analytics tracking
const analytics = {
  track: (userId, event, properties) => {
    console.log(`[Analytics] User ${userId}: ${event}`, properties);
    // Send to analytics service (Mixpanel, Amplitude, etc.)
  },
};

// Hash function for consistent assignment
function hashUserId(userId) {
  let hash = 0;
  for (let i = 0; i < userId.length; i++) {
    const char = userId.charCodeAt(i);
    hash = (hash << 5) - hash + char;
    hash = hash & hash; // Convert to 32bit integer
  }
  return Math.abs(hash);
}

// Assign user to variant
function getVariant(userId, experimentKey) {
  const experiment = experiments[experimentKey];

  if (!experiment || !experiment.enabled) {
    return "control";
  }

  // Check for user segment override
  const userSegment = getUserSegment(userId);
  if (experiment.overrides[userSegment]) {
    return experiment.overrides[userSegment];
  }

  // Consistent hash-based assignment
  const hash = hashUserId(userId);
  const bucket = hash % 100;

  let cumulative = 0;
  for (const [variant, config] of Object.entries(experiment.variants)) {
    cumulative += config.weight;
    if (bucket < cumulative) {
      return variant;
    }
  }

  return "control";
}

// Middleware to assign experiments
app.use((req, res, next) => {
  const userId = req.session?.userId || req.cookies?.userId;

  if (!userId) {
    return next();
  }

  // Assign all active experiments
  req.experiments = {};
  for (const experimentKey in experiments) {
    const variant = getVariant(userId, experimentKey);
    req.experiments[experimentKey] = variant;

    // Track assignment
    analytics.track(userId, "Experiment Assigned", {
      experiment: experimentKey,
      variant: variant,
    });
  }

  next();
});

// Routes with A/B testing
app.get("/checkout", (req, res) => {
  const variant = req.experiments["new-checkout-flow"];

  // Track page view
  analytics.track(req.session.userId, "Checkout Page Viewed", {
    variant: variant,
  });

  if (variant === "treatment") {
    // New checkout flow
    res.render("checkout-v2", {
      variant: "treatment",
      features: {
        oneClickCheckout: true,
        expressShipping: true,
        guestCheckout: true,
      },
    });
  } else {
    // Original checkout flow
    res.render("checkout-v1", {
      variant: "control",
    });
  }
});

// Track conversion
app.post("/checkout/complete", (req, res) => {
  const variant = req.experiments["new-checkout-flow"];
  const orderValue = req.body.total;

  // Track conversion event
  analytics.track(req.session.userId, "Purchase Completed", {
    variant: variant,
    orderValue: orderValue,
    experiment: "new-checkout-flow",
  });

  res.json({ success: true });
});

// Helper: Get user segment
function getUserSegment(userId) {
  // In real app, query from database
  // For example: check if user is VIP, internal, etc.
  return "standard";
}
```

---

### Infrastructure-Level A/B Testing (Istio)

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A/B TESTING - INFRASTRUCTURE LEVEL
# Using Istio for traffic routing based on headers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Destination Rule (Define versions)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: my-app
spec:
  host: my-app
  subsets:
    - name: version-a
      labels:
        version: v1.0.0
    - name: version-b
      labels:
        version: v2.0.0

---
# Virtual Service (Route based on user segment)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
    - my-app
  http:
    # VIP users always get version B
    - match:
        - headers:
            user-segment:
              exact: vip
      route:
        - destination:
            host: my-app
            subset: version-b
          weight: 100

    # Internal team always gets version B
    - match:
        - headers:
            user-segment:
              exact: internal
      route:
        - destination:
            host: my-app
            subset: version-b
          weight: 100

    # Regular users: 50/50 split
    - route:
        - destination:
            host: my-app
            subset: version-a
          weight: 50
          headers:
            response:
              set:
                x-variant: "control"
        - destination:
            host: my-app
            subset: version-b
          weight: 50
          headers:
            response:
              set:
                x-variant: "treatment"
```

---

### Using LaunchDarkly (Feature Flag Service)

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// LAUNCHDARKLY - FEATURE FLAG SERVICE
// Enterprise-grade A/B testing
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const LaunchDarkly = require("launchdarkly-node-server-sdk");

// Initialize LaunchDarkly client
const ldClient = LaunchDarkly.init("YOUR_SDK_KEY");

// Wait for client to be ready
ldClient.once("ready", () => {
  console.log("LaunchDarkly client initialized");
});

// Middleware to add feature flags
app.use(async (req, res, next) => {
  const user = {
    key: req.session.userId,
    email: req.session.email,
    custom: {
      userSegment: req.session.userSegment,
      accountAge: req.session.accountAge,
      totalPurchases: req.session.totalPurchases,
    },
  };

  // Get all feature flags for user
  req.featureFlags = await ldClient.allFlagsState(user);

  next();
});

// Use feature flag in route
app.get("/checkout", async (req, res) => {
  const user = {
    key: req.session.userId,
    email: req.session.email,
    custom: {
      userSegment: req.session.userSegment,
    },
  };

  // Check feature flag
  const useNewCheckout = await ldClient.variation(
    "new-checkout-flow",
    user,
    false // Default value
  );

  if (useNewCheckout) {
    res.render("checkout-v2");
  } else {
    res.render("checkout-v1");
  }

  // Track that user saw this variation
  ldClient.track("checkout-page-viewed", user, {
    variant: useNewCheckout ? "new" : "old",
  });
});

// Graceful shutdown
process.on("SIGTERM", () => {
  ldClient.close();
});
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Test specific features
â€¢ Measure business impact
â€¢ Target specific user segments
â€¢ Data-driven decisions
â€¢ Can run multiple experiments
â€¢ Marketing & product testing

âŒ CONS:
â€¢ Complex implementation
â€¢ Need analytics integration
â€¢ Statistical significance takes time
â€¢ Can confuse users if inconsistent
â€¢ Maintenance overhead

ğŸ¯ USE WHEN:
âœ“ Testing new features
âœ“ Optimizing conversion
âœ“ Product experimentation
âœ“ Marketing campaigns
âœ“ Need data-driven decisions

ğŸ“Š REQUIREMENTS:
â€¢ Analytics platform
â€¢ Feature flag system
â€¢ Statistical analysis
â€¢ Large user base
â€¢ Business metrics tracking

ğŸ¯ PERFECT FOR:
â€¢ E-commerce optimization
â€¢ SaaS feature testing
â€¢ Marketing experiments
â€¢ UI/UX improvements
â€¢ Pricing strategies
```

---

<div align="center">

## ğŸ­ Shadow Deployment

_Test with real traffic without user impact_ ğŸ‘¥

</div>

### How It Works

```
Traffic Flow:

User Request
     â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     â”‚                  â”‚
     â–¼                  â–¼
Production Env     Shadow Env
(v1.0 - Live)     (v2.0 - Testing)
     â”‚                  â”‚
     â”‚                  â”‚ (Response discarded)
     â–¼                  â–¼
User Response      Metrics Only
     â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(Only from production)

Purpose:
â€¢ Test v2.0 with real production traffic
â€¢ Compare metrics: latency, errors, resources
â€¢ No user impact (shadow responses discarded)
â€¢ Validate before actual deployment
```

---

### Istio Shadow Deployment

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHADOW DEPLOYMENT - ISTIO
# Mirror traffic to shadow environment
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
    - my-app
  http:
    - route:
        # Primary route (production)
        - destination:
            host: my-app
            subset: production
          weight: 100
      # Mirror to shadow environment
      mirror:
        host: my-app
        subset: shadow
      mirrorPercentage:
        value: 100 # Mirror 100% of traffic

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: my-app
spec:
  host: my-app
  subsets:
    - name: production
      labels:
        version: v1.0.0
    - name: shadow
      labels:
        version: v2.0.0
```

---

### Nginx Shadow Deployment

```nginx
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SHADOW DEPLOYMENT - NGINX
# Mirror traffic using mirror directive
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

upstream production {
    server app-v1:8080;
}

upstream shadow {
    server app-v2:8080;
}

server {
    listen 80;
    server_name app.example.com;

    location / {
        # Primary request to production
        proxy_pass http://production;

        # Mirror request to shadow
        mirror /mirror;
        mirror_request_body on;

        # Standard proxy headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # Internal location for mirror
    location = /mirror {
        internal;
        proxy_pass http://shadow$request_uri;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;

        # Don't wait for shadow response
        proxy_ignore_client_abort on;
    }
}
```

---

### Application-Level Shadow Testing

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// APPLICATION-LEVEL SHADOW TESTING
// Mirror requests to shadow version
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const express = require("express");
const axios = require("axios");
const app = express();

// Configuration
const PRODUCTION_SERVICE = "http://app-v1:8080";
const SHADOW_SERVICE = "http://app-v2:8080";
const SHADOW_PERCENTAGE = 100; // % of traffic to shadow

// Middleware to mirror requests
app.use(async (req, res, next) => {
  // Forward to production (primary)
  try {
    const productionResponse = await axios({
      method: req.method,
      url: `${PRODUCTION_SERVICE}${req.path}`,
      data: req.body,
      headers: req.headers,
      timeout: 5000,
    });

    // Send production response to user
    res.status(productionResponse.status).json(productionResponse.data);

    // Mirror to shadow (async, don't wait)
    if (Math.random() * 100 < SHADOW_PERCENTAGE) {
      mirrorToShadow(req).catch((err) => {
        console.error("Shadow request failed:", err.message);
      });
    }
  } catch (error) {
    res.status(500).json({ error: "Service unavailable" });
  }
});

// Mirror request to shadow environment
async function mirrorToShadow(req) {
  const startTime = Date.now();

  try {
    const shadowResponse = await axios({
      method: req.method,
      url: `${SHADOW_SERVICE}${req.path}`,
      data: req.body,
      headers: {
        ...req.headers,
        "X-Shadow-Request": "true",
      },
      timeout: 5000,
    });

    const duration = Date.now() - startTime;

    // Log metrics for comparison
    logShadowMetrics({
      path: req.path,
      method: req.method,
      status: shadowResponse.status,
      duration: duration,
      success: true,
    });
  } catch (error) {
    const duration = Date.now() - startTime;

    logShadowMetrics({
      path: req.path,
      method: req.method,
      status: error.response?.status || 500,
      duration: duration,
      success: false,
      error: error.message,
    });
  }
}

// Log shadow metrics to monitoring system
function logShadowMetrics(metrics) {
  // Send to Prometheus, DataDog, CloudWatch, etc.
  console.log("[Shadow Metrics]", JSON.stringify(metrics));
}

app.listen(8080);
```

---

### Monitoring Shadow Deployment

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMETHEUS METRICS FOR SHADOW DEPLOYMENT
# Compare production vs shadow performance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Grafana Dashboard Query Examples:

# 1. Latency Comparison
# Production latency
histogram_quantile(0.99,
  rate(http_request_duration_seconds_bucket{
    version="production"
  }[5m])
)

# Shadow latency
histogram_quantile(0.99,
  rate(http_request_duration_seconds_bucket{
    version="shadow"
  }[5m])
)

# 2. Error Rate Comparison
# Production errors
sum(rate(http_requests_total{
  version="production",
  status=~"5.."
}[5m]))

# Shadow errors
sum(rate(http_requests_total{
  version="shadow",
  status=~"5.."
}[5m]))

# 3. Resource Usage Comparison
# Production CPU
rate(container_cpu_usage_seconds_total{
  pod=~"app-production.*"
}[5m])

# Shadow CPU
rate(container_cpu_usage_seconds_total{
  pod=~"app-shadow.*"
}[5m])

# 4. Alert if shadow performance degrades
- alert: ShadowPerformanceDegraded
  expr: |
    (
      histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{version="shadow"}[5m]))
      /
      histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{version="production"}[5m]))
    ) > 1.2
  for: 10m
  annotations:
    summary: "Shadow environment 20% slower than production"
```

---

### Pros & Cons

```
âœ… PROS:
â€¢ Zero user impact (responses discarded)
â€¢ Test with real production traffic
â€¢ Compare performance directly
â€¢ Validate before rollout
â€¢ Find edge cases
â€¢ Load testing with real patterns

âŒ CONS:
â€¢ 2x infrastructure load
â€¢ Doubles resource usage
â€¢ Can't test user-facing changes
â€¢ Database writes can be tricky
â€¢ Complex to set up

ğŸ¯ USE WHEN:
âœ“ Testing performance improvements
âœ“ Validating refactors
âœ“ Testing new algorithms
âœ“ Load testing
âœ“ Backend changes only

âš ï¸ CHALLENGES:
â€¢ Side effects (don't write to production DB!)
â€¢ Idempotency issues
â€¢ External API calls (2x cost)
â€¢ Real-time systems

ğŸ’¡ BEST PRACTICES:
âœ“ Only mirror read operations
âœ“ Use separate database for shadow
âœ“ Monitor resource usage closely
âœ“ Start with small percentage
âœ“ Set timeout limits
```

---

<div align="center">

## ğŸ”€ Feature Flags

_Decouple deployment from release_ ğŸšï¸

</div>

### Feature Flag Patterns

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FEATURE FLAGS - IMPLEMENTATION PATTERNS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. SIMPLE BOOLEAN FLAG
const features = {
  newCheckout: process.env.FEATURE_NEW_CHECKOUT === "true",
};

if (features.newCheckout) {
  // New feature
} else {
  // Old feature
}

// 2. PERCENTAGE ROLLOUT
function isFeatureEnabled(userId, featureName, percentage) {
  const hash = hashUserId(userId);
  return hash % 100 < percentage;
}

// 10% of users see new feature
if (isFeatureEnabled(userId, "newDashboard", 10)) {
  return renderNewDashboard();
}

// 3. USER SEGMENT TARGETING
const featureConfig = {
  newDashboard: {
    enabled: true,
    rules: [
      { segment: "beta-testers", enabled: true },
      { segment: "enterprise", enabled: true },
      { segment: "free-tier", enabled: false },
    ],
  },
};

function canAccessFeature(user, featureName) {
  const config = featureConfig[featureName];
  const userSegment = getUserSegment(user);

  const rule = config.rules.find((r) => r.segment === userSegment);
  return rule ? rule.enabled : false;
}

// 4. TIME-BASED ACTIVATION
const features = {
  blackFridaySale: {
    enabled: true,
    startDate: "2024-11-29T00:00:00Z",
    endDate: "2024-11-30T23:59:59Z",
  },
};

function isFeatureActive(featureName) {
  const feature = features[featureName];
  const now = new Date();
  const start = new Date(feature.startDate);
  const end = new Date(feature.endDate);

  return feature.enabled && now >= start && now <= end;
}

// 5. DEPENDENCY CHECKING
const features = {
  advancedAnalytics: {
    enabled: true,
    dependencies: ["newDashboard", "premiumPlan"],
  },
};

function canUseFeature(user, featureName) {
  const feature = features[featureName];

  // Check dependencies
  for (const dep of feature.dependencies) {
    if (!canUseFeature(user, dep)) {
      return false;
    }
  }

  return feature.enabled;
}
```

---

### Feature Flag Service (Self-Hosted)

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FEATURE FLAG SERVICE
// Simple self-hosted implementation
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const express = require("express");
const redis = require("redis");
const app = express();

const redisClient = redis.createClient({
  url: process.env.REDIS_URL,
});

// Feature flag configuration in Redis
// Key: feature:{name}
// Value: JSON config

// Get feature flag
app.get("/api/features/:name", async (req, res) => {
  const { name } = req.params;
  const { userId, userSegment } = req.query;

  try {
    // Get feature config from Redis
    const configStr = await redisClient.get(`feature:${name}`);

    if (!configStr) {
      return res.json({ enabled: false });
    }

    const config = JSON.parse(configStr);

    // Evaluate feature flag
    const enabled = evaluateFeatureFlag(config, {
      userId,
      userSegment,
    });

    res.json({
      name,
      enabled,
      variant: enabled ? config.variant : "control",
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Create/Update feature flag
app.put("/api/features/:name", async (req, res) => {
  const { name } = req.params;
  const config = req.body;

  // Validate config
  if (!config.enabled === undefined) {
    return res.status(400).json({ error: "enabled field required" });
  }

  // Save to Redis
  await redisClient.set(`feature:${name}`, JSON.stringify(config));

  // Publish update event
  await redisClient.publish(
    "feature-flag-updates",
    JSON.stringify({ name, config })
  );

  res.json({ success: true });
});

// Evaluate feature flag logic
function evaluateFeatureFlag(config, context) {
  if (!config.enabled) {
    return false;
  }

  // User segment targeting
  if (config.segments && config.segments.length > 0) {
    if (!config.segments.includes(context.userSegment)) {
      return false;
    }
  }

  // Percentage rollout
  if (config.percentage !== undefined) {
    const hash = hashUserId(context.userId);
    if (hash % 100 >= config.percentage) {
      return false;
    }
  }

  // Time window
  if (config.startDate || config.endDate) {
    const now = new Date();
    if (config.startDate && now < new Date(config.startDate)) {
      return false;
    }
    if (config.endDate && now > new Date(config.endDate)) {
      return false;
    }
  }

  return true;
}

// Client SDK
class FeatureFlagClient {
  constructor(apiUrl) {
    this.apiUrl = apiUrl;
    this.cache = new Map();
    this.cacheTimeout = 60000; // 1 minute
  }

  async isEnabled(featureName, context = {}) {
    // Check cache
    const cached = this.cache.get(featureName);
    if (cached && Date.now() - cached.timestamp < this.cacheTimeout) {
      return cached.enabled;
    }

    // Fetch from API
    const response = await fetch(
      `${this.apiUrl}/api/features/${featureName}?` +
        new URLSearchParams(context)
    );

    const data = await response.json();

    // Update cache
    this.cache.set(featureName, {
      enabled: data.enabled,
      timestamp: Date.now(),
    });

    return data.enabled;
  }

  // Real-time updates via WebSocket
  subscribeToUpdates() {
    const ws = new WebSocket(`${this.apiUrl.replace("http", "ws")}/updates`);

    ws.on("message", (data) => {
      const update = JSON.parse(data);
      // Invalidate cache for updated feature
      this.cache.delete(update.name);
    });
  }
}

// Usage in application
const featureFlags = new FeatureFlagClient("http://feature-service:3000");

app.get("/dashboard", async (req, res) => {
  const useNewDashboard = await featureFlags.isEnabled("new-dashboard", {
    userId: req.user.id,
    userSegment: req.user.segment,
  });

  if (useNewDashboard) {
    res.render("dashboard-v2");
  } else {
    res.render("dashboard-v1");
  }
});
```

---

### Feature Flag Best Practices

```

ğŸ¯ FEATURE FLAG COMMANDMENTS:

1ï¸âƒ£ Keep Flags Short-Lived
â€¢ Remove after rollout completes
â€¢ Technical debt accumulates fast
â€¢ Set expiration dates

2ï¸âƒ£ Name Descriptively
âœ… enable-new-checkout-flow-2024
âŒ flag_123 or newFeature

3ï¸âƒ£ Clean Up Old Flags
â€¢ Monthly audit
â€¢ Remove code for removed flags
â€¢ Keep flag list manageable

4ï¸âƒ£ Default to Safe State
â€¢ Default should be "off" or "old behavior"
â€¢ If flag system fails, use safe default

5ï¸âƒ£ Monitor Flag Usage
â€¢ Track which flags are active
â€¢ Log flag evaluations
â€¢ Alert on unused flags

6ï¸âƒ£ Document Purpose
â€¢ Why was this flag created?
â€¢ When can it be removed?
â€¢ Who owns it?

7ï¸âƒ£ Avoid Flag Complexity
â€¢ Don't nest flags deeply
â€¢ Avoid flag dependencies
â€¢ Keep logic simple

8ï¸âƒ£ Test Both States
â€¢ Test with flag ON
â€¢ Test with flag OFF
â€¢ Test flag transitions

9ï¸âƒ£ Secure Flag Management
â€¢ Who can change flags?
â€¢ Audit flag changes
â€¢ Protect production flags

ğŸ”Ÿ Use Kill Switches
â€¢ Ability to instantly disable features
â€¢ Emergency rollback without deploy

```

---

<div align="center">

## ğŸ—„ï¸ Database Migrations

_The deployment challenge nobody talks about_ ğŸ’€

</div>

### The Database Migration Problem

```

The Challenge:
â”œâ”€â”€ New code version needs new database schema
â”œâ”€â”€ Can't have downtime
â”œâ”€â”€ Must support both old and new code during rollout
â””â”€â”€ Need rollback capability

Example Scenario:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Old Code expects: users.name (string) â”‚
â”‚ New Code expects: users.first_name + â”‚
â”‚ users.last_name â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ BAD: Deploy new code + migration together
â†’ Old pods crash when schema changes

âœ… GOOD: 3-phase deployment (expand-migrate-contract)

```

---

### Expand-Migrate-Contract Pattern

```

Phase 1: EXPAND (Add new columns)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ users â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id â”‚ integer â”‚
â”‚ name â”‚ string (keep old) â”‚
â”‚ first_name â”‚ string (add new) â”‚
â”‚ last_name â”‚ string (add new) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deploy: Migration adds new columns
Code: Old code still uses 'name' (works)
New code uses 'first_name' + 'last_name' (works)

Phase 2: MIGRATE (Copy data)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Background job copies data: â”‚
â”‚ Split 'name' â†’ 'first_name' â”‚
â”‚ + 'last_name' â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 3: CONTRACT (Remove old column)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ users â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id â”‚ integer â”‚
â”‚ first_name â”‚ string â”‚
â”‚ last_name â”‚ string â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deploy: Migration removes 'name' column
Code: Only new code deployed (all uses new columns)

```

---

### Migration Examples

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DATABASE MIGRATION - EXPAND PHASE
-- Add new columns without breaking existing code
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Migration 001: Add new columns
CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Add new columns (nullable initially)
ALTER TABLE users
ADD COLUMN first_name VARCHAR(255),
ADD COLUMN last_name VARCHAR(255);

-- Create indexes for new columns
CREATE INDEX idx_users_first_name ON users(first_name);
CREATE INDEX idx_users_last_name ON users(last_name);

-- Trigger to sync old â†’ new (during transition)
CREATE OR REPLACE FUNCTION sync_user_name()
RETURNS TRIGGER AS $$
BEGIN
    IF NEW.name IS NOT NULL AND (NEW.first_name IS NULL OR NEW.last_name IS NULL) THEN
        NEW.first_name := split_part(NEW.name, ' ', 1);
        NEW.last_name := COALESCE(split_part(NEW.name, ' ', 2), '');
    END IF;

    IF NEW.first_name IS NOT NULL AND NEW.last_name IS NOT NULL THEN
        NEW.name := NEW.first_name || ' ' || NEW.last_name;
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER user_name_sync
BEFORE INSERT OR UPDATE ON users
FOR EACH ROW
EXECUTE FUNCTION sync_user_name();

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- MIGRATE PHASE - Background job
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Batch update existing data
DO $$
DECLARE
    batch_size INT := 1000;
    processed INT := 0;
BEGIN
    LOOP
        UPDATE users
        SET
            first_name = split_part(name, ' ', 1),
            last_name = COALESCE(split_part(name, ' ', 2), '')
        WHERE id IN (
            SELECT id
            FROM users
            WHERE first_name IS NULL
            LIMIT batch_size
        );

        GET DIAGNOSTICS processed = ROW_COUNT;
        EXIT WHEN processed = 0;

        -- Sleep between batches to avoid load
        PERFORM pg_sleep(1);
    END LOOP;
END $$;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- CONTRACT PHASE - Remove old column
-- Only after ALL code uses new columns!
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Drop trigger
DROP TRIGGER IF EXISTS user_name_sync ON users;
DROP FUNCTION IF EXISTS sync_user_name();

-- Make new columns NOT NULL
ALTER TABLE users
ALTER COLUMN first_name SET NOT NULL,
ALTER COLUMN last_name SET NOT NULL;

-- Remove old column
ALTER TABLE users DROP COLUMN name;
```

---

### Zero-Downtime Migration Strategy

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// APPLICATION CODE - COMPATIBLE WITH BOTH SCHEMAS
// Supports old and new database schema
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class User {
  constructor(data) {
    this.id = data.id;
    this.email = data.email;

    // Support both old and new schema
    if (data.first_name && data.last_name) {
      // New schema
      this.firstName = data.first_name;
      this.lastName = data.last_name;
      this.name = `${data.first_name} ${data.last_name}`;
    } else if (data.name) {
      // Old schema
      this.name = data.name;
      const parts = data.name.split(" ");
      this.firstName = parts[0] || "";
      this.lastName = parts.slice(1).join(" ") || "";
    }
  }

  // Save method works with both schemas
  async save(db) {
    // Write to both old and new columns (during transition)
    return await db.query(
      `
      UPDATE users
      SET
        name = $1,
        first_name = $2,
        last_name = $3
      WHERE id = $4
    `,
      [this.name, this.firstName, this.lastName, this.id]
    );
  }

  // After migration complete, simplify:
  async saveNew(db) {
    return await db.query(
      `
      UPDATE users
      SET
        first_name = $1,
        last_name = $2
      WHERE id = $3
    `,
      [this.firstName, this.lastName, this.id]
    );
  }
}
```

---

### Migration Tools

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATABASE MIGRATION TOOLS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Flyway (Java-based)
flyway migrate -url=jdbc:postgresql://localhost/mydb -user=user -password=pass

# Liquibase (Java-based)
liquibase update --changelog-file=changelog.xml

# Alembic (Python)
alembic upgrade head

# Prisma (Node.js)
npx prisma migrate deploy

# golang-migrate
migrate -path ./migrations -database postgres://localhost/mydb up

# Atlas (Modern declarative migrations)
atlas schema apply --url "postgres://localhost/mydb"
```

---

### Migration Best Practices

```
ğŸ¯ DATABASE MIGRATION RULES:

1ï¸âƒ£  Never Break Backwards Compatibility
    â€¢ Old code must work with new schema
    â€¢ Use expand-migrate-contract pattern

2ï¸âƒ£  Make Changes in Small Steps
    â€¢ One change per migration
    â€¢ Deploy frequently
    â€¢ Easier to rollback

3ï¸âƒ£  Test Migrations Thoroughly
    â€¢ Test on production-like data
    â€¢ Test rollback procedures
    â€¢ Load test with new schema

4ï¸âƒ£  Use Transactions (When Possible)
    BEGIN;
    -- migration
    COMMIT;

5ï¸âƒ£  Avoid Large Table Locks
    â€¢ Use concurrent indexes
    â€¢ Batch large updates
    â€¢ Avoid ALTER TABLE on huge tables

6ï¸âƒ£  Always Have Rollback Plan
    â€¢ Write down migration
    â€¢ Write rollback migration
    â€¢ Test both!

7ï¸âƒ£  Monitor During Migration
    â€¢ Database locks
    â€¢ Replication lag
    â€¢ Application errors
    â€¢ Query performance

8ï¸âƒ£  Communicate Schema Changes
    â€¢ Notify team before deployment
    â€¢ Document breaking changes
    â€¢ Coordinate with app deployments

ğŸ“‹ SAFE OPERATIONS (No downtime):
âœ… Add new table
âœ… Add new column (nullable)
âœ… Add new index (CONCURRENT)
âœ… Create new trigger
âœ… Add CHECK constraint (NOT VALID, then VALIDATE)

âš ï¸ RISKY OPERATIONS (Need careful planning):
âš ï¸ Drop column (need expand-contract)
âš ï¸ Rename column (need expand-contract)
âš ï¸ Change column type
âš ï¸ Add NOT NULL constraint
âš ï¸ Drop table

âŒ NEVER DO:
âŒ Drop column in same deploy as code change
âŒ Rename column without backward compatibility
âŒ Change column type without migration period
```

---

<div align="center">

## ğŸ”™ Rollback Strategies

_When things go wrong (and they will)_ ğŸš¨

</div>

### Rollback Methods Comparison

<div align="center">

| Method                 |   Speed    | Data Loss Risk | Complexity | Best For    |
| :--------------------- | :--------: | :------------: | :--------: | :---------- |
| **Service Switch**     | âš¡âš¡âš¡âš¡âš¡ |      Low       |    Low     | Blue-Green  |
| **Container Rollback** |  âš¡âš¡âš¡âš¡  |      Low       |    Low     | Kubernetes  |
| **Git Revert**         |   âš¡âš¡âš¡   |     Medium     |   Medium   | Simple apps |
| **Database Restore**   |     âš¡     |      High      |    High    | Last resort |
| **Feature Flag**       | âš¡âš¡âš¡âš¡âš¡ |      None      |    Low     | Modern apps |

</div>

---

### Kubernetes Rollback

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KUBERNETES ROLLBACK COMMANDS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# View deployment history
kubectl rollout history deployment/my-app

# Output:
# REVISION  CHANGE-CAUSE
# 1         Initial deployment
# 2         Update to v2.0.0
# 3         Update to v2.1.0

# Check rollout status
kubectl rollout status deployment/my-app

# Undo to previous version (most common)
kubectl rollout undo deployment/my-app

# Undo to specific revision
kubectl rollout undo deployment/my-app --to-revision=2

# Pause rollout (stop mid-deployment)
kubectl rollout pause deployment/my-app

# Resume paused rollout
kubectl rollout resume deployment/my-app

# Restart deployment (useful for ConfigMap changes)
kubectl rollout restart deployment/my-app
```

---

### Automated Rollback Script

```bash
#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTOMATED ROLLBACK SCRIPT
# Monitors deployment and rolls back if issues detected
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -euo pipefail

APP_NAME="my-app"
NAMESPACE="production"
ERROR_THRESHOLD=0.05
LATENCY_THRESHOLD_MS=1000
MONITORING_DURATION=300  # 5 minutes

# Check if deployment is healthy
check_deployment_health() {
    local deployment=$1

    # Check pod status
    local ready=$(kubectl get deployment $deployment -n $NAMESPACE \
      -o jsonpath='{.status.readyReplicas}')
    local desired=$(kubectl get deployment $deployment -n $NAMESPACE \
      -o jsonpath='{.spec.replicas}')

    if [ "$ready" != "$desired" ]; then
        echo "UNHEALTHY: $ready/$desired pods ready"
        return 1
    fi

    # Check for pod crashes
    local crashes=$(kubectl get pods -n $NAMESPACE -l app=$APP_NAME \
      -o jsonpath='{range .items[*]}{.status.containerStatuses[0].restartCount}{"\n"}{end}' | \
      awk '{sum+=$1} END {print sum}')

    if [ "$crashes" -gt 5 ]; then
        echo "UNHEALTHY: Too many restarts ($crashes)"
        return 1
    fi

    return 0
}

# Check application metrics
check_application_metrics() {
    # Query Prometheus
    local error_rate=$(curl -s 'http://prometheus:9090/api/v1/query' \
      --data-urlencode "query=rate(http_requests_total{app=\"$APP_NAME\",status=~\"5..\"}[5m])" | \
      jq -r '.data.result[0].value[1] // 0')

    local latency=$(curl -s 'http://prometheus:9090/api/v1/query' \
      --data-urlencode "query=histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{app=\"$APP_NAME\"}[5m])) * 1000" | \
      jq -r '.data.result[0].value[1] // 0')

    echo "Metrics: error_rate=$error_rate, p99_latency=${latency}ms"

    # Check thresholds
    if (( $(echo "$error_rate > $ERROR_THRESHOLD" | bc -l) )); then
        echo "UNHEALTHY: Error rate too high ($error_rate)"
        return 1
    fi

    if (( $(echo "$latency > $LATENCY_THRESHOLD_MS" | bc -l) )); then
        echo "UNHEALTHY: Latency too high (${latency}ms)"
        return 1
    fi

    return 0
}

# Perform rollback
perform_rollback() {
    echo "ğŸš¨ ROLLING BACK DEPLOYMENT! ğŸš¨"

    # Get current revision
    local current_rev=$(kubectl rollout history deployment/$APP_NAME -n $NAMESPACE | \
      tail -n 1 | awk '{print $1}')

    # Rollback
    kubectl rollout undo deployment/$APP_NAME -n $NAMESPACE

    # Wait for rollback to complete
    kubectl rollout status deployment/$APP_NAME -n $NAMESPACE --timeout=300s

    # Verify health after rollback
    sleep 30
    if check_deployment_health "$APP_NAME"; then
        echo "âœ… Rollback successful!"
    else
        echo "âŒ Rollback completed but issues remain!"
    fi

    # Send alert
    send_alert "Deployment rolled back from revision $current_rev"
}

# Send alert to team
send_alert() {
    local message=$1

    # Slack
    curl -X POST "$SLACK_WEBHOOK_URL" \
      -H 'Content-Type: application/json' \
      -d "{\"text\":\"ğŸš¨ $message\"}"

    # PagerDuty
    # curl -X POST "https://events.pagerduty.com/v2/enqueue" ...
}

# Main monitoring loop
main() {
    echo "ğŸ” Monitoring deployment of $APP_NAME..."

    local start_time=$(date +%s)
    local end_time=$((start_time + MONITORING_DURATION))
    local check_interval=30
    local consecutive_failures=0
    local max_consecutive_failures=3

    while [ $(date +%s) -lt $end_time ]; do
        echo "--- Health Check ---"

        # Check deployment health
        if ! check_deployment_health "$APP_NAME"; then
            ((consecutive_failures++))
            echo "âš ï¸ Health check failed ($consecutive_failures/$max_consecutive_failures)"
        else
            echo "âœ… Deployment healthy"
            consecutive_failures=0
        fi

        # Check application metrics
        if ! check_application_metrics; then
            ((consecutive_failures++))
            echo "âš ï¸ Metrics check failed ($consecutive_failures/$max_consecutive_failures)"
        else
            echo "âœ… Metrics healthy"
        fi

        # Rollback if too many failures
        if [ $consecutive_failures -ge $max_consecutive_failures ]; then
            perform_rollback
            exit 1
        fi

        # Wait before next check
        sleep $check_interval
    done

    echo "âœ… Monitoring complete - deployment successful!"
}

main "$@"
```

---

### Database Rollback Strategy

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DATABASE ROLLBACK STRATEGIES
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Strategy 1: Keep old columns during transition
-- This allows rollback without data loss

-- Forward migration (can rollback)
ALTER TABLE users ADD COLUMN new_field VARCHAR(255);
UPDATE users SET new_field = old_field;

-- Rollback: Just use old field again
-- No data loss!

-- Strategy 2: Versioned schema
-- Keep schema version in database

CREATE TABLE schema_version (
    version INT PRIMARY KEY,
    applied_at TIMESTAMP DEFAULT NOW(),
    description TEXT
);

-- Each migration records version
INSERT INTO schema_version (version, description)
VALUES (5, 'Added new_field to users table');

-- Application checks compatible version range
-- version >= 5 AND version <= 10

-- Strategy 3: Shadow tables for risky migrations
-- Keep old data in separate table during migration

-- Create shadow table
CREATE TABLE users_backup AS SELECT * FROM users;

-- Perform migration
ALTER TABLE users ADD COLUMN new_structure JSONB;
UPDATE users SET new_structure = to_jsonb(old_structure);

-- If rollback needed:
DROP TABLE users;
ALTER TABLE users_backup RENAME TO users;

-- Strategy 4: Soft deletes instead of hard deletes
-- Never actually delete data

ALTER TABLE users ADD COLUMN deleted_at TIMESTAMP;
CREATE INDEX idx_users_deleted ON users(deleted_at) WHERE deleted_at IS NULL;

-- "Delete" records
UPDATE users SET deleted_at = NOW() WHERE id = 123;

-- Rollback deletion
UPDATE users SET deleted_at = NULL WHERE id = 123;
```

---

### Rollback Checklist

```markdown
## Rollback Decision Checklist

### Trigger Rollback If:

- [ ] Error rate > 5% for 5+ minutes
- [ ] P99 latency > 2x baseline
- [ ] More than 10% of pods crashing
- [ ] Critical functionality broken
- [ ] Database connection issues
- [ ] External service failures
- [ ] Memory leaks detected
- [ ] CPU usage > 90% sustained

### Before Rolling Back:

- [ ] Confirm issue is from deployment (not infrastructure)
- [ ] Document symptoms
- [ ] Capture logs & metrics
- [ ] Notify team
- [ ] Check if hotfix is faster than rollback

### During Rollback:

- [ ] Monitor rollback progress
- [ ] Verify health checks
- [ ] Check database consistency
- [ ] Test critical flows
- [ ] Monitor for new issues

### After Rollback:

- [ ] Verify system stability
- [ ] Conduct post-mortem
- [ ] Fix root cause
- [ ] Update tests
- [ ] Plan re-deployment
```

---

<div align="center">

## ğŸ“Š Monitoring & Observability

_You can't deploy what you can't measure_ ğŸ“ˆ

</div>

### Key Deployment Metrics

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMETHEUS METRICS FOR DEPLOYMENTS
# Track deployment health and performance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Deployment Success Rate
deployment_success_total / deployment_attempts_total

# 2. Deployment Duration
histogram_quantile(0.99, deployment_duration_seconds_bucket)

# 3. Rollback Rate
rollback_total / deployment_attempts_total

# 4. Error Rate During Deployment
rate(http_requests_total{status=~"5.."}[5m])

# 5. Latency During Deployment
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# 6. Pod Restart Count
sum(kube_pod_container_status_restarts_total) by (pod)

# 7. Deployment Frequency (DORA Metric)
count_over_time(deployment_completed_total[7d])

# 8. Mean Time to Recovery (DORA Metric)
avg(time_to_recovery_seconds)

# 9. Change Failure Rate (DORA Metric)
failed_deployments_total / deployment_attempts_total
```

---

### Grafana Dashboard Example

```json
{
  "dashboard": {
    "title": "Deployment Monitoring",
    "panels": [
      {
        "title": "Deployment Status",
        "targets": [
          {
            "expr": "deployment_status{app=\"my-app\"}",
            "legendFormat": "{{version}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "Errors/sec"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [0.05],
                "type": "gt"
              }
            }
          ]
        }
      },
      {
        "title": "P99 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "Pod Health",
        "targets": [
          {
            "expr": "kube_deployment_status_replicas_available / kube_deployment_spec_replicas",
            "legendFormat": "Available %"
          }
        ]
      }
    ]
  }
}
```

---

<div align="center">

## ğŸ› ï¸ Tools & Platforms

_The deployment toolbox_ ğŸ§°

</div>

### Deployment Tool Comparison

<div align="center">

| Tool               | Type       | Learning Curve | Best For         |    Cost     |
| :----------------- | :--------- | :------------: | :--------------- | :---------: |
| **Kubernetes**     | Platform   |   â­â­â­â­â­   | Everything       |    Free     |
| **ArgoCD**         | GitOps     |     â­â­â­     | K8s deployments  |    Free     |
| **Spinnaker**      | Pipeline   |    â­â­â­â­    | Multi-cloud      |    Free     |
| **Flux**           | GitOps     |     â­â­â­     | K8s GitOps       |    Free     |
| **Jenkins**        | CI/CD      |     â­â­â­     | Traditional      |    Free     |
| **GitHub Actions** | CI/CD      |      â­â­      | GitHub projects  |  Free tier  |
| **GitLab CI**      | CI/CD      |      â­â­      | GitLab projects  |  Free tier  |
| **AWS CodeDeploy** | Deployment |      â­â­      | AWS              | Pay-per-use |
| **Vercel**         | Platform   |       â­       | Frontend/Next.js |  Free tier  |
| **Railway**        | Platform   |       â­       | Full-stack apps  |  Free tier  |

</div>

---

<div align="center">

## ğŸ’¡ Best Practices

_MrDib's deployment wisdom_ ğŸ“

</div>

### The Deployment Commandments

```
ğŸ† The 15 Commandments of Safe Deployment:

1ï¸âƒ£  Always Have a Rollback Plan
    â€¢ Know how to rollback BEFORE deploying
    â€¢ Test rollback procedure
    â€¢ Document it

2ï¸âƒ£  Deploy During Low Traffic
    â€¢ 2 AM deployments exist for a reason
    â€¢ Or use gradual rollouts

3ï¸âƒ£  Monitor Everything
    â€¢ Error rates
    â€¢ Latency
    â€¢ Resource usage
    â€¢ Business metrics

4ï¸âƒ£  Automate, Automate, Automate
    â€¢ Manual deployments = human errors
    â€¢ CI/CD pipelines are your friend

5ï¸âƒ£  Test in Production-Like Environment
    â€¢ Staging should mirror production
    â€¢ Test with real data volumes

6ï¸âƒ£  Use Feature Flags
    â€¢ Decouple deploy from release
    â€¢ Instant rollback without redeployment

7ï¸âƒ£  Deploy Small Changes
    â€¢ Small changes = small risk
    â€¢ Deploy frequently

8ï¸âƒ£  Never Skip the Smoke Tests
    â€¢ Basic functionality checks
    â€¢ Critical user flows

9ï¸âƒ£  Communicate with Team
    â€¢ Notify before deployment
    â€¢ Keep team updated
    â€¢ Post-mortem after incidents

ğŸ”Ÿ Document Everything
    â€¢ What changed?
    â€¢ Why?
    â€¢ Who deployed?
    â€¢ What time?

1ï¸âƒ£1ï¸âƒ£  Version Everything
    â€¢ Code (Git tags)
    â€¢ Docker images
    â€¢ Database schemas

1ï¸âƒ£2ï¸âƒ£  Handle Database Migrations Carefully
    â€¢ Backward compatible changes
    â€¢ Expand-migrate-contract
    â€¢ Never break old code

1ï¸âƒ£3ï¸âƒ£  Set Up Proper Alerts
    â€¢ Error rate spikes
    â€¢ Latency increases
    â€¢ Pod crashes

1ï¸âƒ£4ï¸âƒ£  Practice Chaos Engineering
    â€¢ Test failure scenarios
    â€¢ Practice rollbacks
    â€¢ Run game days

1ï¸âƒ£5ï¸âƒ£  Learn from Failures
    â€¢ Blameless post-mortems
    â€¢ Document lessons learned
    â€¢ Improve processes
```

---

### Common Deployment Mistakes

```
âŒ TOP 10 DEPLOYMENT DISASTERS:

1. "It works on my machine"
   â†’ Use containers, ensure parity

2. Deploying Friday afternoon
   â†’ Never deploy before weekend

3. No rollback plan
   â†’ Always have a way back

4. Skipping staging
   â†’ Test in prod-like environment

5. Deploying database changes with code
   â†’ Separate DB migrations

6. Not monitoring after deploy
   â†’ Watch for 30+ minutes

7. Deploying during peak traffic
   â†’ Choose low-traffic windows

8. No health checks
   â†’ Implement proper probes

9. Ignoring warnings in logs
   â†’ Warnings become errors

10. Not testing rollback
    â†’ Test rollback before you need it
```

---

<div align="center">

## ğŸ‰ You're Now a Deployment Master!

**You've learned:**

- âœ… 7 deployment strategies
- âœ… Zero-downtime techniques
- âœ… Database migration patterns
- âœ… Feature flag implementation
- âœ… Rollback strategies
- âœ… Monitoring & observability
- âœ… Production-ready scripts
- âœ… Best practices

### Remember

> **"Hope is not a strategy."** - Traditional DevOps Wisdom

And more importantly:

> **"Deploy small, deploy often, and always have a rollback plan."** - MrDib ğŸš€

</div>

---

### Quick Reference

<div align="center">

| Situation                               | Use This    | Why                 |
| :-------------------------------------- | :---------- | :------------------ |
| **Small app, low traffic**              | Rolling     | Simple, efficient   |
| **Critical app, need instant rollback** | Blue-Green  | Instant switch back |
| **Large scale, gradual testing**        | Canary      | Low risk validation |
| **Testing features**                    | A/B Testing | Measure impact      |
| **Testing performance**                 | Shadow      | No user impact      |
| **Dev/Test only**                       | Recreate    | Simplest            |

</div>

---

<div align="center">

**Built with ğŸš€ by MrDib, for engineers who ship with confidence**

_Now go deploy something amazing (safely)!_ âœ¨

**Remember: The best deployment is the one nobody notices!** ğŸ¯

</div>

---

### Final Pro Tips

```
ğŸ’ MrDib's Deployment Wisdom:

1. Start simple (Rolling) â†’ Add complexity as needed
2. Blue-Green for critical apps worth the cost
3. Canary for large-scale (> 100K users)
4. Feature flags are your insurance policy
5. Database migrations need 3-phase deployment
6. Monitor for 30min minimum after deploy
7. Rollback if in doubt
8. Document everything
9. Automate everything
10. Practice makes perfect (run drills!)

ğŸ“š Essential Reading:
â€¢ "Accelerate" by Nicole Forsgren
â€¢ "Site Reliability Engineering" by Google
â€¢ "The Phoenix Project" by Gene Kim

ğŸ”§ Must-Have Tools:
â€¢ Kubernetes (orchestration)
â€¢ ArgoCD (GitOps)
â€¢ Prometheus + Grafana (monitoring)
â€¢ LaunchDarkly or similar (feature flags)
â€¢ PagerDuty (alerting)

ğŸ¯ The Golden Rules:
â€¢ Deploy frequently (reduce risk per deploy)
â€¢ Monitor everything (you can't fix what you can't see)
â€¢ Automate (humans make mistakes)
â€¢ Test rollbacks (practice before you need it)
â€¢ Learn from failures (blameless post-mortems)

Now go ship code with confidence! ğŸš€
```
